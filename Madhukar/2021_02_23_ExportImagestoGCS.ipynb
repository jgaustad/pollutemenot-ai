{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGNyccn1n8ig",
    "outputId": "b765fce2-a360-400b-c62a-3e4800648ddc"
   },
   "outputs": [],
   "source": [
    "# PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "\n",
    "# !earthengine authenticate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8pYNY3YLn9zJ"
   },
   "outputs": [],
   "source": [
    "# # PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "# # authenticate to Google Colab\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkMz8_N5n92p",
    "outputId": "18571437-d60b-41f8-a8b3-e896b8ab5812"
   },
   "outputs": [],
   "source": [
    "# # USE MIDSCWA@gmail.com/cleanwater\n",
    "# # to access csv file\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62SEig6EPDZ7"
   },
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a_aMcWg6_13U"
   },
   "outputs": [],
   "source": [
    "# Sentinel Level 2A surface reflectances\n",
    "# Note: 2A is a processed file whereby Level 1A top-of-atmosphere TOA \n",
    "# reflectance is converted to surface reflectance\n",
    "\n",
    "\n",
    "# Use the latest Sentinel-2 Cloud Masking with s2cloudless\n",
    "# https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless\n",
    "# Parameter | Type\t| Description\n",
    "# AOI\tee.Geometry\tArea of interest\n",
    "# START_DATE\tstring\tImage collection start date (inclusive)\n",
    "# END_DATE\tstring\tImage collection end date (exclusive)\n",
    "# CLOUD_FILTER\tinteger\tMaximum image cloud cover percent allowed in image \n",
    "# collection\n",
    "# CLD_PRB_THRESH\tinteger\tCloud probability (%); values greater than are \n",
    "# considered cloud\n",
    "# NIR_DRK_THRESH\tfloat\tNear-infrared reflectance; values less than are considered \n",
    "# potential cloud shadow\n",
    "# CLD_PRJ_DIST\tfloat\tMaximum distance (km) to search for cloud shadows from \n",
    "# cloud edges\n",
    "# BUFFER\tinteger\tDistance (m) to dilate the edge of cloud-identified objects\n",
    "\n",
    "\n",
    "START_DATE = '2018-08-01'\n",
    "END_DATE = '2020-04-01'\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "\n",
    "# # Build a Sentinel-2 collection\n",
    "# # Sentinel-2 surface reflectance and Sentinel-2 cloud probability are two \n",
    "# different image collections. Each collection must be filtered similarly \n",
    "# (e.g., by date and bounds) and then the two filtered collections must \n",
    "# be joined.\n",
    "\n",
    "# # Define a function to filter the SR and s2cloudless collections according \n",
    "# to area of interest and date parameters, then join them on the system:index \n",
    "# property. The result is a copy of the SR collection where each image has a \n",
    "# new 's2cloudless' property whose value is the corresponding s2cloudless image.\n",
    "\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by \n",
    "    # the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F9bVlKz2ka6E"
   },
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FqexfNCnkeuA"
   },
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ej9_E_r2kNuX"
   },
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1EU750KEkN4X"
   },
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y3v_crqO313l"
   },
   "outputs": [],
   "source": [
    "def s2_sr_median_func(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_median = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .median())\n",
    "  return s2_sr_median\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B47yUe4SATbx"
   },
   "source": [
    "# SRTM, JRC and NDVI etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xjQ9sH-Gn-Cq"
   },
   "outputs": [],
   "source": [
    "# SRTM for elevation\n",
    "srtm = ee.Image('USGS/SRTMGL1_003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NuSIR949X2GG"
   },
   "outputs": [],
   "source": [
    "# slope of terrain\n",
    "slope = ee.Terrain.slope(srtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VmW0Whdnq5b",
    "outputId": "a5830380-194e-4f9f-82b9-60c5694287b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seasonality', 'transition', 'max_extent']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add JRC bands of interest\n",
    "jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                .bandNames().getInfo()\n",
    "jrc = jrc.select(jrc_bands)\n",
    "jrc.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "D7BzVImhV_nh"
   },
   "outputs": [],
   "source": [
    "def make_ndvi(image, red='B4', nir='B8'):\n",
    "  return image.normalizedDifference([nir, red])  \n",
    "\n",
    "def make_ndwi(image, green='B3', nir='B8'):\n",
    "  return image.normalizedDifference([green, nir])  \n",
    "\n",
    "\n",
    "def make_mndvi(image, red='B4', nir='B8'):\n",
    "  nir = image.select('B8')\n",
    "  red = image.select('B4')\n",
    "  aerosols = image.select('B1')  \n",
    "  mndvi = (nir.subtract(red)\n",
    "                      .divide(\n",
    "                          nir.add(red)\n",
    "                          .subtract(aerosols.add(aerosols))\n",
    "                          )\n",
    "                      .rename('mndvi'))\n",
    "  return mndvi\n",
    "\n",
    "def make_mndwi(image, green='B3', swir='B11'):\n",
    "  green = image.select('B3')\n",
    "  swir = image.select('B11')\n",
    "  mndwi = (green.subtract(swir)\n",
    "                .divide(\n",
    "                    green.add(swir))\n",
    "                .rename('mndwi'))\n",
    "  return mndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lSM5VUj7nynK"
   },
   "outputs": [],
   "source": [
    "# choose median of mndwi image collection\n",
    "def make_med_mndwi(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  med_mndwi = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .median()\n",
    "                             )\n",
    "  return med_mndwi # returns the image with median mndwi in the date range\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S4nXxi5um8bZ"
   },
   "outputs": [],
   "source": [
    "# choose the max water pixel from mndwi image collection\n",
    "def s2_sr_greenestpixel_func2(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_greenestpixel = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .qualityMosaic('mndwi')\n",
    "                             )\n",
    "  # returns the image with highest mndwi in the date range\n",
    "  return s2_sr_greenestpixel \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tt96ySPAYIn"
   },
   "source": [
    "# Read in Rapanos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wheel\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_02_23_ExportImagestoGCS.ipynb Data_combined_regular_clean.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6_t_J2SataRZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# datapath = \"drive/MyDrive/Data/combined_v2.csv\"\n",
    "datapath = \"Data_combined_regular_clean.csv\"\n",
    "df = pd.read_csv(datapath, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# column name 'index' is conflicting with the native index of dataframe\n",
    "# hence, creating a new column named \"Index\"\n",
    "df[\"Index\"] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMZyUm24AcWU"
   },
   "source": [
    "# Set up global variables for Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "uPov83g1tiGH"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR BUCKET HERE:\n",
    "BUCKET = 'pollutemenot-ai'\n",
    "# FOLDER = 'test_final'\n",
    "FOLDER = \"GEE_images_final2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9IYvD7NLC5T"
   },
   "source": [
    "# Exporting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "L9KsSAfHL4nj"
   },
   "outputs": [],
   "source": [
    "def doExport_RBG2(lat, lon, index_danum, band, size=1000):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "  imageRGB = image.visualize(**{'bands': ['B4', 'B3', 'B2'], 'max': 9000, 'min': 0.5})\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  else:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = imageRGB, \n",
    "      description = index_danum,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,      \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,            \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Uu0ei3jQLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_index2(lat, lon, index_danum, band, size=1000, func=make_ndvi):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = func(image), \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "sLq5PXm_o2Wy"
   },
   "outputs": [],
   "source": [
    "def doExport_mmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = make_med_mndwi(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Kak-wbbWpOH2"
   },
   "outputs": [],
   "source": [
    "def doExport_gmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = s2_sr_greenestpixel_func2(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fciKiExpLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_srtm2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  image = ee.Image('USGS/SRTMGL1_003')\n",
    "  # image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "6tewPrA5LBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_slope2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "  image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "# lat = 30\n",
    "# lon = -75\n",
    "\n",
    "# folder = FOLDER\n",
    "# size = 1\n",
    "# size_ = \"1\"\n",
    "# band = \"srtm\"\n",
    "# index_danum = \"1\"\n",
    "# task = ee.batch.Export.image.toCloudStorage(\n",
    "#       image = srtm, \n",
    "#       description = \"1\" + '_' + \"1\",\n",
    "#       bucket = BUCKET,\n",
    "#       # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "#       fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "#       region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "#       crs = 'EPSG:4326',\n",
    "#       # crs_transform = crs_transform,\n",
    "#       dimensions = \"256x256\",\n",
    "#       maxPixels = 1E13,\n",
    "#       fileFormat = \"GeoTIFF\",\n",
    "#       formatOptions = {\n",
    "#       \"cloudOptimized\" : True\n",
    "#       }\n",
    "#       )\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "d5DAwuVxLBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_jrc2(lat, lon, index_danum, band, size=1000, func=None, res='hires'):\n",
    "  jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "  # jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                # .bandNames().getInfo()\n",
    "  if band == \"transition\":\n",
    "    jrc = jrc.select(\"transition\")\n",
    "  elif band == \"max_extent\":\n",
    "    jrc = jrc.select(\"max_extent\")\n",
    "  else:\n",
    "    jrc = jrc.select(\"seasonality\")\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = jrc, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "zUJSFiJ5KSwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def export_data2(index_danum, lat, lon):\n",
    "  for size in [1000, 10000]:\n",
    "    # doExport_RBG2(lat, lon, index_danum, 'RBG', size)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndvi', size, make_ndvi)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndwi', size, make_ndwi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndvi', size, make_mndvi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndwi', size, make_mndwi)\n",
    "    doExport_gmndwi(lat, lon, index_danum, 'gmndwi', size, None)\n",
    "    # doExport_gmndwi(lat, lon, index_danum, 'mmndwi', size, None)\n",
    "    doExport_srtm2(lat, lon, index_danum, 'srtm', size, None)\n",
    "    # doExport_slope2(lat, lon, index_danum, 'slope', size, None)\n",
    "    if size == 1000:\n",
    "      doExport_jrc2(lat, lon, index_danum, 'seasonality', size, None)\n",
    "      doExport_jrc2(lat, lon, index_danum, 'transition', size, None)\n",
    "    # doExport_jrc2(lat, lon, index_danum, 'max_extent', size, None, hires)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XprqTJDAaRo"
   },
   "source": [
    "# Exporting Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "W4n0N8A573Je"
   },
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 4000\n",
    "# SHOBHA: records 4000 - 8000\n",
    "# RADHIKA: records 8000 - 12000\n",
    "# JOE: 12000 - last\n",
    "\n",
    "names = [\"MADHUKAR\", 'SHOBHA', 'RADHIKA', 'JOE']\n",
    "start = [1, 4000, 8000, 12000]\n",
    "end = [4000, 8000, 12000, df.shape[0]]\n",
    "MY_NAME = \"MADHUKAR\"\n",
    "\n",
    "start_dict = dict(zip(names, start))\n",
    "end_dict = dict(zip(names, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijL-clnf74GG",
    "outputId": "7c16f74a-3208-4fd0-e671-5f1a0a3eade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting index = 29\n",
      "Done uploading hires and lores of index = 29\n",
      "Done uploading hires and lores of index = 30\n",
      "Done uploading hires and lores of index = 31\n",
      "Done uploading hires and lores of index = 32\n",
      "Done uploading hires and lores of index = 33\n",
      "Done uploading hires and lores of index = 34\n",
      "Done uploading hires and lores of index = 35\n",
      "Done uploading hires and lores of index = 36\n",
      "Done uploading hires and lores of index = 37\n",
      "Done uploading hires and lores of index = 38\n",
      "Done uploading hires and lores of index = 39\n",
      "Done uploading hires and lores of index = 40\n",
      "Done uploading hires and lores of index = 41\n",
      "Done uploading hires and lores of index = 42\n",
      "Done uploading hires and lores of index = 43\n",
      "Done uploading hires and lores of index = 44\n",
      "Done uploading hires and lores of index = 45\n",
      "Done uploading hires and lores of index = 46\n",
      "Done uploading hires and lores of index = 47\n",
      "Done uploading hires and lores of index = 48\n",
      "Done uploading hires and lores of index = 49\n"
     ]
    }
   ],
   "source": [
    "# started 7:27am 2/24\n",
    "# 1-28 done\n",
    "index_begin = 29\n",
    "index_end = 300\n",
    "\n",
    "if index_begin >= start_dict[MY_NAME] and index_end <= end_dict[MY_NAME]:\n",
    "  for count in range(index_begin, index_end):\n",
    "    if count == index_begin: print(\"exporting index =\", count)\n",
    "    da_number = df.iloc[count-1][\"da_number\"]\n",
    "    latitude = df.iloc[count-1][\"latitude\"]\n",
    "    longitude = df.iloc[count-1][\"longitude\"]\n",
    "    index = count\n",
    "    export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "    print(\"Done uploading hires and lores of index =\", index)\n",
    "else:\n",
    "  print(\"Please ensure the begin and end is within the interval assigned to you\")\n",
    "\n",
    "print(\"Woohoo all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULTYN5stCZXl"
   },
   "outputs": [],
   "source": [
    "stop # stope execution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Y_Ux_ggTCy"
   },
   "source": [
    "# SSURGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxvR5sNlgUrB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ssurgo_path = \"/content/drive/MyDrive/GeoSpatialData/SSURGO/muaggatt.zip\"\n",
    "df_s = pd.read_csv(ssurgo_path, compression='zip', header=0, sep='\\t', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F2QAB0cmMdv"
   },
   "outputs": [],
   "source": [
    "def find_percent_hydric(lat=None, lon=None):\n",
    "  ssurgo = ee.Image(\"users/madhukarreddy/gSSURGO\")\n",
    "  pt = ee.Geometry.Point([lon, lat])\n",
    "  mukey = ssurgo.select('b1').clip(pt).sample(pt).getInfo()[\"features\"][0]['properties']['b1'] \n",
    "  hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "  return int(hydclprs)\n",
    "\n",
    "# lat = float(df[df.index == 100].latitude)\n",
    "# lon = float(df[df.index == 100].longitude)\n",
    "find_percent_hydric(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpXFsD2UxXy"
   },
   "source": [
    "# Opening GeoTIFF images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4p7GDYFtC0L"
   },
   "source": [
    "## Download GCS contents to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jzvTSAcF4BR"
   },
   "outputs": [],
   "source": [
    "# create relevant folder for download using %cd and %mkdir\n",
    "# %cd drive/MyDrive/Madhukar/images\n",
    "# %mkdir /content/drive/MyDrive/Madhukar/images3\n",
    "\n",
    "# https://philipplies.medium.com/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041\n",
    "# !gsutil -m cp -r gs://pollutemenot-ai/test3/hires/gmndwi /content/drive/MyDrive/Madhukar/images/test3_1/hires/gmndwi\n",
    "\n",
    "# !gsutil -m cp -r \"gs://pollutemenot-ai/test3/\" \"/content/drive/MyDrive/Madhukar/images3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQNZ1xBzj-Rp"
   },
   "outputs": [],
   "source": [
    "# use !pwd to get local path\n",
    "# dont forget the / at the end!\n",
    "local_download_path = r\"/content/drive/MyDrive/Madhukar/test_final/lores/srtm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZVPulc6yW-g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [None] * 3\n",
    "\n",
    "# https://howtothink.readthedocs.io/en/latest/PvL_H.html\n",
    "for i in range(1,4):\n",
    "  ax[i-1] = fig.add_subplot(2, 2, i) \n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(local_download_path):\n",
    "    \n",
    "  if filename.endswith(\"tif\"): \n",
    "      print(filename)\n",
    "      try:\n",
    "        dataset = gdal.Open(local_download_path+filename, gdal.GA_ReadOnly) \n",
    "        # Note GetRasterBand() takes band no. starting from 1 not 0\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        arr = band.ReadAsArray()\n",
    "        colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]  # R -> G -> B correct form\n",
    "        # colors = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]  # B -> G -> R\n",
    "        n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "        cmap_name = 'my_list'\n",
    "        cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "\n",
    "        ax[count].imshow(arr, cmap=cm)\n",
    "        ax[count].set_title(filename.split(\"_\")[-2])\n",
    "        # plt.imshow(arr)\n",
    "        # dataset.GetGeoTransform()\n",
    "        print(\"({},{})\".format(dataset.GetRasterBand(1).XSize, dataset.GetRasterBand(1).YSize))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "  count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpnvJSsjpYVh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.02.23_ExportImagestoGCS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
