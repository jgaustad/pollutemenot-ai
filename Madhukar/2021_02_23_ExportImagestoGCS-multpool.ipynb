{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGNyccn1n8ig",
    "outputId": "b765fce2-a360-400b-c62a-3e4800648ddc"
   },
   "outputs": [],
   "source": [
    "# PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "\n",
    "# !earthengine authenticate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8pYNY3YLn9zJ"
   },
   "outputs": [],
   "source": [
    "# # PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "# # authenticate to Google Colab\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkMz8_N5n92p",
    "outputId": "18571437-d60b-41f8-a8b3-e896b8ab5812"
   },
   "outputs": [],
   "source": [
    "# # USE MIDSCWA@gmail.com/cleanwater\n",
    "# # to access csv file\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62SEig6EPDZ7"
   },
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a_aMcWg6_13U"
   },
   "outputs": [],
   "source": [
    "# Sentinel Level 2A surface reflectances\n",
    "# Note: 2A is a processed file whereby Level 1A top-of-atmosphere TOA \n",
    "# reflectance is converted to surface reflectance\n",
    "\n",
    "\n",
    "# Use the latest Sentinel-2 Cloud Masking with s2cloudless\n",
    "# https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless\n",
    "# Parameter | Type\t| Description\n",
    "# AOI\tee.Geometry\tArea of interest\n",
    "# START_DATE\tstring\tImage collection start date (inclusive)\n",
    "# END_DATE\tstring\tImage collection end date (exclusive)\n",
    "# CLOUD_FILTER\tinteger\tMaximum image cloud cover percent allowed in image \n",
    "# collection\n",
    "# CLD_PRB_THRESH\tinteger\tCloud probability (%); values greater than are \n",
    "# considered cloud\n",
    "# NIR_DRK_THRESH\tfloat\tNear-infrared reflectance; values less than are considered \n",
    "# potential cloud shadow\n",
    "# CLD_PRJ_DIST\tfloat\tMaximum distance (km) to search for cloud shadows from \n",
    "# cloud edges\n",
    "# BUFFER\tinteger\tDistance (m) to dilate the edge of cloud-identified objects\n",
    "\n",
    "\n",
    "START_DATE = '2018-08-01'\n",
    "END_DATE = '2020-04-01'\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "\n",
    "# # Build a Sentinel-2 collection\n",
    "# # Sentinel-2 surface reflectance and Sentinel-2 cloud probability are two \n",
    "# different image collections. Each collection must be filtered similarly \n",
    "# (e.g., by date and bounds) and then the two filtered collections must \n",
    "# be joined.\n",
    "\n",
    "# # Define a function to filter the SR and s2cloudless collections according \n",
    "# to area of interest and date parameters, then join them on the system:index \n",
    "# property. The result is a copy of the SR collection where each image has a \n",
    "# new 's2cloudless' property whose value is the corresponding s2cloudless image.\n",
    "\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by \n",
    "    # the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F9bVlKz2ka6E"
   },
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FqexfNCnkeuA"
   },
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ej9_E_r2kNuX"
   },
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1EU750KEkN4X"
   },
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y3v_crqO313l"
   },
   "outputs": [],
   "source": [
    "def s2_sr_median_func(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_median = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .median())\n",
    "  return s2_sr_median\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B47yUe4SATbx"
   },
   "source": [
    "# SRTM, JRC and NDVI etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xjQ9sH-Gn-Cq"
   },
   "outputs": [],
   "source": [
    "# SRTM for elevation\n",
    "srtm = ee.Image('USGS/SRTMGL1_003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NuSIR949X2GG"
   },
   "outputs": [],
   "source": [
    "# slope of terrain\n",
    "slope = ee.Terrain.slope(srtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VmW0Whdnq5b",
    "outputId": "a5830380-194e-4f9f-82b9-60c5694287b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seasonality', 'transition', 'max_extent']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add JRC bands of interest\n",
    "jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                .bandNames().getInfo()\n",
    "jrc = jrc.select(jrc_bands)\n",
    "jrc.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D7BzVImhV_nh"
   },
   "outputs": [],
   "source": [
    "def make_ndvi(image, red='B4', nir='B8'):\n",
    "  return image.normalizedDifference([nir, red])  \n",
    "\n",
    "def make_ndwi(image, green='B3', nir='B8'):\n",
    "  return image.normalizedDifference([green, nir])  \n",
    "\n",
    "\n",
    "def make_mndvi(image, red='B4', nir='B8'):\n",
    "  nir = image.select('B8')\n",
    "  red = image.select('B4')\n",
    "  aerosols = image.select('B1')  \n",
    "  mndvi = (nir.subtract(red)\n",
    "                      .divide(\n",
    "                          nir.add(red)\n",
    "                          .subtract(aerosols.add(aerosols))\n",
    "                          )\n",
    "                      .rename('mndvi'))\n",
    "  return mndvi\n",
    "\n",
    "def make_mndwi(image, green='B3', swir='B11'):\n",
    "  green = image.select('B3')\n",
    "  swir = image.select('B11')\n",
    "  mndwi = (green.subtract(swir)\n",
    "                .divide(\n",
    "                    green.add(swir))\n",
    "                .rename('mndwi'))\n",
    "  return mndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lSM5VUj7nynK"
   },
   "outputs": [],
   "source": [
    "# choose median of mndwi image collection\n",
    "def make_med_mndwi(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  med_mndwi = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .median()\n",
    "                             )\n",
    "  return med_mndwi # returns the image with median mndwi in the date range\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S4nXxi5um8bZ"
   },
   "outputs": [],
   "source": [
    "# choose the max water pixel from mndwi image collection\n",
    "def s2_sr_greenestpixel_func2(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_greenestpixel = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .qualityMosaic('mndwi')\n",
    "                             )\n",
    "  # returns the image with highest mndwi in the date range\n",
    "  return s2_sr_greenestpixel \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tt96ySPAYIn"
   },
   "source": [
    "# Read in Rapanos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wheel\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_02_23_ExportImagestoGCS-Copy1.ipynb\n",
      "2021_02_23_ExportImagestoGCS-Copy2.ipynb\n",
      "2021_02_23_ExportImagestoGCS-Copy3.ipynb\n",
      "2021_02_23_ExportImagestoGCS-Copy4.ipynb\n",
      "2021_02_23_ExportImagestoGCS.ipynb\n",
      "Data_combined_regular_clean.csv\n",
      "Data_combined_regular_clean_with_ssurgo_variables.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6_t_J2SataRZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# datapath = \"drive/MyDrive/Data/combined_v2.csv\"\n",
    "datapath = \"Data_combined_regular_clean.csv\"\n",
    "df = pd.read_csv(datapath, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# column name 'index' is conflicting with the native index of dataframe\n",
    "# hence, creating a new column named \"Index\"\n",
    "df[\"Index\"] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMZyUm24AcWU"
   },
   "source": [
    "# Set up global variables for Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uPov83g1tiGH"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR BUCKET HERE:\n",
    "BUCKET = 'pollutemenot-ai'\n",
    "# FOLDER = 'test_final'\n",
    "FOLDER = \"GEE_images_final2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9IYvD7NLC5T"
   },
   "source": [
    "# Exporting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "L9KsSAfHL4nj"
   },
   "outputs": [],
   "source": [
    "def doExport_RBG2(lat, lon, index_danum, band, size=1000):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "  imageRGB = image.visualize(**{'bands': ['B4', 'B3', 'B2'], 'max': 9000, 'min': 0.5})\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  else:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = imageRGB, \n",
    "      description = index_danum,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,      \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,            \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Uu0ei3jQLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_index2(lat, lon, index_danum, band, size=1000, func=make_ndvi):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = func(image), \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sLq5PXm_o2Wy"
   },
   "outputs": [],
   "source": [
    "def doExport_mmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = make_med_mndwi(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kak-wbbWpOH2"
   },
   "outputs": [],
   "source": [
    "def doExport_gmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = s2_sr_greenestpixel_func2(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fciKiExpLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_srtm2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  image = ee.Image('USGS/SRTMGL1_003')\n",
    "  # image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6tewPrA5LBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_slope2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "  image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "# lat = 30\n",
    "# lon = -75\n",
    "\n",
    "# folder = FOLDER\n",
    "# size = 1\n",
    "# size_ = \"1\"\n",
    "# band = \"srtm\"\n",
    "# index_danum = \"1\"\n",
    "# task = ee.batch.Export.image.toCloudStorage(\n",
    "#       image = srtm, \n",
    "#       description = \"1\" + '_' + \"1\",\n",
    "#       bucket = BUCKET,\n",
    "#       # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "#       fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "#       region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "#       crs = 'EPSG:4326',\n",
    "#       # crs_transform = crs_transform,\n",
    "#       dimensions = \"256x256\",\n",
    "#       maxPixels = 1E13,\n",
    "#       fileFormat = \"GeoTIFF\",\n",
    "#       formatOptions = {\n",
    "#       \"cloudOptimized\" : True\n",
    "#       }\n",
    "#       )\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "d5DAwuVxLBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_jrc2(lat, lon, index_danum, band, size=1000, func=None, res='hires'):\n",
    "  jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "  # jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                # .bandNames().getInfo()\n",
    "  if band == \"transition\":\n",
    "    jrc = jrc.select(\"transition\")\n",
    "  elif band == \"max_extent\":\n",
    "    jrc = jrc.select(\"max_extent\")\n",
    "  else:\n",
    "    jrc = jrc.select(\"seasonality\")\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = jrc, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zUJSFiJ5KSwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def export_data2(index_danum, lat, lon):\n",
    "  for size in [1000, 10000]:\n",
    "    # doExport_RBG2(lat, lon, index_danum, 'RBG', size)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndvi', size, make_ndvi)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndwi', size, make_ndwi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndvi', size, make_mndvi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndwi', size, make_mndwi)\n",
    "    doExport_gmndwi(lat, lon, index_danum, 'gmndwi', size, None)\n",
    "    # doExport_gmndwi(lat, lon, index_danum, 'mmndwi', size, None)\n",
    "    doExport_srtm2(lat, lon, index_danum, 'srtm', size, None)\n",
    "    # doExport_slope2(lat, lon, index_danum, 'slope', size, None)\n",
    "    if size == 1000:\n",
    "      doExport_jrc2(lat, lon, index_danum, 'seasonality', size, None)\n",
    "      doExport_jrc2(lat, lon, index_danum, 'transition', size, None)\n",
    "    # doExport_jrc2(lat, lon, index_danum, 'max_extent', size, None, hires)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XprqTJDAaRo"
   },
   "source": [
    "# Exporting Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "W4n0N8A573Je"
   },
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 4000\n",
    "# SHOBHA: records 4000 - 8000\n",
    "# RADHIKA: records 8000 - 12000\n",
    "# JOE: 12000 - last\n",
    "\n",
    "names = [\"MADHUKAR\", 'SHOBHA', 'RADHIKA', 'JOE']\n",
    "start = [1, 4000, 8000, 12000]\n",
    "end = [4000, 8000, 12000, df.shape[0]]\n",
    "MY_NAME = \"JOE\"\n",
    "\n",
    "start_dict = dict(zip(names, start))\n",
    "end_dict = dict(zip(names, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799 [[13201, 'SWG-2015-00684', 29.589859999999998, -95.02628], [13202, 'SWG-2015-00685', 27.72745, -97.62183], [13203, 'SWG-2015-00687', 29.44333, -94.93656999999999]]\n"
     ]
    }
   ],
   "source": [
    "index_begin = 13201\n",
    "index_end = 14000\n",
    "\n",
    "list_of_parameters = []\n",
    "for count in range(index_begin, index_end):\n",
    "    #if count == index_begin: print(\"exporting index =\", count)\n",
    "    da_number = df.iloc[count-1][\"da_number\"]\n",
    "    latitude = df.iloc[count-1][\"latitude\"]\n",
    "    longitude = df.iloc[count-1][\"longitude\"]\n",
    "    index = count\n",
    "    list_of_parameters.append([index, da_number, latitude, longitude])\n",
    "print(len(list_of_parameters), list_of_parameters[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting processing and uploading of index starting processing and uploading of indexstarting processing and uploading of indexstarting processing and uploading of index13241  \n",
      "starting processing and uploading of index 13321 1336113281\n",
      "\n",
      "\n",
      "13201\n",
      "Done uploading hires and lores of index = 13281\n",
      "starting processing and uploading of index 13282\n",
      "Done uploading hires and lores of index = 13361\n",
      "starting processing and uploading of index 13362\n",
      "Done uploading hires and lores of index = 13241\n",
      "starting processing and uploading of index 13242\n",
      "Done uploading hires and lores of index = 13321\n",
      "starting processing and uploading of index 13322\n",
      "Done uploading hires and lores of index = 13201\n",
      "starting processing and uploading of index 13202\n",
      "Done uploading hires and lores of index = 13282\n",
      "starting processing and uploading of index 13283\n",
      "Done uploading hires and lores of index = 13242\n",
      "starting processing and uploading of index 13243\n",
      "Done uploading hires and lores of index = 13322\n",
      "starting processing and uploading of index 13323\n",
      "Done uploading hires and lores of index = 13362\n",
      "starting processing and uploading of index 13363\n",
      "Done uploading hires and lores of index = 13202\n",
      "starting processing and uploading of index 13203\n",
      "Done uploading hires and lores of index = 13283\n",
      "starting processing and uploading of index 13284\n",
      "Done uploading hires and lores of index = 13243\n",
      "starting processing and uploading of index 13244\n",
      "Done uploading hires and lores of index = 13323\n",
      "starting processing and uploading of index 13324\n",
      "Done uploading hires and lores of index = 13203\n",
      "starting processing and uploading of index 13204\n",
      "Done uploading hires and lores of index = 13363\n",
      "starting processing and uploading of index 13364\n",
      "Done uploading hires and lores of index = 13284\n",
      "starting processing and uploading of index 13285\n",
      "Done uploading hires and lores of index = 13244\n",
      "starting processing and uploading of index 13245\n",
      "Done uploading hires and lores of index = 13324\n",
      "starting processing and uploading of index 13325\n",
      "Done uploading hires and lores of index = 13204\n",
      "starting processing and uploading of index 13205\n",
      "Done uploading hires and lores of index = 13364\n",
      "starting processing and uploading of index 13365\n",
      "Done uploading hires and lores of index = 13245\n",
      "starting processing and uploading of index 13246\n",
      "Done uploading hires and lores of index = 13325\n",
      "starting processing and uploading of index 13326\n",
      "Done uploading hires and lores of index = 13285\n",
      "starting processing and uploading of index 13286\n",
      "Done uploading hires and lores of index = 13205\n",
      "starting processing and uploading of index 13206\n",
      "Done uploading hires and lores of index = 13365\n",
      "starting processing and uploading of index 13366\n",
      "Done uploading hires and lores of index = 13246\n",
      "starting processing and uploading of index 13247\n",
      "Done uploading hires and lores of index = 13286\n",
      "starting processing and uploading of index 13287\n",
      "Done uploading hires and lores of index = 13326\n",
      "starting processing and uploading of index 13327\n",
      "Done uploading hires and lores of index = 13206\n",
      "starting processing and uploading of index 13207\n",
      "Done uploading hires and lores of index = 13366\n",
      "starting processing and uploading of index 13367\n",
      "Done uploading hires and lores of index = 13247\n",
      "starting processing and uploading of index 13248\n",
      "Done uploading hires and lores of index = 13287\n",
      "starting processing and uploading of index 13288\n",
      "Done uploading hires and lores of index = 13207\n",
      "starting processing and uploading of index 13208\n",
      "Done uploading hires and lores of index = 13327\n",
      "starting processing and uploading of index 13328\n",
      "Done uploading hires and lores of index = 13367\n",
      "starting processing and uploading of index 13368\n",
      "Done uploading hires and lores of index = 13288\n",
      "starting processing and uploading of index 13289\n",
      "Done uploading hires and lores of index = 13248\n",
      "starting processing and uploading of index 13249\n",
      "Done uploading hires and lores of index = 13208\n",
      "starting processing and uploading of index 13209\n",
      "Done uploading hires and lores of index = 13328\n",
      "starting processing and uploading of index 13329\n",
      "Done uploading hires and lores of index = 13368\n",
      "starting processing and uploading of index 13369\n",
      "Done uploading hires and lores of index = 13289\n",
      "starting processing and uploading of index 13290\n",
      "Done uploading hires and lores of index = 13249\n",
      "starting processing and uploading of index 13250\n",
      "Done uploading hires and lores of index = 13209\n",
      "starting processing and uploading of index 13210\n",
      "Done uploading hires and lores of index = 13369\n",
      "starting processing and uploading of index 13370\n",
      "Done uploading hires and lores of index = 13329\n",
      "starting processing and uploading of index 13330\n",
      "Done uploading hires and lores of index = 13290\n",
      "starting processing and uploading of index 13291\n",
      "Done uploading hires and lores of index = 13210\n",
      "starting processing and uploading of index 13211\n",
      "Done uploading hires and lores of index = 13250\n",
      "starting processing and uploading of index 13251\n",
      "Done uploading hires and lores of index = 13370\n",
      "starting processing and uploading of index 13371\n",
      "Done uploading hires and lores of index = 13330\n",
      "starting processing and uploading of index 13331\n",
      "Done uploading hires and lores of index = 13291\n",
      "starting processing and uploading of index 13292\n",
      "Done uploading hires and lores of index = 13211\n",
      "starting processing and uploading of index 13212\n",
      "Done uploading hires and lores of index = 13251\n",
      "starting processing and uploading of index 13252\n",
      "Done uploading hires and lores of index = 13371\n",
      "starting processing and uploading of index 13372\n",
      "Done uploading hires and lores of index = 13292\n",
      "starting processing and uploading of index 13293\n",
      "Done uploading hires and lores of index = 13331\n",
      "starting processing and uploading of index 13332\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def export_data_pooler(x):\n",
    "    index, da_number, latitude, longitude = x\n",
    "    print('starting processing and uploading of index', index)\n",
    "    export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "    print(\"Done uploading hires and lores of index =\", index)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        print(p.map(export_data_pooler, list_of_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijL-clnf74GG",
    "outputId": "7c16f74a-3208-4fd0-e671-5f1a0a3eade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting index = 12000\n",
      "Done uploading hires and lores of index = 12000\n",
      "Done uploading hires and lores of index = 12001\n",
      "Done uploading hires and lores of index = 12002\n",
      "Done uploading hires and lores of index = 12003\n",
      "Done uploading hires and lores of index = 12004\n",
      "Done uploading hires and lores of index = 12005\n",
      "Done uploading hires and lores of index = 12006\n",
      "Done uploading hires and lores of index = 12007\n",
      "Done uploading hires and lores of index = 12008\n",
      "Done uploading hires and lores of index = 12009\n",
      "Done uploading hires and lores of index = 12010\n",
      "Done uploading hires and lores of index = 12011\n",
      "Done uploading hires and lores of index = 12012\n",
      "Done uploading hires and lores of index = 12013\n",
      "Done uploading hires and lores of index = 12014\n",
      "Done uploading hires and lores of index = 12015\n",
      "Done uploading hires and lores of index = 12016\n",
      "Done uploading hires and lores of index = 12017\n",
      "Done uploading hires and lores of index = 12018\n",
      "Done uploading hires and lores of index = 12019\n",
      "Done uploading hires and lores of index = 12020\n",
      "Done uploading hires and lores of index = 12021\n",
      "Done uploading hires and lores of index = 12022\n",
      "Done uploading hires and lores of index = 12023\n",
      "Done uploading hires and lores of index = 12024\n",
      "Done uploading hires and lores of index = 12025\n",
      "Done uploading hires and lores of index = 12026\n",
      "Done uploading hires and lores of index = 12027\n",
      "Done uploading hires and lores of index = 12028\n",
      "Done uploading hires and lores of index = 12029\n",
      "Done uploading hires and lores of index = 12030\n",
      "Done uploading hires and lores of index = 12031\n",
      "Done uploading hires and lores of index = 12032\n",
      "Done uploading hires and lores of index = 12033\n",
      "Done uploading hires and lores of index = 12034\n",
      "Done uploading hires and lores of index = 12035\n",
      "Done uploading hires and lores of index = 12036\n",
      "Done uploading hires and lores of index = 12037\n",
      "Done uploading hires and lores of index = 12038\n",
      "Done uploading hires and lores of index = 12039\n",
      "Done uploading hires and lores of index = 12040\n",
      "Done uploading hires and lores of index = 12041\n",
      "Done uploading hires and lores of index = 12042\n",
      "Done uploading hires and lores of index = 12043\n",
      "Done uploading hires and lores of index = 12044\n",
      "Done uploading hires and lores of index = 12045\n",
      "Done uploading hires and lores of index = 12046\n",
      "Done uploading hires and lores of index = 12047\n",
      "Done uploading hires and lores of index = 12048\n",
      "Done uploading hires and lores of index = 12049\n",
      "Done uploading hires and lores of index = 12050\n",
      "Done uploading hires and lores of index = 12051\n",
      "Done uploading hires and lores of index = 12052\n",
      "Done uploading hires and lores of index = 12053\n",
      "Done uploading hires and lores of index = 12054\n",
      "Done uploading hires and lores of index = 12055\n",
      "Done uploading hires and lores of index = 12056\n",
      "Done uploading hires and lores of index = 12057\n",
      "Done uploading hires and lores of index = 12058\n",
      "Done uploading hires and lores of index = 12059\n",
      "Done uploading hires and lores of index = 12060\n",
      "Done uploading hires and lores of index = 12061\n",
      "Done uploading hires and lores of index = 12062\n",
      "Done uploading hires and lores of index = 12063\n",
      "Done uploading hires and lores of index = 12064\n",
      "Done uploading hires and lores of index = 12065\n",
      "Done uploading hires and lores of index = 12066\n",
      "Done uploading hires and lores of index = 12067\n",
      "Done uploading hires and lores of index = 12068\n",
      "Done uploading hires and lores of index = 12069\n",
      "Done uploading hires and lores of index = 12070\n",
      "Done uploading hires and lores of index = 12071\n",
      "Done uploading hires and lores of index = 12072\n",
      "Done uploading hires and lores of index = 12073\n",
      "Done uploading hires and lores of index = 12074\n",
      "Done uploading hires and lores of index = 12075\n",
      "Done uploading hires and lores of index = 12076\n",
      "Done uploading hires and lores of index = 12077\n",
      "Done uploading hires and lores of index = 12078\n",
      "Done uploading hires and lores of index = 12079\n",
      "Done uploading hires and lores of index = 12080\n",
      "Done uploading hires and lores of index = 12081\n",
      "Done uploading hires and lores of index = 12082\n",
      "Done uploading hires and lores of index = 12083\n",
      "Done uploading hires and lores of index = 12084\n",
      "Done uploading hires and lores of index = 12085\n",
      "Done uploading hires and lores of index = 12086\n",
      "Done uploading hires and lores of index = 12087\n",
      "Done uploading hires and lores of index = 12088\n",
      "Done uploading hires and lores of index = 12089\n",
      "Done uploading hires and lores of index = 12090\n",
      "Done uploading hires and lores of index = 12091\n",
      "Done uploading hires and lores of index = 12092\n",
      "Done uploading hires and lores of index = 12093\n",
      "Done uploading hires and lores of index = 12094\n",
      "Done uploading hires and lores of index = 12095\n",
      "Done uploading hires and lores of index = 12096\n",
      "Done uploading hires and lores of index = 12097\n"
     ]
    }
   ],
   "source": [
    "# started 7:27am 2/24\n",
    "# 1-28 done\n",
    "index_begin = 12000\n",
    "index_end = df.shape[0]\n",
    "\n",
    "if index_begin >= start_dict[MY_NAME] and index_end <= end_dict[MY_NAME]:\n",
    "  for count in range(index_begin, index_end):\n",
    "    if count == index_begin: print(\"exporting index =\", count)\n",
    "    da_number = df.iloc[count-1][\"da_number\"]\n",
    "    latitude = df.iloc[count-1][\"latitude\"]\n",
    "    longitude = df.iloc[count-1][\"longitude\"]\n",
    "    index = count\n",
    "    export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "    print(\"Done uploading hires and lores of index =\", index)\n",
    "else:\n",
    "  print(\"Please ensure the begin and end is within the interval assigned to you\")\n",
    "\n",
    "print(\"Woohoo all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULTYN5stCZXl"
   },
   "outputs": [],
   "source": [
    "stop # stope execution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Y_Ux_ggTCy"
   },
   "source": [
    "# SSURGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxvR5sNlgUrB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ssurgo_path = \"/content/drive/MyDrive/GeoSpatialData/SSURGO/muaggatt.zip\"\n",
    "df_s = pd.read_csv(ssurgo_path, compression='zip', header=0, sep='\\t', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F2QAB0cmMdv"
   },
   "outputs": [],
   "source": [
    "def find_percent_hydric(lat=None, lon=None):\n",
    "  ssurgo = ee.Image(\"users/madhukarreddy/gSSURGO\")\n",
    "  pt = ee.Geometry.Point([lon, lat])\n",
    "  mukey = ssurgo.select('b1').clip(pt).sample(pt).getInfo()[\"features\"][0]['properties']['b1'] \n",
    "  hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "  return int(hydclprs)\n",
    "\n",
    "# lat = float(df[df.index == 100].latitude)\n",
    "# lon = float(df[df.index == 100].longitude)\n",
    "find_percent_hydric(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpXFsD2UxXy"
   },
   "source": [
    "# Opening GeoTIFF images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4p7GDYFtC0L"
   },
   "source": [
    "## Download GCS contents to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jzvTSAcF4BR"
   },
   "outputs": [],
   "source": [
    "# create relevant folder for download using %cd and %mkdir\n",
    "# %cd drive/MyDrive/Madhukar/images\n",
    "# %mkdir /content/drive/MyDrive/Madhukar/images3\n",
    "\n",
    "# https://philipplies.medium.com/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041\n",
    "# !gsutil -m cp -r gs://pollutemenot-ai/test3/hires/gmndwi /content/drive/MyDrive/Madhukar/images/test3_1/hires/gmndwi\n",
    "\n",
    "# !gsutil -m cp -r \"gs://pollutemenot-ai/test3/\" \"/content/drive/MyDrive/Madhukar/images3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQNZ1xBzj-Rp"
   },
   "outputs": [],
   "source": [
    "# use !pwd to get local path\n",
    "# dont forget the / at the end!\n",
    "local_download_path = r\"/content/drive/MyDrive/Madhukar/test_final/lores/srtm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZVPulc6yW-g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [None] * 3\n",
    "\n",
    "# https://howtothink.readthedocs.io/en/latest/PvL_H.html\n",
    "for i in range(1,4):\n",
    "  ax[i-1] = fig.add_subplot(2, 2, i) \n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(local_download_path):\n",
    "    \n",
    "  if filename.endswith(\"tif\"): \n",
    "      print(filename)\n",
    "      try:\n",
    "        dataset = gdal.Open(local_download_path+filename, gdal.GA_ReadOnly) \n",
    "        # Note GetRasterBand() takes band no. starting from 1 not 0\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        arr = band.ReadAsArray()\n",
    "        colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]  # R -> G -> B correct form\n",
    "        # colors = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]  # B -> G -> R\n",
    "        n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "        cmap_name = 'my_list'\n",
    "        cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "\n",
    "        ax[count].imshow(arr, cmap=cm)\n",
    "        ax[count].set_title(filename.split(\"_\")[-2])\n",
    "        # plt.imshow(arr)\n",
    "        # dataset.GetGeoTransform()\n",
    "        print(\"({},{})\".format(dataset.GetRasterBand(1).XSize, dataset.GetRasterBand(1).YSize))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "  count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpnvJSsjpYVh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.02.23_ExportImagestoGCS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
