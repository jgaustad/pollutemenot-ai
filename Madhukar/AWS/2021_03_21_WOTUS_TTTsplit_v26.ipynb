{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gh39s2KK_dGM"
   },
   "outputs": [],
   "source": [
    "# - v1: works till the categorical pipeline where it errors out for unknown variables\n",
    "# - v2: fully functional\n",
    "# - v3: baseline run (on the old dataset)\n",
    "# - v4: baseline run (on the full 2021.03.17_full_dataset)\n",
    "# - v5: baseline run (on 2021.03.19_full_dataset with all SSURGO variables)\n",
    "# - v6: aoc_roc (on 2021.03.19_full_dataset with all SSURGO variables). Mute the Stacking as it is not possible with RandomizedSearchCV. Have to have a dev split.\n",
    "# - v7: model v7 random state 123 on train/test, model v7.1 random_state 431 on train/dev/test\n",
    "# - v8g: Golden model\n",
    "\n",
    "# - ROC_AUC with lat, lon (baseline)\n",
    "# ====================\n",
    "# lr:           0.64103\n",
    "# xgb:          0.82366\n",
    "# voting_clf:   0.83027\n",
    "# lgbm:         0.86395\n",
    "# stacking:     0.85923\n",
    "\n",
    "\n",
    "# - v9: without lat, lon\n",
    "\n",
    "# - ROC_AUC without lat, lon\n",
    "# =======================\n",
    "# lr:           0.64211\n",
    "# xgb:          0.8031\n",
    "# voting_clf:   0.83105\n",
    "# lgbm:         0.85952\n",
    "# stacking:     0.85427\n",
    "\n",
    "# - v9.1: without lat, lon, potential_wetland\n",
    "\n",
    "# - ROC_AUC without lat, lon, potential_wetland\n",
    "# ===============\n",
    "# lr:           0.63992\n",
    "# xgb:          0.79093\n",
    "# voting_clf:   0.82987\n",
    "# lgbm:         0.85143\n",
    "# stacking:     0.84328\n",
    "\n",
    "# - v9.2: without lat, lon, potential_wetland, district\n",
    "\n",
    "# - ROC_AUC without lat, lon, potential_wetland, district\n",
    "# ===============\n",
    "# lr:           0.64208\n",
    "# xgb:          0.77907\n",
    "# voting_clf:   0.79829\n",
    "# lgbm:         0.82675\n",
    "# stacking:     0.82185\n",
    "\n",
    "# - v9.3: without lat, lon, potential_wetland, district and flodfreqdc and drclassdcd as ordinal\n",
    "\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.6382\n",
    "# xgb:          0.73089\n",
    "# voting_clf:   0.79126\n",
    "# lgbm:         0.82512\n",
    "# stacking:     0.8195\n",
    "\n",
    "# - v9.4: without lat, lon, potential_wetland, district, county \n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.6395\n",
    "# xgb:          0.74096\n",
    "# voting_clf:   0.7911\n",
    "# lgbm:         0.82448\n",
    "# stacking:     0.82343\n",
    "\n",
    "# - v9.5: without lat, lon, potential_wetland, district, county, mukey\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.65309\n",
    "# xgb:          0.75117\n",
    "# voting_clf:   0.77845\n",
    "# lgbm:         0.80626\n",
    "# stacking:     0.80424\n",
    "\n",
    "# - v9.5: without lat, lon, potential_wetland, district, county, mukey, and filtering out bad latitude\n",
    "# THIS IS SUSPECT. CANT REPEAT IT.\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.63894\n",
    "# xgb:          0.74922\n",
    "# voting_clf:   0.79839\n",
    "# lgbm:         0.86986\n",
    "# stacking:     0.86281\n",
    "\n",
    "# - v9,5_r2: supposed to be repeat of v9.5 but unsuccessful (probably bcoz of wrong cat features)\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.64148\n",
    "# xgb:          0.7066\n",
    "# lgbm:         0.79702\n",
    "# stacking:     0.78626\n",
    "# voting_clf:   0.7632\n",
    "\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.62669\n",
    "# xgb:          0.71814\n",
    "# lgbm:         0.8006\n",
    "# stacking:     0.80005\n",
    "# voting_clf:   0.7683\n",
    "\n",
    "# - v9.6: baseline and filtering out bad latitude\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.6306\n",
    "# xgb:          0.79659\n",
    "# voting_clf:   0.82816\n",
    "# lgbm:         0.85251\n",
    "# stacking:     0.84547\n",
    "\n",
    "# - v9.6_r2: baseline and filtering out bad latitude\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.6302\n",
    "# xgb:          0.81153\n",
    "# voting_clf:   0.83019\n",
    "# lgbm:         0.85353\n",
    "# stacking:     0.85285\n",
    "\n",
    "\n",
    "# - ROC_AUC with lat, lon (baseline)\n",
    "# ====================\n",
    "# lr:           0.64103\n",
    "# xgb:          0.82366\n",
    "# voting_clf:   0.83027\n",
    "# lgbm:         0.86395\n",
    "# stacking:     0.85923\n",
    "\n",
    "# - v9.4: without lat, lon, potential_wetland, district, county \n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.6395\n",
    "# xgb:          0.74096\n",
    "# voting_clf:   0.7911\n",
    "# lgbm:         0.82448\n",
    "# stacking:     0.82343\n",
    "\n",
    "# - v10: baseline (with huc6, nearest wb/fl parameters added)\n",
    "\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.64444\n",
    "# xgb:          0.80494\n",
    "# voting_clf:   0.82554\n",
    "# lgbm:         0.85336\n",
    "# stacking:     0.8514\n",
    "\n",
    "# - v10.1: baseline (with huc4, nearest wb/fl parameters added)\n",
    "# -ROC_AUC on Test\n",
    "# ===============\n",
    "\n",
    "# lr:           0.64381\n",
    "# voting_clf:   0.82879\n",
    "# xgb:          0.8032\n",
    "# lgbm:         0.85318\n",
    "# stacking:     0.8495\n",
    "\n",
    "# - ROC_AUC on Test: supposed to be repeat of v10.1, but not sure why results are not reproducible\n",
    "# ===============\n",
    "\n",
    "# lr:           0.64381\n",
    "# voting_clf:   0.77483\n",
    "# xgb:          0.75691\n",
    "# lgbm:         0.79563\n",
    "# stacking:     0.78974\n",
    "\n",
    "# - v10.1_rerun: rerunning again and tested by reading back the model file. reproducible results. \n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.64384\n",
    "# voting_clf:   0.8299\n",
    "# xgb:          0.78993\n",
    "# lgbm:         0.85403\n",
    "# stacking:     0.8468\n",
    "\n",
    "\n",
    "# - v10.2: v10.1 with log transformed closest distances (slight improvement seen)\n",
    "# - ROC_AUC on Test: can not reproduce these results the next day. So rerunning again below:\n",
    "# ===============\n",
    "# lr:           0.64219\n",
    "# voting_clf:   0.82924\n",
    "# xgb:          0.7662\n",
    "# lgbm:         0.85495\n",
    "# stacking:     0.85232\n",
    "\n",
    "# - v10.2_rerun:\n",
    "# - ROC_AUC on Test\n",
    "# ===============\n",
    "# lr:           0.64251\n",
    "# voting_clf:   0.82815\n",
    "# xgb:          0.74756\n",
    "# lgbm:         0.854\n",
    "# stacking:     0.84738\n",
    "\n",
    "# - v12: implemented the ppv/npv version\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7711, npv = 0.7734 @ threshold = 0.4949\n",
    "# lr:            ppv = 0.6522, npv = 0.6365 @ threshold = 0.5152\n",
    "# xgb:           ppv = 0.7448, npv = 0.7208 @ threshold = 0.5152\n",
    "# stacking:      ppv = 0.7691, npv = 0.7698 @ threshold = 0.5354\n",
    "# voting_clf:    ppv = 0.7598, npv = 0.7562 @ threshold = 0.4949\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lr:           0.62682\n",
    "# voting_clf:   0.82891\n",
    "# xgb:          0.7816\n",
    "# lgbm:         0.84274\n",
    "# stacking:     0.83437\n",
    "\n",
    "    \n",
    "    \n",
    "# v13: data stratification by district\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7893, npv = 0.7873 @ threshold = 0.5253\n",
    "# lr:            ppv = 0.7053, npv = 0.6413 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7731, npv = 0.7593 @ threshold = 0.4545\n",
    "# stacking:      ppv = 0.7992, npv = 0.7788 @ threshold = 0.5253\n",
    "# voting_clf:    ppv = 0.7757, npv = 0.771 @ threshold = 0.5152\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "\n",
    "# lr:           0.63472\n",
    "# voting_clf:   0.84121\n",
    "# xgb:          0.81637\n",
    "# lgbm:         0.86547\n",
    "# stacking:     0.86017\n",
    "\n",
    "# v13.1: with use_features_in_secondary=False in Stacking (this is slightly better)\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7885, npv = 0.7862 @ threshold = 0.5253\n",
    "# lr:            ppv = 0.7053, npv = 0.6413 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7619, npv = 0.7509 @ threshold = 0.4646\n",
    "# stacking:      ppv = 0.7752, npv = 0.7748 @ threshold = 0.5657 (notice the threshold has increased slightly, ppv (and npv?) decreased)\n",
    "# voting_clf:    ppv = 0.7806, npv = 0.7711 @ threshold = 0.5051\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "\n",
    "# lr:           0.63471\n",
    "# voting_clf:   0.84039\n",
    "# xgb:          0.80691\n",
    "# lgbm:         0.86276\n",
    "# stacking:     0.86068\n",
    "\n",
    "# v13.2:\n",
    "# Using 475 principal components\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.754, npv = 0.7611 @ threshold = 0.4949\n",
    "# lr:            ppv = 0.7241, npv = 0.6409 @ threshold = 0.5455\n",
    "# xgb:           ppv = 0.712, npv = 0.7091 @ threshold = 0.4343\n",
    "# stacking:      ppv = 0.7581, npv = 0.7472 @ threshold = 0.5152\n",
    "# voting_clf:    ppv = 0.7472, npv = 0.7506 @ threshold = 0.4848\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.82615\n",
    "# lr:           0.63732\n",
    "# xgb:          0.73877\n",
    "# stacking:     0.81755\n",
    "# voting_clf:   0.81281\n",
    "\n",
    "# v15: knn imputing of variables at nan_th = 0.2\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7746, npv = 0.7718 @ threshold = 0.5556\n",
    "# lr:            ppv = 0.7053, npv = 0.6413 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7313, npv = 0.7284 @ threshold = 0.4848\n",
    "# stacking:      ppv = 0.7785, npv = 0.7732 @ threshold = 0.5253\n",
    "# voting_clf:    ppv = 0.7531, npv = 0.7545 @ threshold = 0.5152\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lr:           0.62915\n",
    "# voting_clf:   0.81977\n",
    "# xgb:          0.77361\n",
    "# lgbm:         0.8542\n",
    "# stacking:     0.85236\n",
    "\n",
    "\n",
    "# v17: with svc and knn\n",
    "\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7787, npv = 0.77 @ threshold = 0.5455\n",
    "# lr:            ppv = 0.7053, npv = 0.6413 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7273, npv = 0.7205 @ threshold = 0.5051\n",
    "# stacking:      ppv = 0.784, npv = 0.766 @ threshold = 0.5657\n",
    "# voting_clf:    ppv = 0.7534, npv = 0.7585 @ threshold = 0.5051\n",
    "# lgbm_second_level: ppv = 0.8, npv = 0.6308 @ threshold = 0.5657\n",
    "# svc:           ppv = 0.6718, npv = 0.6441 @ threshold = 0.5859\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.85012\n",
    "# lr:           0.62915\n",
    "# xgb:          0.76222\n",
    "# stacking:     0.84442\n",
    "# voting_clf:   0.82\n",
    "# lgbm_second_level:0.50845\n",
    "# svc:          0.67335\n",
    "\n",
    "# v18: Removed Alaska because no SSURGO data. Also, the saved model works!\n",
    "\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7828, npv = 0.7813 @ threshold = 0.5253\n",
    "# lr:            ppv = 0.7053, npv = 0.6359 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7216, npv = 0.7305 @ threshold = 0.3636\n",
    "# stacking:      ppv = 0.758, npv = 0.7954 @ threshold = 0.4242\n",
    "# voting_clf:    ppv = 0.7854, npv = 0.765 @ threshold = 0.5152\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.8642\n",
    "# lr:           0.63127\n",
    "# xgb:          0.77811\n",
    "# stacking:     0.85698\n",
    "# voting_clf:   0.83492\n",
    "\n",
    "# v21: 1000m and 2500m data\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7762, npv = 0.7737 @ threshold = 0.5556\n",
    "# lgbm_groups:   ppv = 0.649, npv = 0.6461 @ threshold = 0.1111\n",
    "# lr:            ppv = 1.0, npv = 0.6252 @ threshold = 0.6364\n",
    "# xgb:           ppv = 0.7304, npv = 0.7347 @ threshold = 0.6061\n",
    "# stacking:      ppv = 0.7869, npv = 0.7513 @ threshold = 0.5354\n",
    "# voting_clf:    ppv = 0.7614, npv = 0.7614 @ threshold = 0.5455\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.85726\n",
    "# lgbm_groups:  0.68616\n",
    "# lr:           0.62552\n",
    "# xgb:          0.79837\n",
    "# stacking:     0.84893\n",
    "# voting_clf:   0.83079\n",
    "\n",
    "# v22: 200m and 1000m... clearly this is the best. Next thing to try is capture the seasonality of the nearby wbs and fls\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7881, npv = 0.7837 @ threshold = 0.5354\n",
    "# lgbm_groups:   ppv = 0.6699, npv = 0.6475 @ threshold = 0.1111\n",
    "# lr:            ppv = 0.7473, npv = 0.6368 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7412, npv = 0.7333 @ threshold = 0.4343\n",
    "# stacking:      ppv = 0.7808, npv = 0.7836 @ threshold = 0.4545\n",
    "# voting_clf:    ppv = 0.7759, npv = 0.7733 @ threshold = 0.5051\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.8641\n",
    "# lgbm_groups:  0.66912\n",
    "# lr:           0.64632\n",
    "# xgb:          0.78758\n",
    "# stacking:     0.85629\n",
    "# voting_clf:   0.84187\n",
    "\n",
    "# v22.1: remove Charleston that contributed the most to fn's in v22\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.8063, npv = 0.805 @ threshold = 0.5657\n",
    "# lgbm_groups:   ppv = 0.6667, npv = 0.6606 @ threshold = 0.0909\n",
    "# lr:            ppv = 0.7471, npv = 0.6427 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7738, npv = 0.7726 @ threshold = 0.4949\n",
    "# stacking:      ppv = 0.7994, npv = 0.7991 @ threshold = 0.5758\n",
    "# voting_clf:    ppv = 0.7906, npv = 0.7922 @ threshold = 0.5556\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.88234\n",
    "# lgbm_groups:  0.69274\n",
    "# lr:           0.65399\n",
    "# xgb:          0.83822\n",
    "# stacking:     0.8749\n",
    "# voting_clf:   0.86033    \n",
    "\n",
    "# \"east_coast = [\"Galveston\", \"New Orleans\", \n",
    "# \"Mobile\", \"Jacksonville\", \"Savannah\", \"Charleston\", 'Wilmington', \"Norfolk\", \"Baltimore\", \"Philadelphia\"]\n",
    "# clear but small improvement but the problem is Charleston was filtered out\n",
    "\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.8074, npv = 0.8068 @ threshold = 0.5657\n",
    "# lgbm_groups:   ppv = 0.6823, npv = 0.6554 @ threshold = 0.101\n",
    "# lr:            ppv = 0.7471, npv = 0.6427 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7807, npv = 0.7757 @ threshold = 0.5253\n",
    "# stacking:      ppv = 0.7992, npv = 0.8091 @ threshold = 0.5556\n",
    "# voting_clf:    ppv = 0.7957, npv = 0.7966 @ threshold = 0.5455\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.88673\n",
    "# lgbm_groups:  0.68095\n",
    "# lr:           0.65398\n",
    "# xgb:          0.84006\n",
    "# stacking:     0.88012\n",
    "# voting_clf:   0.86537\n",
    "\n",
    "\n",
    "# east_coast = [\"New Orleans\", \"Charleston\"]  but the problem is Charleston was filtered out\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.8079, npv = 0.8078 @ threshold = 0.5758\n",
    "# lgbm_groups:   ppv = 0.6776, npv = 0.6583 @ threshold = 0.0909\n",
    "# lr:            ppv = 0.7471, npv = 0.6427 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7495, npv = 0.7353 @ threshold = 0.5253\n",
    "# stacking:      ppv = 0.8023, npv = 0.8002 @ threshold = 0.5758\n",
    "# voting_clf:    ppv = 0.7912, npv = 0.7915 @ threshold = 0.5253\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.8876\n",
    "# lgbm_groups:  0.68694\n",
    "# lr:           0.65398\n",
    "# xgb:          0.82051\n",
    "# stacking:     0.88411\n",
    "# voting_clf:   0.86108\n",
    "\n",
    "# east_coast = [\"New Orleans\", \"Charleston\"]  with Charleston included\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7797, npv = 0.7796 @ threshold = 0.5253\n",
    "# lgbm_groups:   ppv = 0.652, npv = 0.6459 @ threshold = 0.1111\n",
    "# lr:            ppv = 0.7473, npv = 0.6368 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7587, npv = 0.7543 @ threshold = 0.4646\n",
    "# stacking:      ppv = 0.7835, npv = 0.7811 @ threshold = 0.5253\n",
    "# voting_clf:    ppv = 0.7691, npv = 0.7706 @ threshold = 0.5152\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.85836\n",
    "# lgbm_groups:  0.67909\n",
    "# lr:           0.64631\n",
    "# xgb:          0.82203\n",
    "# stacking:     0.84927\n",
    "# voting_clf:   0.83851\n",
    "\n",
    "\n",
    "# v23: best so far. v22 with east_coast variable\n",
    "# \"east_coast = [\"Galveston\", \"New Orleans\", \n",
    "# \"Mobile\", \"Jacksonville\", \"Savannah\", \"Charleston\", 'Wilmington', \"Norfolk\", \"Baltimore\", \"Philadelphia\"]\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7841, npv = 0.778 @ threshold = 0.5354\n",
    "# lgbm_groups:   ppv = 0.6432, npv = 0.6432 @ threshold = 0.1212\n",
    "# lr:            ppv = 0.7473, npv = 0.6368 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7236, npv = 0.7226 @ threshold = 0.5152\n",
    "# stacking:      ppv = 0.7843, npv = 0.7784 @ threshold = 0.5253\n",
    "# voting_clf:    ppv = 0.7728, npv = 0.7706 @ threshold = 0.5253\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.85921\n",
    "# lgbm_groups:  0.66969\n",
    "# lr:           0.64629\n",
    "# xgb:          0.78917\n",
    "# stacking:     0.84483\n",
    "# voting_clf:   0.83592\n",
    "\n",
    "\n",
    "\n",
    "# v24: without lat, lon, potential_wetland, district, county, mukey\n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7711, npv = 0.7684 @ threshold = 0.5152\n",
    "# lgbm_groups:   ppv = 0.6779, npv = 0.6412 @ threshold = 0.1313\n",
    "# lr:            ppv = 0.7473, npv = 0.6368 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7458, npv = 0.7338 @ threshold = 0.5152\n",
    "# stacking:      ppv = 0.7683, npv = 0.7651 @ threshold = 0.5354\n",
    "# voting_clf:    ppv = 0.7667, npv = 0.76 @ threshold = 0.5455\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.84013\n",
    "# lgbm_groups:  0.64607\n",
    "# lr:           0.64698\n",
    "# xgb:          0.79118\n",
    "# stacking:     0.83714\n",
    "# voting_clf:   0.82432\n",
    "\n",
    "# v24.1: without lat, lon, potential_wetland, district, county, mukey with lr as meta estimator in the stacking: no improvement seen over xgb\n",
    "# \n",
    "# ppv and npv on Test:\n",
    "# lgbm:          ppv = 0.7711, npv = 0.7684 @ threshold = 0.5152\n",
    "# lgbm_groups:   ppv = 0.6779, npv = 0.6412 @ threshold = 0.1313\n",
    "# lr:            ppv = 0.7473, npv = 0.6368 @ threshold = 0.5051\n",
    "# xgb:           ppv = 0.7458, npv = 0.7338 @ threshold = 0.5152\n",
    "# stacking:      ppv = 0.7674, npv = 0.7598 @ threshold = 0.5455\n",
    "# voting_clf:    ppv = 0.7667, npv = 0.76 @ threshold = 0.5455\n",
    "# =========================================\n",
    "# ROC_AUC on Test\n",
    "# lgbm:         0.84013\n",
    "# lgbm_groups:  0.64607\n",
    "# lr:           0.64698\n",
    "# xgb:          0.79118\n",
    "# stacking:     0.83563\n",
    "# voting_clf:   0.82432\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj4LjNpHOzc9",
    "outputId": "6b01c399-d6fc-46c6-f622-ac200674afb8"
   },
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install mlxtend\n",
    "# !pip install seaborn\n",
    "# !pip install shapely\n",
    "# !pip install geopandas\n",
    "# !pip install metric_learn\n",
    "# import pickle\n",
    "\n",
    "# # !pip3 install keras\n",
    "# # !pip3 install tensorflow\n",
    "# !pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "y6CJ-2pyO-Pu"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from optimize_ppv_npv_scorer_ import optimize_ppv_npv_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# SK-learn libraries for machine learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import *\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = \"Merge_all_datasets/2021.03.31_200m_1000m\"\n",
    "FILE_VERSION = \"v26\"\n",
    "\n",
    "run_models = True # BEWARE! This overwrites the models stored on disk\n",
    "run_logistic = True\n",
    "no_lat_lon = False\n",
    "\n",
    "run_svc = False\n",
    "run_autoencoder = False\n",
    "run_knn = False\n",
    "run_svc_second_level = False\n",
    "\n",
    "# file params\n",
    "golden_models = [\"v21\", \"v22\", \"v23\", \"v24\"]\n",
    "stop_before_models = False\n",
    "\n",
    "if run_models:\n",
    "    model_dict = {}\n",
    "    file_param_dict = {}\n",
    "    random_state = 123 # for train, dev, test splits\n",
    "    dev = True\n",
    "    file_param_dict[\"random_state\"] = random_state\n",
    "    file_param_dict[\"dev\"] = dev\n",
    "    file_param_dict[\"input_file_name\"] = INPUT_FILE_NAME\n",
    "\n",
    "else:\n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    try:\n",
    "        file_param_dict = model_dict[\"file_params\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "    random_state = file_param_dict[\"random_state\"]\n",
    "    dev = file_param_dict[\"dev\"]\n",
    "    INPUT_FILE_NAME = file_param_dict[\"input_file_name\"]\n",
    "\n",
    "if FILE_VERSION in golden_models:\n",
    "    if run_models:\n",
    "        print(\"Warning: you are overwriting golden model\")\n",
    "        stop\n",
    "\n",
    "# SMOTE object\n",
    "smt = SMOTE(random_state=random_state)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1q95q-9wPEky"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14619, 495)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_pickle(\"../../../data/\" + INPUT_FILE_NAME)\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWA determinations as groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_full = pd.read_pickle(\"../../../data/\" + INPUT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_full.columns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1: 1,2,3,5\n",
    "# # 2: 4,6,7\n",
    "# # 3: 8 ,9\n",
    "\n",
    "# def check_box_grouping(x):\n",
    "#     if x.cwa1 + x.cwa2 + x.cwa3 + x.cwa5 > 0:\n",
    "#         return 1\n",
    "#     elif x.cwa4 + x.cwa6 + x.cwa7 > 0:\n",
    "#         return 2\n",
    "#     else:\n",
    "#         return 3\n",
    "    \n",
    "    \n",
    "# def check_box_grouping(x):\n",
    "#     if x.cwa1 + x.cwa3 + x.cwa5 > 0:\n",
    "#         return 1\n",
    "#     elif  x.cwa2 + x.cwa4 + x.cwa6 + x.cwa7 > 0:\n",
    "#         return 2\n",
    "#     else:\n",
    "#         return 3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full[\"cwa_determination_groups\"] = df_full.apply(lambda x: check_box_grouping(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove longitude > -50 (bad datapoints)\n",
    "\n",
    "# df_full = df_full[df_full.longitude < -50]\n",
    "# df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce East Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "east_coast = [\"Galveston\", \"New Orleans\", \"Mobile\", \"Jacksonville\", \"Savannah\", \"Charleston\", 'Wilmington', \"Norfolk\", \"Baltimore\", \"Philadelphia\"]\n",
    "# east_coast = [\"New Orleans\", \"Charleston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"east_coast\"] = df_full.apply(lambda x: \"yes\" if x.district in east_coast else \"no\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vmXibdpmoSgz"
   },
   "outputs": [],
   "source": [
    "# df_full = df_full[df_full.district != \"Alaska\"] # no SSURGO data for Alaska\n",
    "# df_full = df_full[df_full.district != \"Charleston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4OSS7EfFWLD",
    "outputId": "5cba5f6a-118e-4979-8194-000e45123b90"
   },
   "outputs": [],
   "source": [
    "# # any records where the cwa_determination is contrary to expectations?\n",
    "# good_records = (df_full.apply(lambda x: \n",
    "#                (np.sum(x.cwa1 + x.cwa2 + x.cwa3 + x.cwa4 + x.cwa5 + \n",
    "#                        x.cwa6 + x.cwa7 + x.cwa8 + x.cwa9) > 0) * 1 \n",
    "#                == x.cwa_determination, \n",
    "#                axis=1))\n",
    "\n",
    "# print(\"%good records = {}%\".format(round(np.mean(good_records) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "MUI--cUNFmXp",
    "outputId": "08a07e58-d2a0-4ead-c85a-7e9edbf94883"
   },
   "outputs": [],
   "source": [
    "# # peek at not good records\n",
    "# df_full[~good_records].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_wb_distance_m), axis=1))\n",
    "# df_full[\"closest_wb_distance_m\"] = df_full.apply(lambda x: np.log(x.closest_wb_distance_m), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_fl_distance_m), axis=1))\n",
    "# df_full[\"closest_fl_distance_m\"] = df_full.apply(lambda x: np.log(x.closest_fl_distance_m), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_wb_area_sqkm) if x.closest_wb_area_sqkm > 0 else np.nan, axis=1))\n",
    "# df_full[\"closest_wb_area_sqkm\"] = df_full.apply(lambda x: np.log(x.closest_wb_area_sqkm) if x.closest_wb_area_sqkm > 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_fl_area_sqkm) if x.closest_fl_area_sqkm > 0 else np.nan, axis=1))\n",
    "# df_full[\"closest_fl_area_sqkm\"] = df_full.apply(lambda x: np.log(x.closest_fl_area_sqkm) if x.closest_fl_area_sqkm > 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_wb_elevation) if x.closest_wb_elevation > 0 else np.nan, axis=1))\n",
    "# df_full[\"closest_wb_elevation\"] = df_full.apply(lambda x: np.log(x.closest_wb_elevation) if x.closest_wb_elevation > 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_full.apply(lambda x: np.log(x.closest_fl_elevation) if x.closest_fl_elevation > 0 else np.nan, axis=1))\n",
    "# df_full[\"closest_fl_elevation\"] = df_full.apply(lambda x: np.log(x.closest_fl_elevation) if x.closest_fl_elevation > 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified by District Train-Test1-Test2 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7768     POA-2016-00203\n",
       "7890     POA-2017-00434\n",
       "7916     POA-2018-00037\n",
       "7639     POA-1993-00223\n",
       "7702     POA-2011-01046\n",
       "              ...      \n",
       "11111    SAW-2015-01818\n",
       "12106    SAW-2019-01500\n",
       "12041    SAW-2019-00581\n",
       "11818    SAW-2017-02329\n",
       "11617    SAW-2016-02311\n",
       "Name: da_number, Length: 9991, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_strat_train_da_nums = pd.read_pickle(\"2021.04.01_train_dataset\").da_number\n",
    "df_strat_test1_da_nums = pd.read_pickle(\"2021.04.01_test1_dataset\").da_number\n",
    "df_strat_test2_da_nums = pd.read_pickle(\"2021.04.01_test2_dataset\").da_number\n",
    "df_strat_train_da_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_full.apply(lambda x: x.da_number in list(df_strat_train_da_nums), axis=1)\n",
    "test1_bool = df_full.apply(lambda x: x.da_number in list(df_strat_test1_da_nums), axis=1)\n",
    "test2_bool = df_full.apply(lambda x: x.da_number in list(df_strat_test2_da_nums), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[train_bool]\n",
    "df_dev = df_full[test1_bool]\n",
    "df_test = df_full[test2_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9991, 496)\n",
      "(2160, 496)\n",
      "(2165, 496)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_dev.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bODkeQCDPr24"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zs23ZZxPtnN"
   },
   "source": [
    "### Remove cols with all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "vBwbLX6YPwT_"
   },
   "outputs": [],
   "source": [
    "nan_cols = []\n",
    "for col in df.columns:\n",
    "  nan_frac = np.mean(df[str(col)].isna())\n",
    "  if nan_frac == 1:\n",
    "    nan_cols.append(col)\n",
    "nan_cols\n",
    "df.drop(nan_cols, inplace=True, axis=1)\n",
    "\n",
    "# two cols are removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9991, 494)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxWXqayaFhXp",
    "outputId": "52c2ceb0-fc0c-49ce-8fb4-4050e7c7f778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"county\" in df_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlZ2nLgVPOn1"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlE_M1C2PIgp",
    "outputId": "af65405c-4220-4fe4-c5b9-c922d395bfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cwa1\n",
      "1 cwa7\n",
      "2 rha1\n",
      "3 Index\n",
      "4 cwa_determination\n",
      "5 cwa3\n",
      "6 cwa9\n",
      "7 cwa6\n",
      "8 cwa2\n",
      "9 cwa5\n",
      "10 rha2\n",
      "11 cwa8\n",
      "12 potential_wetland\n",
      "13 rha_determination\n",
      "14 latitude\n",
      "15 cwa4\n",
      "16 longitude\n",
      "17 awmmfpwwta\n",
      "18 wtdepaprju\n",
      "19 aws025wta\n",
      "20 hydclprs\n",
      "21 wtdepannmi\n",
      "22 iccdcdpct\n",
      "23 niccdcd\n",
      "24 niccdcdpct\n",
      "25 brockdepmi\n",
      "26 slopegradd\n",
      "27 mukey\n",
      "28 pondfreqpr\n",
      "29 slopegradw\n",
      "30 aws0100wta\n",
      "31 iccdcd\n",
      "32 aws0150wta\n",
      "33 aws050wta\n",
      "34 urbrecptwt\n",
      "35 transition_3_200m\n",
      "36 recurrence_min_200m\n",
      "37 slope_min_200m\n",
      "38 elevation_mean_200m\n",
      "39 recurrence_max_200m\n",
      "40 seasonality_mean_200m\n",
      "41 seasonality_max_200m\n",
      "42 elevation_stdev_200m\n",
      "43 elevation_max_200m\n",
      "44 transition_8_200m\n",
      "45 recurrence_mean_200m\n",
      "46 transition_2_200m\n",
      "47 slope_max_200m\n",
      "48 elevation_min_200m\n",
      "49 slope_stdev_200m\n",
      "50 transition_1_200m\n",
      "51 transition_7_200m\n",
      "52 slope_mean_200m\n",
      "53 seasonality_min_200m\n",
      "54 transition_9_200m\n",
      "55 transition_5_200m\n",
      "56 seasonality_stdev_200m\n",
      "57 transition_0_200m\n",
      "58 transition_6_200m\n",
      "59 recurrence_stdev_200m\n",
      "60 transition_4_200m\n",
      "61 fl_ftype_artificialpath_200m\n",
      "62 fl_flow_type_sum_200m\n",
      "63 fl_ftype_coastline_200m\n",
      "64 fl_length_mean_200m\n",
      "65 fl_flow_type_count_200m\n",
      "66 fl_gnis_name_ind_sum_200m\n",
      "67 wb_ftype_canalditch_200m\n",
      "68 fl_length_count_200m\n",
      "69 fl_totdasqkm_mean_200m\n",
      "70 fl_areasqkm_count_200m\n",
      "71 fl_divergence_sum_200m\n",
      "72 wb_ftype_artificialpath_200m\n",
      "73 fl_intephem_sum_200m\n",
      "74 wb_area_mean_200m\n",
      "75 fl_ftype_pipeline_200m\n",
      "76 fl_gnis_name_ind_count_200m\n",
      "77 fl_totdasqkm_sum_200m\n",
      "78 wb_area_count_200m\n",
      "79 wb_gnis_name_ind_sum_200m\n",
      "80 wb_ftype_streamriver_200m\n",
      "81 fl_ftype_connector_200m\n",
      "82 wb_gnis_name_ind_count_200m\n",
      "83 fl_ftype_streamriver_200m\n",
      "84 fl_divergence_count_200m\n",
      "85 fl_divergence_mean_200m\n",
      "86 fl_flow_type_mean_200m\n",
      "87 fl_startflag_sum_200m\n",
      "88 fl_intephem_mean_200m\n",
      "89 fl_areasqkm_sum_200m\n",
      "90 wb_area_sum_200m\n",
      "91 wb_ftype_connector_200m\n",
      "92 wb_ftype_coastline_200m\n",
      "93 wb_gnis_name_ind_mean_200m\n",
      "94 fl_streamorde_count_200m\n",
      "95 fl_length_sum_200m\n",
      "96 fl_streamorde_mean_200m\n",
      "97 fl_intephem_count_200m\n",
      "98 wb_ftype_pipeline_200m\n",
      "99 fl_startflag_count_200m\n",
      "100 fl_streamorde_sum_200m\n",
      "101 fl_gnis_name_ind_mean_200m\n",
      "102 fl_startflag_mean_200m\n",
      "103 fl_ftype_canalditch_200m\n",
      "104 fl_areasqkm_mean_200m\n",
      "105 fl_totdasqkm_count_200m\n",
      "106 nwi_FIRST_MODIFIER_NAME_euthaline/eusaline_200m\n",
      "107 nwi_SPLIT_SUBCLASS_NAME_algal_200m\n",
      "108 nwi_CLASS_NAME_emergent_200m\n",
      "109 nwi_SUBCLASS_NAME_organic_200m\n",
      "110 nwi_SUBCLASS_NAME_coral_200m\n",
      "111 nwi_SUBCLASS_NAME_rubble_200m\n",
      "112 nwi_SPLIT_SUBCLASS_NAME_vegetated_200m\n",
      "113 nwi_SPLIT_SUBCLASS_NAME_cobble-gravel_200m\n",
      "114 nwi_SPLIT_SUBCLASS_NAME_persistent_200m\n",
      "115 nwi_SUBCLASS_NAME_vegetated_200m\n",
      "116 nwi_WATER_REGIME_NAME_regularly_flooded_200m\n",
      "117 nwi_CLASS_NAME_zzz_200m\n",
      "118 nwi_SUBCLASS_NAME_aquatic_moss_200m\n",
      "119 nwi_SUBCLASS_NAME_phragmites_australis_200m\n",
      "120 nwi_SYSTEM_NAME_lacustrine_200m\n",
      "121 nwi_CLASS_NAME_scrub-shrub_200m\n",
      "122 nwi_SUBCLASS_NAME_mollusk_200m\n",
      "123 nwi_SPLIT_CLASS_NAME_aquatic_bed_200m\n",
      "124 nwi_FIRST_MODIFIER_NAME_artificial_substrate_200m\n",
      "125 nwi_WATER_REGIME_NAME_artificially_flooded_200m\n",
      "126 nwi_WATER_REGIME_NAME_continuously__saturated_200m\n",
      "127 nwi_estuarine_and_marine_wetland_200m\n",
      "128 nwi_SPLIT_SUBCLASS_NAME_broad-leaved_evergreen_200m\n",
      "129 nwi_SUBCLASS_NAME_floating_vascular_200m\n",
      "130 nwi_SPLIT_SUBCLASS_NAME_deciduous_200m\n",
      "131 nwi_SPLIT_SUBCLASS_NAME_organic_200m\n",
      "132 nwi_SPLIT_SUBCLASS_NAME_needle-leaved_deciduous_200m\n",
      "133 nwi_FIRST_MODIFIER_NAME_oligohaline_200m\n",
      "134 nwi_SUBCLASS_NAME_algal_200m\n",
      "135 nwi_FIRST_MODIFIER_NAME_mineral_200m\n",
      "136 nwi_FIRST_MODIFIER_NAME_hyperhaline/hypersaline_200m\n",
      "137 nwi_WATER_REGIME_NAME_seasonally_flooded-tidal_200m\n",
      "138 nwi_estuarine_and_marine_deepwater_200m\n",
      "139 nwi_SUBSYSTEM_NAME_subtidal_200m\n",
      "140 nwi_CLASS_NAME_rocky_shore_200m\n",
      "141 nwi_FIRST_MODIFIER_NAME_excavated_200m\n",
      "142 nwi_SPLIT_SUBCLASS_NAME_rubble_200m\n",
      "143 nwi_SYSTEM_NAME_riverine_200m\n",
      "144 nwi_SPLIT_SUBCLASS_NAME_needle-leaved_evergreen_200m\n",
      "145 nwi_SPLIT_SUBCLASS_NAME_moss_200m\n",
      "146 nwi_WATER_REGIME_NAME_seasonally_flooded_200m\n",
      "147 nwi_SUBCLASS_NAME_rooted_vascular_200m\n",
      "148 nwi_WATER_REGIME_NAME_intermittently_exposed_200m\n",
      "149 nwi_SPLIT_CLASS_NAME_scrub-shrub_200m\n",
      "150 nwi_FIRST_MODIFIER_NAME_managed_200m\n",
      "151 nwi_SUBCLASS_NAME_sand_200m\n",
      "152 nwi_SPLIT_CLASS_NAME_moss-lichen_200m\n",
      "153 nwi_SUBCLASS_NAME_broad-leaved_deciduous_200m\n",
      "154 nwi_freshwater_pond_200m\n",
      "155 nwi_FIRST_MODIFIER_NAME_mixohaline/mixosaline_(brackish)_200m\n",
      "156 nwi_SYSTEM_NAME_marine_200m\n",
      "157 nwi_SPLIT_SUBCLASS_NAME_non_persistent_200m\n",
      "158 nwi_FIRST_MODIFIER_NAME_partially_drained/ditched_200m\n",
      "159 nwi_SUBCLASS_NAME_needle-leaved_evergreen_200m\n",
      "160 nwi_SPLIT_CLASS_NAME_emergent_200m\n",
      "161 nwi_shrub_wetland_200m\n",
      "162 nwi_CLASS_NAME_moss-lichen_200m\n",
      "163 nwi_CLASS_NAME_rock_bottom_200m\n",
      "164 nwi_SPLIT_SUBCLASS_NAME_lichen_200m\n",
      "165 nwi_feature_count_200m\n",
      "166 nwi_SPLIT_SUBCLASS_NAME_zzz_200m\n",
      "167 nwi_SUBCLASS_NAME_needle-leaved_deciduous_200m\n",
      "168 nwi_CLASS_NAME_reef_200m\n",
      "169 nwi_FIRST_MODIFIER_NAME_polyhaline_200m\n",
      "170 nwi_SPLIT_CLASS_NAME_unconsolidated_shore_200m\n",
      "171 nwi_WATER_REGIME_NAME_irregularly_exposed_200m\n",
      "172 nwi_SUBSYSTEM_NAME_littoral_200m\n",
      "173 nwi_WATER_REGIME_SUBGROUP_nontidal_200m\n",
      "174 nwi_SUBCLASS_NAME_deciduous_200m\n",
      "175 nwi_SUBSYSTEM_NAME_lower_perennial_200m\n",
      "176 nwi_CLASS_NAME_aquatic_bed_200m\n",
      "177 nwi_SUBCLASS_NAME_zzz_200m\n",
      "178 nwi_freshwater_emergent_wetland_200m\n",
      "179 nwi_SUBCLASS_NAME_moss_200m\n",
      "180 nwi_CLASS_NAME_forested_200m\n",
      "181 nwi_WATER_REGIME_NAME_permanently_flooded_200m\n",
      "182 nwi_WATER_REGIME_NAME_seasonally_flooded/saturated_200m\n",
      "183 nwi_FIRST_MODIFIER_NAME_farmed_200m\n",
      "184 nwi_SYSTEM_NAME_palustrine_200m\n",
      "185 nwi_SUBSYSTEM_NAME_tidal_200m\n",
      "186 nwi_SPLIT_CLASS_NAME_unconsolidated_bottom_200m\n",
      "187 nwi_WATER_REGIME_NAME_seasonally_saturated_200m\n",
      "188 nwi_WATER_REGIME_SUBGROUP_freshwater_tidal_200m\n",
      "189 nwi_SUBCLASS_NAME_broad-leaved_evergreen_200m\n",
      "190 nwi_SPLIT_CLASS_NAME_forested_200m\n",
      "191 nwi_FIRST_MODIFIER_NAME_beaver_200m\n",
      "192 nwi_SPLIT_CLASS_NAME_rocky_shore_200m\n",
      "193 nwi_SPLIT_SUBCLASS_NAME_sand_200m\n",
      "194 nwi_CLASS_NAME_unconsolidated_shore_200m\n",
      "195 nwi_SUBSYSTEM_NAME_limnetic_200m\n",
      "196 nwi_WATER_REGIME_NAME_zzz_200m\n",
      "197 nwi_FIRST_MODIFIER_NAME_organic_200m\n",
      "198 nwi_SUBCLASS_NAME_mud_200m\n",
      "199 nwi_WATER_REGIME_NAME_intermittently_flooded_200m\n",
      "200 nwi_WATER_REGIME_NAME_permanently_flooded-tidal_200m\n",
      "201 nwi_FIRST_MODIFIER_NAME_spoil_200m\n",
      "202 nwi_SUBSYSTEM_NAME_intermittent_200m\n",
      "203 nwi_SUBCLASS_NAME_dead_200m\n",
      "204 nwi_SPLIT_SUBCLASS_NAME_floating_vascular_200m\n",
      "205 nwi_lake_200m\n",
      "206 nwi_WATER_REGIME_NAME_semipermanently_flooded-tidal_200m\n",
      "207 nwi_WATER_REGIME_NAME_temporary_flooded_200m\n",
      "208 nwi_SPLIT_SUBCLASS_NAME_phragmites_australis_200m\n",
      "209 nwi_SUBCLASS_NAME_evergreen_200m\n",
      "210 nwi_WATER_REGIME_NAME_temporary_flooded-tidal_200m\n",
      "211 nwi_SUBSYSTEM_NAME_unknown_perennial_200m\n",
      "212 nwi_SPLIT_SUBCLASS_NAME_mollusk_200m\n",
      "213 nwi_SUBCLASS_NAME_non_persistent_200m\n",
      "214 nwi_CLASS_NAME_streambed_200m\n",
      "215 nwi_SPLIT_SUBCLASS_NAME_broad-leaved_deciduous_200m\n",
      "216 nwi_SUBSYSTEM_NAME_upper_perennial_200m\n",
      "217 nwi_freshwater_forested_200m\n",
      "218 nwi_WATER_REGIME_NAME_subtidal_200m\n",
      "219 nwi_WATER_REGIME_NAME_irregularly_flooded_200m\n",
      "220 nwi_WATER_REGIME_SUBGROUP_zzz_200m\n",
      "221 nwi_CLASS_NAME_unconsolidated_bottom_200m\n",
      "222 nwi_SPLIT_SUBCLASS_NAME_coral_200m\n",
      "223 nwi_SPLIT_SUBCLASS_NAME_aquatic_moss_200m\n",
      "224 nwi_WATER_REGIME_SUBGROUP_saltwater_tidal_200m\n",
      "225 nwi_SPLIT_CLASS_NAME_zzz_200m\n",
      "226 nwi_SYSTEM_NAME_estuarine_200m\n",
      "227 nwi_SPLIT_SUBCLASS_NAME_rooted_vascular_200m\n",
      "228 nwi_WATER_REGIME_NAME_semipermanently_flooded_200m\n",
      "229 nwi_FIRST_MODIFIER_NAME_alkaline_200m\n",
      "230 nwi_other_200m\n",
      "231 nwi_SUBCLASS_NAME_lichen_200m\n",
      "232 nwi_SUBCLASS_NAME_persistent_200m\n",
      "233 nwi_SPLIT_SUBCLASS_NAME_dead_200m\n",
      "234 nwi_riverine_200m\n",
      "235 nwi_SPLIT_CLASS_NAME_reef_200m\n",
      "236 nwi_FIRST_MODIFIER_NAME_acid_200m\n",
      "237 nwi_FIRST_MODIFIER_NAME_zzz_200m\n",
      "238 nwi_FIRST_MODIFIER_NAME_mesohaline_200m\n",
      "239 nwi_FIRST_MODIFIER_NAME_diked/impounded_200m\n",
      "240 nwi_SUBSYSTEM_NAME_intertidal_200m\n",
      "241 nwi_SUBCLASS_NAME_bedrock_200m\n",
      "242 nwi_SPLIT_SUBCLASS_NAME_evergreen_200m\n",
      "243 nwi_SPLIT_SUBCLASS_NAME_bedrock_200m\n",
      "244 nwi_SUBCLASS_NAME_cobble-gravel_200m\n",
      "245 nwi_SPLIT_SUBCLASS_NAME_mud_200m\n",
      "246 closest_fl_distance_m\n",
      "247 closest_wb_area_sqkm\n",
      "248 closest_wb_distance_m\n",
      "249 closest_wb_elevation\n",
      "250 closest_fl_area_sqkm\n",
      "251 closest_fl_elevation\n",
      "252 seasonality_stdev_1000m\n",
      "253 slope_mean_1000m\n",
      "254 seasonality_mean_1000m\n",
      "255 transition_2_1000m\n",
      "256 transition_0_1000m\n",
      "257 seasonality_min_1000m\n",
      "258 transition_5_1000m\n",
      "259 transition_1_1000m\n",
      "260 transition_8_1000m\n",
      "261 slope_min_1000m\n",
      "262 elevation_min_1000m\n",
      "263 recurrence_mean_1000m\n",
      "264 slope_stdev_1000m\n",
      "265 transition_7_1000m\n",
      "266 recurrence_max_1000m\n",
      "267 transition_6_1000m\n",
      "268 elevation_max_1000m\n",
      "269 elevation_mean_1000m\n",
      "270 transition_9_1000m\n",
      "271 seasonality_max_1000m\n",
      "272 transition_3_1000m\n",
      "273 recurrence_min_1000m\n",
      "274 slope_max_1000m\n",
      "275 recurrence_stdev_1000m\n",
      "276 transition_4_1000m\n",
      "277 elevation_stdev_1000m\n",
      "278 fl_gnis_name_ind_count_1000m\n",
      "279 fl_streamorde_mean_1000m\n",
      "280 wb_gnis_name_ind_mean_1000m\n",
      "281 fl_gnis_name_ind_sum_1000m\n",
      "282 fl_intephem_sum_1000m\n",
      "283 wb_gnis_name_ind_sum_1000m\n",
      "284 wb_ftype_artificialpath_1000m\n",
      "285 wb_ftype_pipeline_1000m\n",
      "286 wb_area_count_1000m\n",
      "287 fl_startflag_count_1000m\n",
      "288 wb_area_mean_1000m\n",
      "289 fl_length_count_1000m\n",
      "290 wb_gnis_name_ind_count_1000m\n",
      "291 fl_gnis_name_ind_mean_1000m\n",
      "292 wb_ftype_connector_1000m\n",
      "293 fl_intephem_mean_1000m\n",
      "294 fl_divergence_mean_1000m\n",
      "295 fl_ftype_artificialpath_1000m\n",
      "296 fl_areasqkm_count_1000m\n",
      "297 fl_flow_type_mean_1000m\n",
      "298 wb_ftype_streamriver_1000m\n",
      "299 fl_divergence_sum_1000m\n",
      "300 fl_areasqkm_sum_1000m\n",
      "301 fl_ftype_coastline_1000m\n",
      "302 fl_length_sum_1000m\n",
      "303 fl_totdasqkm_mean_1000m\n",
      "304 fl_streamorde_count_1000m\n",
      "305 fl_totdasqkm_count_1000m\n",
      "306 fl_flow_type_sum_1000m\n",
      "307 fl_ftype_connector_1000m\n",
      "308 wb_ftype_coastline_1000m\n",
      "309 wb_area_sum_1000m\n",
      "310 wb_ftype_canalditch_1000m\n",
      "311 fl_ftype_streamriver_1000m\n",
      "312 fl_areasqkm_mean_1000m\n",
      "313 fl_totdasqkm_sum_1000m\n",
      "314 fl_divergence_count_1000m\n",
      "315 fl_startflag_sum_1000m\n",
      "316 fl_ftype_canalditch_1000m\n",
      "317 fl_length_mean_1000m\n",
      "318 fl_intephem_count_1000m\n",
      "319 fl_streamorde_sum_1000m\n",
      "320 fl_startflag_mean_1000m\n",
      "321 fl_flow_type_count_1000m\n",
      "322 fl_ftype_pipeline_1000m\n",
      "323 nwi_SPLIT_SUBCLASS_NAME_needle-leaved_evergreen_1000m\n",
      "324 nwi_SUBCLASS_NAME_deciduous_1000m\n",
      "325 nwi_SUBSYSTEM_NAME_upper_perennial_1000m\n",
      "326 nwi_SPLIT_CLASS_NAME_unconsolidated_shore_1000m\n",
      "327 nwi_SUBCLASS_NAME_vegetated_1000m\n",
      "328 nwi_SPLIT_SUBCLASS_NAME_bedrock_1000m\n",
      "329 nwi_estuarine_and_marine_deepwater_1000m\n",
      "330 nwi_SUBCLASS_NAME_rooted_vascular_1000m\n",
      "331 nwi_WATER_REGIME_NAME_temporary_flooded-tidal_1000m\n",
      "332 nwi_feature_count_1000m\n",
      "333 nwi_SPLIT_SUBCLASS_NAME_persistent_1000m\n",
      "334 nwi_FIRST_MODIFIER_NAME_mesohaline_1000m\n",
      "335 nwi_FIRST_MODIFIER_NAME_partially_drained/ditched_1000m\n",
      "336 nwi_SUBCLASS_NAME_mud_1000m\n",
      "337 nwi_SUBCLASS_NAME_needle-leaved_evergreen_1000m\n",
      "338 nwi_SYSTEM_NAME_palustrine_1000m\n",
      "339 nwi_SPLIT_CLASS_NAME_rocky_shore_1000m\n",
      "340 nwi_SUBCLASS_NAME_bedrock_1000m\n",
      "341 nwi_SPLIT_CLASS_NAME_forested_1000m\n",
      "342 nwi_WATER_REGIME_SUBGROUP_nontidal_1000m\n",
      "343 nwi_SUBSYSTEM_NAME_tidal_1000m\n",
      "344 nwi_SUBCLASS_NAME_phragmites_australis_1000m\n",
      "345 nwi_SPLIT_SUBCLASS_NAME_lichen_1000m\n",
      "346 nwi_SUBSYSTEM_NAME_intertidal_1000m\n",
      "347 nwi_CLASS_NAME_zzz_1000m\n",
      "348 nwi_FIRST_MODIFIER_NAME_oligohaline_1000m\n",
      "349 nwi_estuarine_and_marine_wetland_1000m\n",
      "350 nwi_SUBCLASS_NAME_mollusk_1000m\n",
      "351 nwi_CLASS_NAME_aquatic_bed_1000m\n",
      "352 nwi_SUBSYSTEM_NAME_unknown_perennial_1000m\n",
      "353 nwi_WATER_REGIME_NAME_irregularly_flooded_1000m\n",
      "354 nwi_SPLIT_CLASS_NAME_unconsolidated_bottom_1000m\n",
      "355 nwi_SYSTEM_NAME_riverine_1000m\n",
      "356 nwi_FIRST_MODIFIER_NAME_managed_1000m\n",
      "357 nwi_SPLIT_SUBCLASS_NAME_cobble-gravel_1000m\n",
      "358 nwi_SPLIT_CLASS_NAME_reef_1000m\n",
      "359 nwi_SUBSYSTEM_NAME_lower_perennial_1000m\n",
      "360 nwi_CLASS_NAME_unconsolidated_shore_1000m\n",
      "361 nwi_SPLIT_SUBCLASS_NAME_vegetated_1000m\n",
      "362 nwi_lake_1000m\n",
      "363 nwi_FIRST_MODIFIER_NAME_hyperhaline/hypersaline_1000m\n",
      "364 nwi_WATER_REGIME_NAME_seasonally_flooded-tidal_1000m\n",
      "365 nwi_WATER_REGIME_SUBGROUP_saltwater_tidal_1000m\n",
      "366 nwi_SUBCLASS_NAME_cobble-gravel_1000m\n",
      "367 nwi_riverine_1000m\n",
      "368 nwi_FIRST_MODIFIER_NAME_mixohaline/mixosaline_(brackish)_1000m\n",
      "369 nwi_SPLIT_SUBCLASS_NAME_floating_vascular_1000m\n",
      "370 nwi_SUBCLASS_NAME_rubble_1000m\n",
      "371 nwi_WATER_REGIME_NAME_regularly_flooded_1000m\n",
      "372 nwi_SUBSYSTEM_NAME_littoral_1000m\n",
      "373 nwi_SUBSYSTEM_NAME_intermittent_1000m\n",
      "374 nwi_WATER_REGIME_NAME_semipermanently_flooded-tidal_1000m\n",
      "375 nwi_WATER_REGIME_SUBGROUP_freshwater_tidal_1000m\n",
      "376 nwi_SUBSYSTEM_NAME_subtidal_1000m\n",
      "377 nwi_CLASS_NAME_unconsolidated_bottom_1000m\n",
      "378 nwi_SPLIT_SUBCLASS_NAME_coral_1000m\n",
      "379 nwi_FIRST_MODIFIER_NAME_artificial_substrate_1000m\n",
      "380 nwi_SUBCLASS_NAME_aquatic_moss_1000m\n",
      "381 nwi_SPLIT_SUBCLASS_NAME_non_persistent_1000m\n",
      "382 nwi_WATER_REGIME_NAME_subtidal_1000m\n",
      "383 nwi_WATER_REGIME_NAME_temporary_flooded_1000m\n",
      "384 nwi_SPLIT_CLASS_NAME_scrub-shrub_1000m\n",
      "385 nwi_SPLIT_SUBCLASS_NAME_mollusk_1000m\n",
      "386 nwi_SUBCLASS_NAME_coral_1000m\n",
      "387 nwi_SPLIT_SUBCLASS_NAME_rooted_vascular_1000m\n",
      "388 nwi_FIRST_MODIFIER_NAME_mineral_1000m\n",
      "389 nwi_FIRST_MODIFIER_NAME_farmed_1000m\n",
      "390 nwi_FIRST_MODIFIER_NAME_polyhaline_1000m\n",
      "391 nwi_WATER_REGIME_SUBGROUP_zzz_1000m\n",
      "392 nwi_SPLIT_SUBCLASS_NAME_rubble_1000m\n",
      "393 nwi_FIRST_MODIFIER_NAME_excavated_1000m\n",
      "394 nwi_FIRST_MODIFIER_NAME_euthaline/eusaline_1000m\n",
      "395 nwi_CLASS_NAME_emergent_1000m\n",
      "396 nwi_SPLIT_SUBCLASS_NAME_phragmites_australis_1000m\n",
      "397 nwi_CLASS_NAME_forested_1000m\n",
      "398 nwi_FIRST_MODIFIER_NAME_beaver_1000m\n",
      "399 nwi_SUBCLASS_NAME_dead_1000m\n",
      "400 nwi_SPLIT_SUBCLASS_NAME_zzz_1000m\n",
      "401 nwi_SPLIT_SUBCLASS_NAME_sand_1000m\n",
      "402 nwi_WATER_REGIME_NAME_seasonally_flooded_1000m\n",
      "403 nwi_SYSTEM_NAME_estuarine_1000m\n",
      "404 nwi_WATER_REGIME_NAME_intermittently_exposed_1000m\n",
      "405 nwi_freshwater_forested_1000m\n",
      "406 nwi_SPLIT_SUBCLASS_NAME_broad-leaved_deciduous_1000m\n",
      "407 nwi_SYSTEM_NAME_marine_1000m\n",
      "408 nwi_SPLIT_SUBCLASS_NAME_needle-leaved_deciduous_1000m\n",
      "409 nwi_CLASS_NAME_scrub-shrub_1000m\n",
      "410 nwi_freshwater_pond_1000m\n",
      "411 nwi_WATER_REGIME_NAME_continuously__saturated_1000m\n",
      "412 nwi_WATER_REGIME_NAME_zzz_1000m\n",
      "413 nwi_SUBCLASS_NAME_lichen_1000m\n",
      "414 nwi_SUBCLASS_NAME_moss_1000m\n",
      "415 nwi_SUBCLASS_NAME_broad-leaved_deciduous_1000m\n",
      "416 nwi_WATER_REGIME_NAME_seasonally_flooded/saturated_1000m\n",
      "417 nwi_CLASS_NAME_moss-lichen_1000m\n",
      "418 nwi_SUBSYSTEM_NAME_limnetic_1000m\n",
      "419 nwi_FIRST_MODIFIER_NAME_zzz_1000m\n",
      "420 nwi_SPLIT_CLASS_NAME_aquatic_bed_1000m\n",
      "421 nwi_SPLIT_SUBCLASS_NAME_deciduous_1000m\n",
      "422 nwi_FIRST_MODIFIER_NAME_organic_1000m\n",
      "423 nwi_SPLIT_CLASS_NAME_emergent_1000m\n",
      "424 nwi_SPLIT_CLASS_NAME_zzz_1000m\n",
      "425 nwi_WATER_REGIME_NAME_permanently_flooded_1000m\n",
      "426 nwi_SUBCLASS_NAME_sand_1000m\n",
      "427 nwi_CLASS_NAME_streambed_1000m\n",
      "428 nwi_SUBCLASS_NAME_needle-leaved_deciduous_1000m\n",
      "429 nwi_freshwater_emergent_wetland_1000m\n",
      "430 nwi_SUBCLASS_NAME_algal_1000m\n",
      "431 nwi_CLASS_NAME_rocky_shore_1000m\n",
      "432 nwi_SPLIT_SUBCLASS_NAME_evergreen_1000m\n",
      "433 nwi_other_1000m\n",
      "434 nwi_SUBCLASS_NAME_floating_vascular_1000m\n",
      "435 nwi_FIRST_MODIFIER_NAME_spoil_1000m\n",
      "436 nwi_FIRST_MODIFIER_NAME_alkaline_1000m\n",
      "437 nwi_SPLIT_CLASS_NAME_moss-lichen_1000m\n",
      "438 nwi_SUBCLASS_NAME_evergreen_1000m\n",
      "439 nwi_WATER_REGIME_NAME_seasonally_saturated_1000m\n",
      "440 nwi_WATER_REGIME_NAME_semipermanently_flooded_1000m\n",
      "441 nwi_SUBCLASS_NAME_broad-leaved_evergreen_1000m\n",
      "442 nwi_SUBCLASS_NAME_non_persistent_1000m\n",
      "443 nwi_shrub_wetland_1000m\n",
      "444 nwi_WATER_REGIME_NAME_permanently_flooded-tidal_1000m\n",
      "445 nwi_SUBCLASS_NAME_organic_1000m\n",
      "446 nwi_WATER_REGIME_NAME_intermittently_flooded_1000m\n",
      "447 nwi_SUBCLASS_NAME_zzz_1000m\n",
      "448 nwi_SPLIT_SUBCLASS_NAME_organic_1000m\n",
      "449 nwi_SPLIT_SUBCLASS_NAME_algal_1000m\n",
      "450 nwi_WATER_REGIME_NAME_irregularly_exposed_1000m\n",
      "451 nwi_SPLIT_SUBCLASS_NAME_broad-leaved_evergreen_1000m\n",
      "452 nwi_FIRST_MODIFIER_NAME_diked/impounded_1000m\n",
      "453 nwi_SUBCLASS_NAME_persistent_1000m\n",
      "454 nwi_CLASS_NAME_reef_1000m\n",
      "455 nwi_SPLIT_SUBCLASS_NAME_dead_1000m\n",
      "456 nwi_WATER_REGIME_NAME_artificially_flooded_1000m\n",
      "457 nwi_SPLIT_SUBCLASS_NAME_moss_1000m\n",
      "458 nwi_SYSTEM_NAME_lacustrine_1000m\n",
      "459 nwi_SPLIT_SUBCLASS_NAME_mud_1000m\n",
      "460 nwi_CLASS_NAME_rock_bottom_1000m\n",
      "461 nwi_SPLIT_SUBCLASS_NAME_aquatic_moss_1000m\n",
      "462 nwi_FIRST_MODIFIER_NAME_acid_1000m\n"
     ]
    }
   ],
   "source": [
    "df_num_features = pd.DataFrame(df.describe().columns)\n",
    "for count, col in enumerate(df.describe().columns):\n",
    "  print(count, col)\n",
    "\n",
    "# 5, 7, 14, 17, 19:445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQVQGqHhGkE_"
   },
   "source": [
    "## Numerical Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viN9PWn5PY7H",
    "outputId": "25a607e3-e822-47b5-908e-ff201a1ac251"
   },
   "outputs": [],
   "source": [
    "# NOTE: for kNN which uses only lat, lon, have to pass lat, lon as first two cols\n",
    "\n",
    "if INPUT_FILE_NAME == \"Merge_all_datasets/2021.03.31_200m_1000m\":\n",
    "    # numerical features of interest: \n",
    "    ssurgo = list(set(range(17, 35)) - set([27])) # 22 = mukey\n",
    "    nhd = list(range(61, 106)) + list(range(278, 323)) \n",
    "    nwi = list(range(106, 246)) + list(range(323, 463)) \n",
    "    srtm = list(range(35, 61)) + list(range(252, 278)) \n",
    "    # closest_wb_fl = list(range(456, 462)) \n",
    "\n",
    "    if run_models:\n",
    "        if not no_lat_lon:\n",
    "            imp_num_feature_list = [14, 16, 12] + ssurgo + nhd + nwi + srtm\n",
    "        else:\n",
    "            imp_num_feature_list = ssurgo + nhd + nwi + srtm # remove lat, lon, pot wetland\n",
    "\n",
    "        imp_num_feature = df_num_features.loc[imp_num_feature_list]\n",
    "        imp_num_feature = list(imp_num_feature.values.flatten())\n",
    "        file_param_dict[\"imp_num_feature\"] = imp_num_feature\n",
    "    else:\n",
    "        imp_num_feature = file_param_dict[\"imp_num_feature\"]\n",
    "        \n",
    "        \n",
    "if INPUT_FILE_NAME == \"Merge_all_datasets/2021.03.31_1000m_2500m\":\n",
    "    # numerical features of interest: \n",
    "    ssurgo = list(set(range(17, 34)) - set([22])) # 22 = mukey\n",
    "    nhd = list(range(51, 94)) + list(range(241, 284)) \n",
    "    nwi = list(range(94, 233)) + list(range(284, 424)) \n",
    "    srtm = list(range(34, 51)) + list(range(233, 241)) \n",
    "    # closest_wb_fl = list(range(456, 462)) \n",
    "\n",
    "    if run_models:\n",
    "        imp_num_feature_list = [2, 9, 10] + ssurgo + nhd + nwi + srtm\n",
    "\n",
    "        imp_num_feature = df_num_features.loc[imp_num_feature_list]\n",
    "        imp_num_feature = list(imp_num_feature.values.flatten())\n",
    "        file_param_dict[\"imp_num_feature\"] = imp_num_feature\n",
    "    else:\n",
    "        imp_num_feature = file_param_dict[\"imp_num_feature\"]        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfKbTgYTGohQ"
   },
   "source": [
    "## Categorical Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'county',\n",
       " 'da_number',\n",
       " 'date_issued_or_denied',\n",
       " 'district',\n",
       " 'drclassdcd',\n",
       " 'drclasswet',\n",
       " 'east_coast',\n",
       " 'engcmssdcd',\n",
       " 'engcmssmp',\n",
       " 'engdwbdcd',\n",
       " 'engdwbll',\n",
       " 'engdwbml',\n",
       " 'engdwobdcd',\n",
       " 'englrsdcd',\n",
       " 'engsldcd',\n",
       " 'engsldcp',\n",
       " 'engstafdcd',\n",
       " 'engstafll',\n",
       " 'engstafml',\n",
       " 'flodfreqdc',\n",
       " 'flodfreqma',\n",
       " 'forpehrtdc',\n",
       " 'huc4',\n",
       " 'huc6',\n",
       " 'hydgrpdcd',\n",
       " 'jurisdiction_type',\n",
       " 'mustatus',\n",
       " 'nwi_wetland_list_1000m',\n",
       " 'nwi_wetland_list_200m',\n",
       " 'state',\n",
       " 'urbrecptdc'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek at categorical features\n",
    "set(df.columns) - set(df.describe().columns)\n",
    "# len(set(df.describe().columns))\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "n5JCwDs_Q8pW"
   },
   "outputs": [],
   "source": [
    "# call out the important categorical features\n",
    "\n",
    "\n",
    "# imp_cat_feature = ['district', 'flodfreqdc', 'drclassdcd', 'county', 'jurisdiction_type'] # baseline\n",
    "# # imp_cat_feature = ['flodfreqdc', 'drclassdcd','jurisdiction_type'] # v9.5 \n",
    "\n",
    "# if run_models:\n",
    "#     imp_cat_feature = ['county',\n",
    "#      'district',\n",
    "#      'drclassdcd',\n",
    "#      'drclasswet',\n",
    "#      'engcmssdcd',\n",
    "#      'engcmssmp',\n",
    "#      'engdwbdcd',\n",
    "#      'engdwbll',\n",
    "#      'engdwbml',\n",
    "#      'engdwobdcd',\n",
    "#      'englrsdcd',\n",
    "#      'engsldcd',\n",
    "#      'engsldcp',\n",
    "#      'engstafdcd',\n",
    "#      'engstafll',\n",
    "#      'engstafml',\n",
    "#      'flodfreqdc',\n",
    "#      'flodfreqma',\n",
    "#      'forpehrtdc',\n",
    "#      'huc4',\n",
    "#      'hydgrpdcd',\n",
    "#      'jurisdiction_type',\n",
    "#      'state',\n",
    "#      'urbrecptdc',\n",
    "#      'east_coast']\n",
    "#     file_param_dict[\"imp_cat_feature\"] = imp_cat_feature\n",
    "    \n",
    "#remove district and county\n",
    "if run_models: \n",
    "    imp_cat_feature = ['drclassdcd',\n",
    "     'drclasswet',\n",
    "     'engcmssdcd',\n",
    "     'engcmssmp',\n",
    "     'engdwbdcd',\n",
    "     'engdwbll',\n",
    "     'engdwbml',\n",
    "     'engdwobdcd',\n",
    "     'englrsdcd',\n",
    "     'engsldcd',\n",
    "     'engsldcp',\n",
    "     'engstafdcd',\n",
    "     'engstafll',\n",
    "     'engstafml',\n",
    "     'flodfreqdc',\n",
    "     'flodfreqma',\n",
    "     'forpehrtdc',\n",
    "     'huc4',\n",
    "     'hydgrpdcd',\n",
    "     'jurisdiction_type',\n",
    "     'state',\n",
    "     'urbrecptdc',\n",
    "     'east_coast']\n",
    "    file_param_dict[\"imp_cat_feature\"] = imp_cat_feature    \n",
    "    \n",
    "else:\n",
    "    imp_cat_feature = file_param_dict[\"imp_cat_feature\"]\n",
    "    \n",
    "# imp_cat_feature = ['jurisdiction_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minority Oversamping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smt = SMOTE(random_state=random_state)\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VweeykE-4Ter"
   },
   "source": [
    "# Order Train-Dev-Test splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqbXGvZIRHGi",
    "outputId": "6238c23e-8db0-4cc0-fe3c-71f547ef6e6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2165, 465)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-arrange so numerical columns go first, then the categorical\n",
    "df1 = df[imp_num_feature]\n",
    "df2 = df[imp_cat_feature]\n",
    "\n",
    "# train\n",
    "df_X_combined_ordered = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# dev\n",
    "df_dev_X = pd.concat([df_dev[imp_num_feature], df_dev[imp_cat_feature]], axis=1)\n",
    "\n",
    "\n",
    "# test\n",
    "df_test_X = pd.concat([df_test[imp_num_feature], df_test[imp_cat_feature]], axis=1)\n",
    "\n",
    "\n",
    "df_X_combined_ordered.columns #44\n",
    "df_X_combined_ordered.shape # (10000, 44)\n",
    "df_test_X.shape # (4500, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdBKul2wROi4",
    "outputId": "fb607dae-a36a-4e9b-e4be-eb54ae133426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([34.,  0.,  2.,  4.,  5.,  0.,  1.,  9.,  7., 11.]),\n",
       " array([0.02161946, 0.11896707, 0.21631468, 0.3136623 , 0.41100991,\n",
       "        0.50835752, 0.60570513, 0.70305275, 0.80040036, 0.89774797,\n",
       "        0.99509559]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fraction of nan's in each numerical variable\n",
    "count = 0\n",
    "errors = 0\n",
    "nan_dist = []\n",
    "nan_vars = []\n",
    "for var in df_X_combined_ordered.describe().columns:\n",
    "    try:\n",
    "        if np.mean(df_X_combined_ordered[str(var)].isna()) != 0:\n",
    "            nan_vars.append(var)\n",
    "            count += 1\n",
    "            nan_dist.append(np.mean(df_X_combined_ordered[str(var)].isna()))\n",
    "#             print(var, round(np.mean(df_X_combined_ordered[str(var)].isna()), 2))\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(var, \"<-------------------\")\n",
    "print(count, errors    )\n",
    "plt.hist(nan_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute nearest 10 neighbor average for all nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(lat, lon, value, var):\n",
    "    if np.isnan(value):\n",
    "        return knn_model_dict[var].predict(pd.DataFrame([[lat, lon]], columns=[\"latitude\", \"longitude\"]))[0]\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>potential_wetland</th>\n",
       "      <th>awmmfpwwta</th>\n",
       "      <th>wtdepaprju</th>\n",
       "      <th>aws025wta</th>\n",
       "      <th>hydclprs</th>\n",
       "      <th>wtdepannmi</th>\n",
       "      <th>iccdcdpct</th>\n",
       "      <th>niccdcd</th>\n",
       "      <th>...</th>\n",
       "      <th>engstafml</th>\n",
       "      <th>flodfreqdc</th>\n",
       "      <th>flodfreqma</th>\n",
       "      <th>forpehrtdc</th>\n",
       "      <th>huc4</th>\n",
       "      <th>hydgrpdcd</th>\n",
       "      <th>jurisdiction_type</th>\n",
       "      <th>state</th>\n",
       "      <th>urbrecptdc</th>\n",
       "      <th>east_coast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.85821</td>\n",
       "      <td>-76.70773</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Slight</td>\n",
       "      <td>0414</td>\n",
       "      <td>A/D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>36</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.15230</td>\n",
       "      <td>-75.85524</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>0414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>36</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.68911</td>\n",
       "      <td>-78.04046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>0413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>36</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.97994</td>\n",
       "      <td>-78.77134</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Slight</td>\n",
       "      <td>0412</td>\n",
       "      <td>C/D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>36</td>\n",
       "      <td>Somewhat limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.21616</td>\n",
       "      <td>-78.97142</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Slight</td>\n",
       "      <td>0413</td>\n",
       "      <td>C/D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>36</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>26.29676</td>\n",
       "      <td>-98.35956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1211</td>\n",
       "      <td>B</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>48</td>\n",
       "      <td>Somewhat limited</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14610</th>\n",
       "      <td>27.61664</td>\n",
       "      <td>-97.22171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>1211</td>\n",
       "      <td>D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>48</td>\n",
       "      <td>Not rated</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>34.40516</td>\n",
       "      <td>-92.14308</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>Frequent</td>\n",
       "      <td>Frequent</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1111</td>\n",
       "      <td>C</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>05</td>\n",
       "      <td>Somewhat limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>36.12439</td>\n",
       "      <td>-95.55320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Severe</td>\n",
       "      <td>1107</td>\n",
       "      <td>D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>40</td>\n",
       "      <td>Somewhat limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14617</th>\n",
       "      <td>35.65544</td>\n",
       "      <td>-97.58169</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very limited</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1105</td>\n",
       "      <td>D</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>40</td>\n",
       "      <td>Somewhat limited</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9991 rows × 465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude  longitude  potential_wetland  awmmfpwwta  wtdepaprju  \\\n",
       "0      42.85821  -76.70773                  1         1.0         0.0   \n",
       "1      43.15230  -75.85524                  0         0.0         0.0   \n",
       "2      42.68911  -78.04046                  0         0.0         0.0   \n",
       "3      42.97994  -78.77134                  1         1.0        54.0   \n",
       "4      43.21616  -78.97142                  1         1.0         0.0   \n",
       "...         ...        ...                ...         ...         ...   \n",
       "14607  26.29676  -98.35956                  0         0.0         0.0   \n",
       "14610  27.61664  -97.22171                  0         0.0         0.0   \n",
       "14612  34.40516  -92.14308                  1         1.0         0.0   \n",
       "14616  36.12439  -95.55320                  0         1.0         0.0   \n",
       "14617  35.65544  -97.58169                  1         1.0         0.0   \n",
       "\n",
       "       aws025wta  hydclprs  wtdepannmi  iccdcdpct  niccdcd  ...     engstafml  \\\n",
       "0           7.40     100.0         0.0      100.0      8.0  ...  Very limited   \n",
       "1           0.00       0.0         0.0      100.0      NaN  ...     Not rated   \n",
       "2           0.00       0.0         0.0      100.0      NaN  ...     Not rated   \n",
       "3           2.25       5.0        54.0      100.0      2.0  ...  Very limited   \n",
       "4           5.20      93.0         0.0      100.0      4.0  ...  Very limited   \n",
       "...          ...       ...         ...        ...      ...  ...           ...   \n",
       "14607       3.25       0.0         0.0       85.0      2.0  ...     Not rated   \n",
       "14610       0.00       0.0         0.0      100.0      8.0  ...     Not rated   \n",
       "14612       4.75      15.0         0.0      100.0      4.0  ...  Very limited   \n",
       "14616       3.61       0.0         0.0      100.0      7.0  ...  Very limited   \n",
       "14617       4.45       0.0         0.0      100.0      3.0  ...  Very limited   \n",
       "\n",
       "       flodfreqdc  flodfreqma  forpehrtdc  huc4  hydgrpdcd  jurisdiction_type  \\\n",
       "0            None        None      Slight  0414        A/D            RAPANOS   \n",
       "1             NaN         NaN   Not rated  0414        NaN            RAPANOS   \n",
       "2             NaN         NaN   Not rated  0413        NaN            RAPANOS   \n",
       "3            None        None      Slight  0412        C/D            RAPANOS   \n",
       "4            None        None      Slight  0413        C/D            RAPANOS   \n",
       "...           ...         ...         ...   ...        ...                ...   \n",
       "14607        None        None      Slight  1211          B            RAPANOS   \n",
       "14610        None        None   Not rated  1211          D            RAPANOS   \n",
       "14612    Frequent    Frequent      Slight  1111          C            RAPANOS   \n",
       "14616        None        None      Severe  1107          D            RAPANOS   \n",
       "14617        None        None    Moderate  1105          D            RAPANOS   \n",
       "\n",
       "       state        urbrecptdc  east_coast  \n",
       "0         36      Very limited          no  \n",
       "1         36         Not rated          no  \n",
       "2         36         Not rated          no  \n",
       "3         36  Somewhat limited          no  \n",
       "4         36      Very limited          no  \n",
       "...      ...               ...         ...  \n",
       "14607     48  Somewhat limited         yes  \n",
       "14610     48         Not rated         yes  \n",
       "14612     05  Somewhat limited          no  \n",
       "14616     40  Somewhat limited          no  \n",
       "14617     40  Somewhat limited          no  \n",
       "\n",
       "[9991 rows x 465 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_combined_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model_dict = {}\n",
    "\n",
    "if not no_lat_lon:\n",
    "    for var in nan_vars:\n",
    "        knn = KNeighborsRegressor(n_neighbors=30)\n",
    "        temp_X = df_X_combined_ordered[~df_X_combined_ordered[var].isna()][[\"latitude\", \"longitude\"]]\n",
    "        temp_Y = df_X_combined_ordered[var][~df_X_combined_ordered[var].isna()]\n",
    "        knn.fit(temp_X, temp_Y)\n",
    "        knn_model_dict[var] = knn\n",
    "        df_X_combined_ordered[var] = df_X_combined_ordered.apply(lambda x: impute_nan(x.latitude, x.longitude, x[var], var), axis=1)\n",
    "        df_dev_X[var] = df_dev_X.apply(lambda x: impute_nan(x.latitude, x.longitude, x[var], var), axis=1)\n",
    "        df_test_X[var] = df_test_X.apply(lambda x: impute_nan(x.latitude, x.longitude, x[var], var), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4kq__Giqima"
   },
   "outputs": [],
   "source": [
    "# impute 0's into wb_area_mean, fl_length_sum, fl_length_mean because they were\n",
    "# assigned np.nan if they were absent\n",
    "# A non-existent water feature should be assigned 0 given definition of each\n",
    "\n",
    "def fill_na(df):\n",
    "  try:\n",
    "    df.fl_length_sum_200m = df.fl_length_sum_200m.fillna(0)\n",
    "    df.fl_length_mean_200m = df.fl_length_sum_200m.fillna(0)\n",
    "    df.fl_length_sum_2500m = df.fl_length_sum_200m.fillna(0)\n",
    "    df.fl_length_mean_2500m = df.fl_length_sum_200m.fillna(0)\n",
    "  except:\n",
    "    pass\n",
    "  return df\n",
    "\n",
    "# No need for fill_na() since imputing by knn\n",
    "\n",
    "if no_lat_lon:\n",
    "    df_X_combined_ordered = fill_na(df_X_combined_ordered)\n",
    "    df_dev_X_combined_ordered = fill_na(df_dev_X)\n",
    "    df_test_X_combined_ordered = fill_na(df_test_X)\n",
    "else:\n",
    "    df_X_combined_ordered = df_X_combined_ordered\n",
    "    df_dev_X_combined_ordered = df_dev_X\n",
    "    df_test_X_combined_ordered = df_test_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qy2KEjaNq487",
    "outputId": "077aa965-5d64-4ad9-abc7-001580eb74d8"
   },
   "outputs": [],
   "source": [
    "# fraction of nan's in each variable\n",
    "def print_na(df_X_combined_ordered):\n",
    "  for var in df_X_combined_ordered.describe().columns:\n",
    "    try:\n",
    "        if np.mean(df_X_combined_ordered[str(var)].isna()) != 0:\n",
    "          print(var, round(np.mean(df_X_combined_ordered[str(var)].isna()), 2))\n",
    "    except Exception as e:\n",
    "        print(var)\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "print(\"Train\")    \n",
    "print_na(df_X_combined_ordered)      \n",
    "print()\n",
    "print(\"Dev\")\n",
    "print_na(df_dev_X_combined_ordered)\n",
    "print()\n",
    "print(\"Test\")\n",
    "print_na(df_test_X_combined_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiZZiHI7rroC"
   },
   "source": [
    "# Offline OHE to keep track of variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wgvKVZ7rNb8"
   },
   "outputs": [],
   "source": [
    "# ohe-hot-encode the columns\n",
    "# get_dummies only encodes cat columns\n",
    "df_X_combined_dummies_ordered = pd.get_dummies(df_X_combined_ordered)\n",
    "# df_X_combined_dummies_ordered.columns # 90\n",
    "\n",
    "df_dev_X_combined_dummies_ordered = pd.get_dummies(df_dev_X_combined_ordered)\n",
    "df_test_X_combined_dummies_ordered = pd.get_dummies(df_test_X_combined_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2DzQYz-guOe",
    "outputId": "d15cc667-b613-4161-fdee-cb63bd03ce29"
   },
   "outputs": [],
   "source": [
    "print(df_X_combined_ordered.shape)\n",
    "print(df_dev_X_combined_ordered.shape)\n",
    "print(df_test_X_combined_ordered.shape)\n",
    "print(df_X_combined_dummies_ordered.shape)\n",
    "print(df_dev_X_combined_dummies_ordered.shape)\n",
    "print(df_test_X_combined_dummies_ordered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1Eq49x-r55M"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoBhg5wPrxlx"
   },
   "outputs": [],
   "source": [
    "# impute categorical data\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "  \"\"\"\n",
    "  By inheriting TransformerMixin, you get fit_transform method for free \n",
    "  if you implement fit and transform methods\n",
    "  \"\"\" \n",
    "\n",
    "  def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with median of column.\n",
    "        \"\"\"\n",
    "  def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X], \n",
    "            index=X.columns)\n",
    "        return self\n",
    "\n",
    "  def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E_VtoXpr8W9"
   },
   "outputs": [],
   "source": [
    "# Pipeline for numerical columns\n",
    "# 1. fill NA's with median values\n",
    "# 2. scale them\n",
    "\n",
    "# num_pipeline_impute_ss = Pipeline([        # should be list of tuples\n",
    "#                           (\"num_imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#                           (\"std_scaler\", StandardScaler())\n",
    "#                           ])                      \n",
    "\n",
    "# num_pipeline_impute_ss = Pipeline([        # should be list of tuples\n",
    "#                           (\"num_imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#                           (\"robust_scaler\", RobustScaler())\n",
    "#                           ])                      \n",
    "\n",
    "num_pipeline_impute_ss = Pipeline([        # should be list of tuples\n",
    "                          (\"num_imputer\", SimpleImputer(strategy=\"median\"))\n",
    "                          ])                      \n",
    "\n",
    "\n",
    "# Pipleline for categorical columns\n",
    "# 1. fill NA's with most frequent values\n",
    "# 2. one hot code\n",
    "\n",
    "# cat_pipeline_impute_ohe = Pipeline([(\"cat_imputer\", DataFrameImputer()),\n",
    "#                          (\"one_hot_encoder\", OneHotEncoder(drop=\"first\", \\\n",
    "#                                                            sparse=False))\n",
    "#                          ])\n",
    "\n",
    "\n",
    "# you want to do the following where you handle_unknown categories in the \n",
    "# test data by ignoring them. However, in the imeplementation, I am using\n",
    "# df_X_combined_dummies_ordered to indicate the numerical and cat columns \n",
    "# hence need to fix the df_X_combined_dummies_ordered such that the first \n",
    "# ohe is not dropped (as is being done in immediately above)\n",
    "\n",
    "cat_pipeline_impute_ohe = Pipeline([(\"cat_imputer\", DataFrameImputer()),\n",
    "                         (\"one_hot_encoder\", OneHotEncoder(sparse=False,\n",
    "                                                           handle_unknown = \"ignore\"))\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6zsqDgLsIhp"
   },
   "outputs": [],
   "source": [
    "numericals_list = list(df_X_combined_ordered.describe().columns)\n",
    "categories_list = list(set(df_X_combined_ordered.columns) - set(numericals_list))\n",
    "\n",
    "# here trying to do numerical and categorical transformation in isolation\n",
    "# this because ColumnTransformer removes column name information :-(\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# only the cat columns will be one-hot encoded\n",
    "partial_transformer_impute_ohe = ColumnTransformer([\n",
    "                                   (\"categorical_ohe\", cat_pipeline_impute_ohe,\\\n",
    "                                    categories_list)\n",
    "])\n",
    "\n",
    "# only the numerical columns withh get standard scaling\n",
    "partial_transformer_impute_ss = ColumnTransformer([\n",
    "                                   (\"numerical_ss_impute\", num_pipeline_impute_ss,\\\n",
    "                                    numericals_list)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of Dev and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DxNTfIFtT9y",
    "outputId": "5d8a5f9d-f079-43d8-b8db-65afc2d25027"
   },
   "outputs": [],
   "source": [
    "# Pass the numerical columns through Numerical Pipeline \n",
    "\n",
    "# train\n",
    "full_data_ohe_ss_imputed = (partial_transformer_impute_ss\n",
    "                            .fit(df_X_combined_ordered[numericals_list])\n",
    "                            .transform(df_X_combined_ordered[numericals_list])) \n",
    "print(full_data_ohe_ss_imputed.shape)\n",
    "\n",
    "# dev\n",
    "dev_ohe_ss_imputed = (partial_transformer_impute_ss\n",
    "                            .fit(df_X_combined_ordered[numericals_list])\n",
    "                            .transform(df_dev_X_combined_ordered[numericals_list])) \n",
    "print(dev_ohe_ss_imputed.shape)\n",
    "\n",
    "\n",
    "# test\n",
    "test_ohe_ss_imputed = (partial_transformer_impute_ss\n",
    "                            .fit(df_X_combined_ordered[numericals_list])\n",
    "                            .transform(df_test_X_combined_ordered[numericals_list])) \n",
    "print(test_ohe_ss_imputed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQD-xNjqtUA9",
    "outputId": "9bf01104-653e-4175-e099-3c7a0296e530"
   },
   "outputs": [],
   "source": [
    "# Pass the cat columns through Categorical Pipeline\n",
    "\n",
    "# train\n",
    "cat_data_OHE = (partial_transformer_impute_ohe\n",
    "                .fit(df_X_combined_ordered)\n",
    "                .transform(df_X_combined_ordered))\n",
    "print(cat_data_OHE.shape)\n",
    "\n",
    "# test\n",
    "dev_cat_data_OHE = (partial_transformer_impute_ohe\n",
    "                .fit(df_X_combined_ordered)\n",
    "                .transform(df_dev_X_combined_ordered))\n",
    "print(dev_cat_data_OHE.shape)\n",
    "\n",
    "# test\n",
    "test_cat_data_OHE = (partial_transformer_impute_ohe\n",
    "                .fit(df_X_combined_ordered)\n",
    "                .transform(df_test_X_combined_ordered))\n",
    "print(test_cat_data_OHE.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy X and Y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgK2Nv2HycZo",
    "outputId": "9af08aa1-945f-49a2-a36b-42064875d6d6"
   },
   "outputs": [],
   "source": [
    "# join the arrays into one array that can be passed into models\n",
    "\n",
    "# train\n",
    "X = np.hstack((full_data_ohe_ss_imputed, cat_data_OHE))\n",
    "Y = np.array(df.cwa_determination)\n",
    "Y_groups = np.array(df.cwa_determination_groups)\n",
    "\n",
    "# dev\n",
    "dev_X = np.hstack((dev_ohe_ss_imputed, dev_cat_data_OHE))\n",
    "dev_Y = np.array(df_dev.cwa_determination)\n",
    "dev_Y_groups = np.array(df_dev.cwa_determination_groups)\n",
    "\n",
    "# test\n",
    "test_X = np.hstack((test_ohe_ss_imputed, test_cat_data_OHE))\n",
    "test_Y = np.array(df_test.cwa_determination)\n",
    "test_Y_groups = np.array(df_test.cwa_determination_groups)\n",
    "\n",
    "print(X.shape, Y.shape, dev_X.shape, dev_Y.shape, test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNnvAK30tUDx"
   },
   "outputs": [],
   "source": [
    "# Convert numerical and cat transforms back to dataframe (for housekeeping)\n",
    "\n",
    "# convert numerical arrays into dataframe\n",
    "\n",
    "def make_dataframe(full_data_ohe_ss_imputed, cat_data_OHE):\n",
    "  df_num_data_ohe_ss = (pd.DataFrame(\n",
    "      full_data_ohe_ss_imputed,\n",
    "      columns=list(df_X_combined_dummies_ordered[numericals_list].columns)\n",
    "  ))\n",
    "\n",
    "  # # convert cat arrays into dataframe\n",
    "  ohe_categories_list = (list(set(df_X_combined_dummies_ordered.columns) - set(numericals_list)))\n",
    "  df_cat_data_OHE = (pd.DataFrame(\n",
    "      cat_data_OHE,\n",
    "      columns=list(df_X_combined_dummies_ordered[ohe_categories_list].columns))\n",
    "  )\n",
    "\n",
    "  # concatenate into one dataframe\n",
    "\n",
    "  return pd.concat([df_num_data_ohe_ss, df_cat_data_OHE], axis=1)\n",
    "\n",
    "\n",
    "df_train_X_dummies = make_dataframe(full_data_ohe_ss_imputed, cat_data_OHE)\n",
    "df_dev_X_dummies = make_dataframe(dev_ohe_ss_imputed, dev_cat_data_OHE)\n",
    "df_test_X_dummies = make_dataframe(test_ohe_ss_imputed, test_cat_data_OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY1gOlOk5-ZM"
   },
   "outputs": [],
   "source": [
    "if stop_before_models:\n",
    "    stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "if False: # True if you want to run this\n",
    "    # define inliers as negatives and outliers as positive examples\n",
    "    inliers = Y == 0\n",
    "    outliers = Y == 1\n",
    "    X_inliers = X[inliers]\n",
    "    Y_inliers = Y[inliers]\n",
    "\n",
    "    clf = OneClassSVM(gamma='auto').fit(X_inliers)\n",
    "    \n",
    "    # predict on filtered train, train and dev data\n",
    "    X_inliers_predict = clf.predict(X_inliers)\n",
    "    X_predict = clf.predict(X)\n",
    "    dev_predict = clf.predict(dev_X)\n",
    "    \n",
    "    # transform on train and dev data\n",
    "    train_score_samples = clf.score_samples(X)\n",
    "    dev_score_samples = clf.score_samples(dev_X)\n",
    "\n",
    "    # replace 1's by 0's (1 of OneClassSVM is the inlier or the majority class which is 0)\n",
    "    # replace 1's by -1's (define outliers as minority class)\n",
    "    dev_predict[dev_predict == 1] = 0\n",
    "    dev_predict[dev_predict == -1] = 1\n",
    "    print(np.mean(dev_predict == dev_Y)) # 0.38839590443686006\n",
    "    \n",
    "    # do same on filtered X data\n",
    "    X_inliers_predict[X_inliers_predict == 1] = 0\n",
    "    X_inliers_predict[X_inliers_predict == -1] = 1\n",
    "    print(np.mean(X_inliers_predict == Y_inliers)) # 0.5502357635110627\n",
    "\n",
    "    # do same on train data\n",
    "    X_predict[X_predict == 1] = 0\n",
    "    X_predict[X_predict == -1] = 1\n",
    "    print(np.mean(X_predict == Y)) # 0.708762296957218\n",
    "\n",
    "    plt.hist(dev_score_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "# dist = DistanceMetric.get_metric(\"mahalanobis\", V=cov.get_mahalanobis_matrix())\n",
    "# dist = DistanceMetric.get_metric(\"mahalanobis\", V=np.cov(X))\n",
    "# dist.pairwise(X)\n",
    "\n",
    "# np.linalg.det(np.cov(X))\n",
    "# np.linalg.det(np.linalg.pinv(np.cov(X)))\n",
    "# np.linalg.cond(X)\n",
    "# np.linalg.pinv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from metric_learn import Covariance\n",
    "# from sklearn.datasets import load_iris\n",
    "# iris = load_iris()['data']\n",
    "# cov = Covariance().fit(iris)\n",
    "# x = cov.transform(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P2(n_components, data): # from Project 3!\n",
    "  \"\"\"\n",
    "  Takes target dimensionality reduction (k) and the data to reduce\n",
    "  Returns the reduced data\n",
    "  \"\"\"\n",
    "  \n",
    "  pca = PCA(n_components)\n",
    "  pca.fit(data)\n",
    "  return pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # True if you want to run this\n",
    "    n_components = 475 # maximum is X.shape[1]\n",
    "    d = 2\n",
    "    fig, axes = plt.subplots(d, d, figsize=(10, 10))\n",
    "    # Dimension reduction\n",
    "    pca = P2(n_components=n_components, data=X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "    import seaborn as sns\n",
    "    colors = [\"g\", \"r\"]\n",
    "\n",
    "    for i in range(d**2):\n",
    "        pc_i, pc_i_1 = X[:, i], X[:, i+1]\n",
    "        sns.scatterplot(ax=axes[i//d, i%d], x=pc_i, y=pc_i_1, hue=np.array(Y).flatten())\n",
    "        axes[i//d, i%d].set_xlabel(\"PC\" + str(i+1))\n",
    "        axes[i//d, i%d].set_ylabel(\"PC\" + str(i+2))\n",
    "        \n",
    "    dev_X = pca.transform(dev_X)\n",
    "    test_X = pca.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZOqLgq5EsPm"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmWtOA8lEThm",
    "outputId": "3a4a38ac-dfe3-4cc8-ddd0-80ae86b3eec8"
   },
   "outputs": [],
   "source": [
    "# print(sorted(metrics.SCORERS.keys()))\n",
    "# sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search.__dir__()\n",
    "# random_search.return_train_score\n",
    "\n",
    "# random_search.scoring # roc_auc\n",
    "# random_search.best_score_ # \n",
    "# random_search.scorer_ # make_scorer(roc_auc_score, needs_threshold=True)\n",
    "\n",
    "# random_search.cv_results_\n",
    "# random_search.predict_proba(X)\n",
    "# random_search.predict_log_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_taken(start, end):\n",
    "    delta = end - start\n",
    "    print(\"Time taken (min):\", round(delta.seconds/60, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(fitted_model, test_X, test_Y, model_name):\n",
    "\n",
    "    y_predict_proba = fitted_model.predict_proba(test_X)[:, 1]\n",
    "    pv = ppv_npv_opt_th(test_Y, y_predict_proba)\n",
    "#     print(\"{}: {}ppv = {}, npv = {}\".format(fitted_model.estimator, \" \"*(13 - len(str(fitted_model.estimator))), round(pv[0], 4), round(pv[1], 4)))\n",
    "    print(\"{}: {}ppv = {}, npv = {} @ threshold = {}\".format(model_name, \" \"*(13 - len(model_name)), round(pv[0], 4), round(pv[1], 4), round(pv[2], 4)))\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "    # AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold\n",
    "    print(\"average_precision_score:\", round(metrics.average_precision_score(test_Y, fitted_model.predict_proba(test_X)[:, 1], average=\"weighted\"), 5))\n",
    "        \n",
    "    y_prob = fitted_model.predict_proba(test_X)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_Y, y_prob[:, 1], pos_label=1)\n",
    "    print(\"roc_auc\",\":\", round(metrics.auc(fpr, tpr), 5))\n",
    "        \n",
    "    print(\"Classification Report:\") # threshold agnostic because you pass in the test labels instead of scores (probabilities)\n",
    "    print(classification_report(test_Y, fitted_model.predict(test_X)))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_Y, fitted_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_roc_auc(fitted_model, test_X, test_Y):\n",
    "    y_prob = fitted_model.predict_proba(test_X)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_Y, y_prob[:, 1], pos_label=1)\n",
    "    return round(metrics.auc(fpr, tpr), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppv_npv_opt_th(y_true=np.array([1,0,1]), y_predict_proba=np.array([0.5, 0.25, 0.3])):\n",
    "    \"\"\"\n",
    "    Inputs: y_true labels and prediction scores\n",
    "    Outputs: optimized positive predictive value and negative predictive values per this reference\n",
    "    https://arxiv.org/pdf/2007.05073.pdf\n",
    "    \"\"\"\n",
    "    min_ppv_npv_list = []\n",
    "    th_list = np.linspace(0, 1, 100)\n",
    "    for th in th_list:\n",
    "        y_predict = 1 * (y_predict_proba > th)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "        ppv = tp / (tp + fp) \n",
    "        npv = tn / (fn + tn)\n",
    "        min_ppv_npv = np.min(np.nan_to_num(np.array((ppv, npv))))\n",
    "        min_ppv_npv_list.append(min_ppv_npv)\n",
    "    max_ppv_npv = np.nanmax(np.array(min_ppv_npv_list))\n",
    "    opt_th_index = np.array(min_ppv_npv_list).argmax(axis=0)\n",
    "    opt_th = th_list[opt_th_index]\n",
    "    opt_y_predict = 1 * (y_predict_proba > opt_th)\n",
    "    opt_tn, opt_fp, opt_fn, opt_tp = confusion_matrix(y_true, opt_y_predict).ravel()\n",
    "    opt_ppv = opt_tp / (opt_tp + opt_fp) \n",
    "    opt_npv = opt_tn / (opt_fn + opt_tn)\n",
    "    return opt_ppv, opt_npv, round(opt_th, 4)\n",
    "ppv_npv_opt_th()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ppv_npv(y_true, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "    ppv = tp / (tp + fp) \n",
    "    npv = tn / (fn + tn)    \n",
    "    return ppv, npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(df.closest_wb_distance_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(df.closest_fl_distance_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(df.closest_fl_area_sqkm.apply(lambda x: x if x > 0 else np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(df.closest_wb_area_sqkm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(df.closest_fl_elevation.apply(lambda x: x if x > 0 else np.nan)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ez1BBgTW_Bk"
   },
   "source": [
    "# Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nU49jh6tULS",
    "outputId": "166d48d9-0f92-4c84-a479-c662c5172834"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/binilg/lightgbm-with-randomsearchcv-and-feature-imp\n",
    "# Implementation: https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n",
    "# Documentation: https://lightgbm.readthedocs.io/en/latest/Features.html\n",
    "# LightGBM Classifier: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#\n",
    "\n",
    "from optimize_ppv_npv_scorer_ import optimize_ppv_npv_scorer\n",
    "\n",
    "import lightgbm\n",
    "param_dict = {\n",
    "    'learning_rate': [0.05],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7],\n",
    "    'min_split_gain' : [0.01],\n",
    "    'min_data_in_leaf':[10],\n",
    "#     'metric':['auc']\n",
    "    }\n",
    "#modelling\n",
    "clf = lightgbm.LGBMClassifier()\n",
    "\n",
    "if run_models:\n",
    "    random_search_model = (RandomizedSearchCV(clf, \n",
    "                               param_dict, \n",
    "                               verbose=1, \n",
    "                               cv=10, \n",
    "                               n_jobs = -1, \n",
    "                               random_state=random_state,\n",
    "                               n_iter=10,\n",
    "                               scoring=optimize_ppv_npv_scorer))\n",
    "        # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y)\n",
    "    model_dict[\"lgbm\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "lgbm = model_dict[\"lgbm\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(lgbm, dev_X, dev_Y, model_name=\"lgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "SaFbWsFQX5Vs",
    "outputId": "f00a317c-8b5f-4dd4-e538-8056b8a803be"
   },
   "outputs": [],
   "source": [
    "#Feature importance for top 50 predictors\n",
    "predictors = [x for x in df_X_combined_dummies_ordered.columns]\n",
    "feat_imp = pd.Series(lgbm.feature_importances_, predictors).sort_values(ascending=False)\n",
    "feat_imp = feat_imp[0:50]\n",
    "plt.rcParams['figure.figsize'] = 20, 5\n",
    "feat_imp.plot(kind='bar', title='Feature Importance')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(smt, clf)\n",
    "\n",
    "# param_dict = {\n",
    "#     'lgbmclassifier__objective' : ['binary'],\n",
    "#     }\n",
    "# pipeline = make_pipeline(SMOTE(random_state=random_state),\n",
    "#              lightgbm.LGBMClassifier())\n",
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/binilg/lightgbm-with-randomsearchcv-and-feature-imp\n",
    "# Implementation: https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n",
    "# Documentation: https://lightgbm.readthedocs.io/en/latest/Features.html\n",
    "# LightGBM Classifier: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#\n",
    "\n",
    "from optimize_ppv_npv_scorer_ import optimize_ppv_npv_scorer\n",
    "\n",
    "import lightgbm\n",
    "param_dict = {\n",
    "    'lgbmclassifier__learning_rate': [0.05],\n",
    "    'lgbmclassifier__num_leaves': [90,200],\n",
    "    'lgbmclassifier__boosting_type' : ['gbdt'],\n",
    "    'lgbmclassifier__objective' : ['binary'],\n",
    "    'lgbmclassifier__max_depth' : [5,6,7,8],\n",
    "    'lgbmclassifier__random_state' : [501], \n",
    "    'lgbmclassifier__colsample_bytree' : [0.5,0.7],\n",
    "    'lgbmclassifier__subsample' : [0.5,0.7],\n",
    "    'lgbmclassifier__min_split_gain' : [0.01],\n",
    "    'lgbmclassifier__min_data_in_leaf':[10],\n",
    "#     'metric':['auc']\n",
    "    }\n",
    "#modelling\n",
    "clf = lightgbm.LGBMClassifier()\n",
    "pipeline = make_pipeline(smt, clf)\n",
    "\n",
    "\n",
    "if run_models:\n",
    "    random_search_model = (RandomizedSearchCV(pipeline, \n",
    "                               param_dict, \n",
    "                               verbose=1, \n",
    "                               cv=10, \n",
    "                               n_jobs = -1, \n",
    "                               n_iter=10,\n",
    "                               random_state=random_state,\n",
    "                               scoring=optimize_ppv_npv_scorer))\n",
    "        # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y)\n",
    "    model_dict[\"lgbm_smote\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "lgbm_smote = model_dict[\"lgbm_smote\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(lgbm_smote, dev_X, dev_Y, model_name=\"lgbm_smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature importance for top 50 predictors\n",
    "# predictors = [x for x in df_X_combined_dummies_ordered.columns]\n",
    "# feat_imp = pd.Series(lgbm_smote.feature_importances_, predictors).sort_values(ascending=False)\n",
    "# feat_imp = feat_imp[0:50]\n",
    "# plt.rcParams['figure.figsize'] = 20, 5\n",
    "# feat_imp.plot(kind='bar', title='Feature Importance')\n",
    "# plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/binilg/lightgbm-with-randomsearchcv-and-feature-imp\n",
    "# Implementation: https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n",
    "# Documentation: https://lightgbm.readthedocs.io/en/latest/Features.html\n",
    "# LightGBM Classifier: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#\n",
    "\n",
    "from optimize_ppv_npv_scorer_ import optimize_ppv_npv_scorer\n",
    "\n",
    "import lightgbm\n",
    "param_dict = {\n",
    "    'learning_rate': [0.05],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['multiclass'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7],\n",
    "    'min_split_gain' : [0.01],\n",
    "    'min_data_in_leaf':[10],\n",
    "#     'metric':['auc']\n",
    "    }\n",
    "#modelling\n",
    "clf = lightgbm.LGBMClassifier()\n",
    "\n",
    "if run_models:\n",
    "    random_search_model = (RandomizedSearchCV(clf, \n",
    "                               param_dict, \n",
    "                               verbose=1, \n",
    "                               cv=10, \n",
    "                               n_jobs = -1, \n",
    "                               n_iter=10,\n",
    "                               scoring=optimize_ppv_npv_scorer))\n",
    "        # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y_groups)\n",
    "    model_dict[\"lgbm_groups\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "lgbm_groups = model_dict[\"lgbm_groups\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = Y_groups == 1\n",
    "group_2 = Y_groups == 2\n",
    "group_3 = Y_groups == 3\n",
    "\n",
    "print(np.mean(Y_groups[group_1] == lgbm_groups.predict(X[group_1])))\n",
    "print(np.mean(Y_groups[group_2] == lgbm_groups.predict(X[group_2])))\n",
    "print(np.mean(Y_groups[group_3] == lgbm_groups.predict(X[group_3])))\n",
    "\n",
    "train_predict = lgbm_groups.predict(X)\n",
    "pd.DataFrame(train_predict).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_group_1 = dev_Y_groups == 1\n",
    "dev_group_2 = dev_Y_groups == 2\n",
    "dev_group_3 = dev_Y_groups == 3\n",
    "\n",
    "print(np.mean(dev_Y_groups[dev_group_1] == lgbm_groups.predict(dev_X[dev_group_1])))\n",
    "print(np.mean(dev_Y_groups[dev_group_2] == lgbm_groups.predict(dev_X[dev_group_2])))\n",
    "print(np.mean(dev_Y_groups[dev_group_3] == lgbm_groups.predict(dev_X[dev_group_3])))\n",
    "\n",
    "dev_predict = lgbm_groups.predict(dev_X)\n",
    "pd.DataFrame(dev_predict).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(test_Y_groups).value_counts())\n",
    "\n",
    "test_group_1 = test_Y_groups == 1\n",
    "test_group_2 = test_Y_groups == 2\n",
    "test_group_3 = test_Y_groups == 3\n",
    "\n",
    "print(np.mean(test_Y_groups[test_group_1] == lgbm_groups.predict(test_X[test_group_1])))\n",
    "print(np.mean(test_Y_groups[test_group_2] == lgbm_groups.predict(test_X[test_group_2])))\n",
    "print(np.mean(test_Y_groups[test_group_3] == lgbm_groups.predict(test_X[test_group_3])))\n",
    "\n",
    "test_predict = lgbm_groups.predict(test_X)\n",
    "print(pd.DataFrame(test_predict).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following for \n",
    "# 1: 1,2,3,5\n",
    "# 2: 4,6,7\n",
    "# 3: 8 ,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = Y_groups == 1\n",
    "group_2 = Y_groups == 2\n",
    "group_3 = Y_groups == 3\n",
    "\n",
    "print(np.mean(Y_groups[group_1] == lgbm_groups.predict(X[group_1])))\n",
    "print(np.mean(Y_groups[group_2] == lgbm_groups.predict(X[group_2])))\n",
    "print(np.mean(Y_groups[group_3] == lgbm_groups.predict(X[group_3])))\n",
    "\n",
    "train_predict = lgbm_groups.predict(X)\n",
    "pd.DataFrame(train_predict).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_group_1 = dev_Y_groups == 1\n",
    "dev_group_2 = dev_Y_groups == 2\n",
    "dev_group_3 = dev_Y_groups == 3\n",
    "\n",
    "print(np.mean(dev_Y_groups[dev_group_1] == lgbm_groups.predict(dev_X[dev_group_1])))\n",
    "print(np.mean(dev_Y_groups[dev_group_2] == lgbm_groups.predict(dev_X[dev_group_2])))\n",
    "print(np.mean(dev_Y_groups[dev_group_3] == lgbm_groups.predict(dev_X[dev_group_3])))\n",
    "\n",
    "dev_predict = lgbm_groups.predict(dev_X)\n",
    "pd.DataFrame(dev_predict).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_1 = test_Y_groups == 1\n",
    "test_group_2 = test_Y_groups == 2\n",
    "test_group_3 = test_Y_groups == 3\n",
    "\n",
    "print(np.mean(test_Y_groups[test_group_1] == lgbm_groups.predict(test_X[test_group_1])))\n",
    "print(np.mean(test_Y_groups[test_group_2] == lgbm_groups.predict(test_X[test_group_2])))\n",
    "print(np.mean(test_Y_groups[test_group_3] == lgbm_groups.predict(test_X[test_group_3])))\n",
    "\n",
    "test_predict = lgbm_groups.predict(test_X)\n",
    "pd.DataFrame(test_predict).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM: Second level learner to minimize False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(lgbm, X, Y, model_name=\"lgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all the true and false negatives on train data\n",
    "\n",
    "lgbm = model_dict[\"lgbm\"]  \n",
    "\n",
    "negs = model_dict[\"lgbm\"].predict(X) == 0\n",
    "X_negs = X[negs]\n",
    "Y_negs = Y[negs]\n",
    "np.mean(Y_negs) # 13% of the predicted negatives are true negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'learning_rate': [0.05],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7],\n",
    "    'min_split_gain' : [0.01],\n",
    "    'min_data_in_leaf':[10],\n",
    "#     'metric':['auc']\n",
    "    }\n",
    "clf = lightgbm.LGBMClassifier()\n",
    "\n",
    "if run_models:\n",
    "    random_search_model = (RandomizedSearchCV(clf, \n",
    "                               param_dict, \n",
    "                               verbose=1, \n",
    "                               cv=10, \n",
    "                               n_jobs = -1, \n",
    "                               n_iter=10,\n",
    "                               scoring='precision'))\n",
    "        # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X_negs, Y_negs)\n",
    "    model_dict[\"lgbm_second_level\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "lgbm_second_level = model_dict[\"lgbm_second_level\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_negs = model_dict[\"lgbm\"].predict(dev_X) == 0\n",
    "dev_X_negs = dev_X[dev_negs]\n",
    "dev_Y_negs = dev_Y[dev_negs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(lgbm_second_level, dev_X_negs, dev_Y_negs, model_name=\"lgbm_second_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 100 # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    " \n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(X.shape[1],))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(413, activation='sigmoid')(encoded)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "# configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = X\n",
    "x_test = dev_X\n",
    "# normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n",
    "x_train = x_train.astype('float32') / np.float(x_train.shape[1] - 1)\n",
    "x_test = x_test.astype('float32') / np.float(x_test.shape[1] - 1)\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_autoencoder:\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "    epochs=50,\n",
    "    batch_size=x_train.shape[1],\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test))\n",
    "    # encode and decode some digits\n",
    "    # note that we take them from the *test* set\n",
    "    encoded_imgs = encoder.predict(x_test)\n",
    "    decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/binilg/lightgbm-with-randomsearchcv-and-feature-imp\n",
    "# Implementation: https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n",
    "# Documentation: https://lightgbm.readthedocs.io/en/latest/Features.html\n",
    "# LightGBM Classifier: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#\n",
    "\n",
    "encoded_train_imgs = encoder.predict(x_train)\n",
    "from optimize_ppv_npv_scorer_ import optimize_ppv_npv_scorer\n",
    "\n",
    "import lightgbm\n",
    "param_dict = {\n",
    "    'learning_rate': [0.05],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7],\n",
    "    'min_split_gain' : [0.01],\n",
    "    'min_data_in_leaf':[10],\n",
    "#     'metric':['auc']\n",
    "    }\n",
    "#modelling\n",
    "\n",
    "if run_autoencoder:\n",
    "    clf = lightgbm.LGBMClassifier()\n",
    "\n",
    "    if run_models:\n",
    "        random_search_model = (RandomizedSearchCV(clf, \n",
    "                                   param_dict, \n",
    "                                   verbose=1, \n",
    "                                   cv=10, \n",
    "                                   n_jobs = -1, \n",
    "                                   n_iter=10,\n",
    "                                   scoring=optimize_ppv_npv_scorer))\n",
    "            # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "        random_search_model.fit(encoded_train_imgs, Y)\n",
    "        model_dict[\"lgbm_autoencoder\"] = random_search_model\n",
    "        model_dict[\"file_params\"] = file_param_dict\n",
    "        pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "\n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    lgbm_autoencoder = model_dict[\"lgbm_autoencoder\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_autoencoder:\n",
    "    model_results(lgbm_autoencoder, encoded_train_imgs, Y, model_name=\"lgbm_autoencoder\")\n",
    "    model_results(lgbm_autoencoder, encoded_imgs, dev_Y, model_name=\"lgbm_autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(2, random_state=random_state)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(labels == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # plot the input data\n",
    "    ax = ax or plt.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=random_state)\n",
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working because determinant of covariance matrix is 0 due to multicollinearity\n",
    "# from sklearn.mixture import GaussianMixture as GMM\n",
    "# gmm = GMM(n_components=4).fit(X)\n",
    "# labels = gmm.predict(X)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_knn:\n",
    "    knn = KNeighborsClassifier(n_neighbors=30)\n",
    "    X_ = X.copy()[:, :2]\n",
    "    knn.fit(X_, Y)\n",
    "    \n",
    "    model_dict[\"knn\"] = knn.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    knn = model_dict[\"knn\"]  \n",
    "    model_results(knn, dev_X.copy()[:, :2], dev_Y, model_name=\"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_knn:\n",
    "    knn = KNeighborsClassifier(n_neighbors=30)\n",
    "    X_ = X_negs.copy()[:, :2]\n",
    "    knn.fit(X_, Y_negs)\n",
    "    \n",
    "    model_dict[\"knn\"] = knn.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    knn = model_dict[\"knn\"]  \n",
    "    model_results(knn, dev_X_negs.copy()[:, :2], dev_Y_negs, model_name=\"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_svc:\n",
    "    param_dict = {'kernel': ['rbf'],\n",
    "                  'C': [1, 10, 100]}\n",
    "\n",
    "    # param_dict = {}\n",
    "\n",
    "    clf = SVC(gamma='scale', probability=True)\n",
    "    # clf.fit(X_negs, Y_negs)\n",
    "    if run_models:\n",
    "        random_search_model = (RandomizedSearchCV(clf, \n",
    "                                   param_dict, \n",
    "                                   verbose=1, \n",
    "                                   cv=10, \n",
    "                                   n_jobs = -1, \n",
    "                                   n_iter=10,\n",
    "                                   scoring='roc_auc'))\n",
    "            # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "        random_search_model.fit(X, Y)\n",
    "        model_dict[\"svc\"] = random_search_model.best_estimator_\n",
    "        model_dict[\"file_params\"] = file_param_dict\n",
    "        pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "\n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    svc = model_dict[\"svc\"] \n",
    "    model_results(svc, dev_X, dev_Y, model_name=\"svc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier (on negative predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {'kernel': ['rbf'],\n",
    "              'C': [1, 10, 100]}\n",
    "\n",
    "# param_dict = {}\n",
    "\n",
    "if run_svc_second_level:\n",
    "    clf = SVC(gamma='scale')\n",
    "    # clf.fit(X_negs, Y_negs)\n",
    "    if run_models:\n",
    "        random_search_model = (RandomizedSearchCV(clf, \n",
    "                                   param_dict, \n",
    "                                   verbose=1, \n",
    "                                   cv=10, \n",
    "                                   n_jobs = -1, \n",
    "                                   n_iter=10,\n",
    "                                   scoring='roc_auc'))\n",
    "            # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "        random_search_model.fit(X_negs, Y_negs)\n",
    "        model_dict[\"svc_second_level\"] = random_search_model.best_estimator_\n",
    "        model_dict[\"file_params\"] = file_param_dict\n",
    "        pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "\n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    svc_second_level = model_dict[\"svc_second_level\"]\n",
    "    confusion_matrix(dev_Y_negs, svc_second_level.predict(dev_X_negs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpKXfQfXzKLf"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzQrUXTW6J6b",
    "outputId": "506c9dc8-7ddd-46a3-f171-91e9993e52c9"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqOn82eq0Hsy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# build a classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "param_dict = {\"C\":np.logspace(-3,3,7), \n",
    "              \"penalty\":[\"l1\", \"l2\", \"elasticnet\"],\n",
    "              \"l1_ratio\":np.linspace(0,1,10),\n",
    "              \"solver\":[\"saga\"]\n",
    "              }# l1 lasso l2 ridge\n",
    "\n",
    "# run randomized search\n",
    "if run_logistic:\n",
    "    random_search_model = RandomizedSearchCV(clf, \n",
    "                                       param_distributions=param_dict,\n",
    "                                       n_iter=20, \n",
    "                                       scoring=optimize_ppv_npv_scorer, \n",
    "                                       random_state=random_state,\n",
    "                                       cv=10, \n",
    "                                       n_jobs=-1)\n",
    "\n",
    "\n",
    "    # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y)\n",
    "    model_dict[\"lr\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYoge1Lo5Xy5",
    "outputId": "608485a4-911d-4c7f-b89b-b2ae06d5b4c8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "    lr = model_dict[\"lr\"]\n",
    "    model_results(lr, dev_X, dev_Y, model_name=\"lr\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if useful\n",
    "# precision, recall, thresholds = metrics.precision_recall_curve(test_Y, lr.predict_proba(test_X)[:, 1], pos_label=1)\n",
    "\n",
    "# metrics.plot_precision_recall_curve(lr, test_X, test_Y, response_method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLlnRF_s062y"
   },
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug-A0ZPMDgZ7"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8k3ZsgjHNjC"
   },
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "clf = XGBRFClassifier()\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "# https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "param_dict = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "# run randomized search\n",
    "if run_models:\n",
    "    random_search_model = RandomizedSearchCV(clf, \n",
    "                                   param_distributions=param_dict,\n",
    "                                   n_iter=1, \n",
    "                                   scoring=optimize_ppv_npv_scorer, \n",
    "                                   random_state=random_state,\n",
    "                                   cv=10, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "\n",
    "    # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y)\n",
    "    model_dict[\"xgb\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict    \n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "xgb = model_dict[\"xgb\"]\n",
    "y_predict = lr.predict(test_X) \n",
    "\n",
    "# threshold is taken as 0.5, as proven here\n",
    "# y_predict_ = 1 * (lr.predict_proba(test_X)[:, 1]>0.5) # \n",
    "# np.mean(y_predict == y_predict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(xgb, dev_X, dev_Y, model_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "DqyeGmeqZXnZ",
    "outputId": "36dc804f-a60b-4a0c-87dd-d4a9bc62411f"
   },
   "outputs": [],
   "source": [
    "# #Feature importance for top 50 predictors\n",
    "# predictors = [x for x in df_X_combined_dummies_ordered.columns]\n",
    "# feat_imp = pd.Series(xgb.best_estimator_.feature_importances_, predictors).sort_values(ascending=False)\n",
    "# feat_imp = feat_imp[0:50]\n",
    "# plt.rcParams['figure.figsize'] = 20, 5\n",
    "# feat_imp.plot(kind='bar', title='Feature Importance')\n",
    "# plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "clf = XGBRFClassifier()\n",
    "pipeline = make_pipeline(smt, clf)\n",
    "# pipeline.get_params()\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "# https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "param_dict = {\n",
    "        'xgbrfclassifier__min_child_weight': [1, 5, 10],\n",
    "        'xgbrfclassifier__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'xgbrfclassifier__subsample': [0.6, 0.8, 1.0],\n",
    "        'xgbrfclassifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'xgbrfclassifier__max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "# run randomized search\n",
    "if run_models:\n",
    "    random_search_model = RandomizedSearchCV(pipeline, \n",
    "                                   param_distributions=param_dict,\n",
    "                                   n_iter=1, \n",
    "                                   scoring=optimize_ppv_npv_scorer, \n",
    "                                   random_state=random_state,\n",
    "                                   cv=10, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "\n",
    "    # from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    random_search_model.fit(X, Y)\n",
    "    model_dict[\"xgb_smote\"] = random_search_model.best_estimator_\n",
    "    model_dict[\"file_params\"] = file_param_dict    \n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "xgb_smote = model_dict[\"xgb_smote\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(xgb_smote, dev_X, dev_Y, model_name=\"xgb_smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3RElmg55sOb"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers, meta_classifier, use_probas=False, cv=2, \n",
    "# use_features_in_secondary=False, stratify=True, shuffle=True, verbose=0, store_train_meta_features=False, use_clones=True)\n",
    "\n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "# xgb = model_dict[\"xgb\"]\n",
    "lgbm = model_dict[\"lgbm\"]\n",
    "# knn = model_dict[\"knn\"]\n",
    "\n",
    "try:\n",
    "    knn = model_dict[\"knn\"]\n",
    "except:\n",
    "    knn = None\n",
    "\n",
    "try:\n",
    "    svc = model_dict[\"svc\"]\n",
    "except:\n",
    "    svc = None\n",
    "\n",
    "if run_models:\n",
    "    stack_gen_model = (StackingCVClassifier(classifiers=[xgb,\n",
    "                                                         lgbm,\n",
    "                                            meta_classifier=xgb,\n",
    "                                            use_features_in_secondary=False,\n",
    "                                            use_probas=True,\n",
    "                                            random_state=random_state))\n",
    "\n",
    "    stack_gen_model.fit(X, Y)\n",
    "    model_dict[\"stacking\"] = stack_gen_model\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "    \n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "stacking = model_dict[\"stacking\"]\n",
    "y_predict = stacking.predict(test_X) \n",
    "y_score = stacking.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute ROC curve and ROC area for each class\n",
    "# n_classes = 2\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(test_Y, y_score[:, 1])\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# roc_auc\n",
    "\n",
    "# # # Compute micro-average ROC curve and ROC area\n",
    "# # fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_Y.ravel(), y_score.ravel())\n",
    "# # roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# # plt.figure()\n",
    "# # lw = 2\n",
    "# # plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "# #          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "# # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# # plt.xlim([0.0, 1.0])\n",
    "# # plt.ylim([0.0, 1.05])\n",
    "# # plt.xlabel('False Positive Rate')\n",
    "# # plt.ylabel('True Positive Rate')\n",
    "# # plt.title('Receiver operating characteristic example')\n",
    "# # plt.legend(loc=\"lower right\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(stacking, test_X, test_Y, model_name=\"stacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efv12Vc6ijNT"
   },
   "outputs": [],
   "source": [
    "if run_models:\n",
    "    vc_clf = (VotingClassifier(estimators=[(\"xbg\", model_dict[\"xgb\"]), \n",
    "                                           (\"lightgbm\", model_dict[\"lgbm\"]),\n",
    "                                          (\"stacking\", model_dict[\"stacking\"])],\n",
    "                                           voting=\"soft\",\n",
    "                                           flatten_transform=False))\n",
    "\n",
    "    vc_fit = vc_clf.fit(dev_X, dev_Y)\n",
    "    model_dict[\"voting_clf\"] = vc_fit\n",
    "    model_dict[\"file_params\"] = file_param_dict\n",
    "    pickle.dump(model_dict, open(\"random_search_fitted_models_\" + FILE_VERSION, \"wb\"), protocol=3)\n",
    "\n",
    "model_dict = pd.read_pickle(\"random_search_fitted_models_\" + FILE_VERSION)\n",
    "voting_clf = model_dict[\"voting_clf\"]\n",
    "y_predict = voting_clf.predict(test_X) \n",
    "y_score = voting_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results(voting_clf, test_X, test_Y, model_name=\"voting_clf\")\n",
    "# find_roc_auc(vc_fit, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "time_taken(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ppv and npv on Test:\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        y_predict_proba = model_dict[model].predict_proba(test_X)[:, 1]\n",
    "        pv = ppv_npv_opt_th(test_Y, y_predict_proba)\n",
    "        print(\"{}: {}ppv = {}, npv = {} @ threshold = {}\".format(model, \" \"*(13 - len(model)), round(pv[0], 4), round(pv[1], 4), round(pv[2], 4)))\n",
    "    \n",
    "print(\"=========================================\")\n",
    "print(\"ROC_AUC on Test\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        y_predict = model_dict[model].predict(test_X)\n",
    "        print(\"{}:{}{}\".format(model, \" \"*(13 - len(model)), find_roc_auc(model_dict.get(model), test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ppv and npv on Dev:\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        y_predict_proba = model_dict[model].predict_proba(dev_X)[:, 1]\n",
    "        pv = ppv_npv_opt_th(dev_Y, y_predict_proba)\n",
    "        print(\"{}: {}ppv = {}, npv = {} @ threshold = {}\".format(model, \" \"*(13 - len(model)), round(pv[0], 4), round(pv[1], 4), round(pv[2], 4)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"ROC_AUC on Dev\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        print(\"{}:{}{}\".format(model, \" \"*(13 - len(model)), find_roc_auc(model_dict.get(model), dev_X, dev_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ppv and npv on Train:\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        y_predict_proba = model_dict[model].predict_proba(X)[:, 1]\n",
    "        pv = ppv_npv_opt_th(Y, y_predict_proba)\n",
    "        print(\"{}: {}ppv = {}, npv = {} @ threshold = {}\".format(model, \" \"*(13 - len(model)), round(pv[0], 4), round(pv[1], 4), round(pv[2], 4)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"ROC_AUC on Training\")\n",
    "for model in model_dict.keys():\n",
    "    if model not in [\"knn\", \"file_params\", \"lgbm_autoencoder\", \"svc_second_level\", \"lgbm_second_level\"]:\n",
    "        print(\"{}:{}{}\".format(model, \" \"*(13 - len(model)), find_roc_auc(model_dict.get(model), X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Index\" in df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array(df_test)\n",
    "model = \"lgbm\"\n",
    "y_predict = model_dict[model].predict(test_X)\n",
    "array2 = y_predict.reshape(y_predict.shape[0],1)\n",
    "array3 = np.hstack((array1, array2))\n",
    "df_pred = pd.DataFrame(array3, columns = list(df_test.columns) + [\"predictions\"])\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "\n",
    "df_fn = df_pred[(df_pred.predictions == 0) & (df_pred.cwa_determination == 1)]\n",
    "df_fn.head()\n",
    "\n",
    "# true negatives\n",
    "\n",
    "df_tn = df_pred[(df_pred.predictions == 0) & (df_pred.cwa_determination == 0)]\n",
    "df_tn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols = ['cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6' , 'cwa7', 'cwa8', 'cwa9', 'cwa_determination','longitude', 'latitude', 'Index','da_number',\n",
    "       'jurisdiction_type', 'potential_wetland', \"predictions\"]\n",
    "df_fn[imp_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fn.drop('geometry', axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df_fn.columns[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_tn.closest_wb_distance_m.mean())\n",
    "# print(df_fn.closest_wb_distance_m.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## which states have the most false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn_tn = pd.concat([pd.DataFrame(df_fn.district).value_counts(), pd.DataFrame(df_tn.district).value_counts()], axis=1)\n",
    "df_fn_tn.columns = [\"FN\", \"TN\"]\n",
    "df_fn_tn[\"1-npv\"] = df_fn_tn.apply(lambda x: round(x.FN/(x.FN + x.TN), 2), axis=1)#.sort_values(ascending=False)\n",
    "df_fn_tn.sort_values(by=\"1-npv\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_tn.district).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(feature=\"closest_wb_elevation\"):\n",
    "    import scipy\n",
    "    try:\n",
    "        return (scipy.stats.ttest_ind(np.array(df_tn[feature], dtype=float), \n",
    "                           np.array(df_fn[feature], dtype=float), \n",
    "                           nan_policy='omit'))[1]\n",
    "    except Exception as e:\n",
    "#         print(e)\n",
    "        pass\n",
    "p_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_fn.columns:\n",
    "    try:\n",
    "        if p_value(feature) < 0.0001:\n",
    "            if feature in imp_num_feature:\n",
    "                delta = (np.nanmean(np.array(df_tn[feature], dtype=float)) \n",
    "                         - np.nanmean(np.array(df_fn[feature], dtype=float)))\n",
    "                print (feature, \":\", delta)\n",
    "    except Exception as e:\n",
    "#         print(feature, \"XXXX\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_fn.columns:\n",
    "    try:\n",
    "        if p_value(feature) < 0.001:\n",
    "            if feature in imp_num_feature:\n",
    "                delta = (np.nanmean(np.array(df_tn[feature], dtype=float)) \n",
    "                         - np.nanmean(np.array(df_fn[feature], dtype=float)))\n",
    "                print (feature, \":\", delta)\n",
    "    except Exception as e:\n",
    "#         print(feature, \"XXXX\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_fn.columns:\n",
    "    try:\n",
    "        if p_value(feature) < 0.01:\n",
    "            if feature in imp_num_feature:\n",
    "                delta = (np.nanmean(np.array(df_tn[feature], dtype=float)) \n",
    "                         - np.nanmean(np.array(df_fn[feature], dtype=float)))\n",
    "                print (feature, \":\", delta)\n",
    "    except Exception as e:\n",
    "#         print(feature, \"XXXX\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_fn.columns:\n",
    "    try:\n",
    "        if p_value(feature) < 0.1:\n",
    "            if feature in imp_num_feature:\n",
    "                delta = (np.nanmean(np.array(df_tn[feature], dtype=float)) \n",
    "                         - np.nanmean(np.array(df_fn[feature], dtype=float)))\n",
    "                print (feature, \":\", delta)\n",
    "    except Exception as e:\n",
    "#         print(feature, \"XXXX\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"lgbm\"\n",
    "model_results(model_dict[model], test_X, test_Y, model)\n",
    "confusion_matrix(test_Y, model_dict[model].predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_thresholding(threshold=0.5):\n",
    "    y_threshold = (model_dict[\"lgbm\"].predict_proba(test_X)[:, 1] > threshold) * 1\n",
    "    np.mean(y_threshold == test_Y)\n",
    "    print(classification_report(test_Y, y_threshold))\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(test_Y, y_threshold))\n",
    "do_thresholding()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thresholding(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred[~df_pred.county.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_pred.county.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[\"count_1_pred_by_county\"] = df_pred.groupby([\"county\"])[\"predictions\"].transform(np.mean)#apply(lambda x: np.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.groupby_agg(\n",
    "#     by='item',\n",
    "#     agg='mean',\n",
    "#     agg_column_name='MRP',\n",
    "#     new_column_name='Avg_MRP'\n",
    "# )\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "df_pred[\"percent_1_actual\"] = df_pred.groupby(\"county\")[\"cwa_determination\"].transform(find_mean)\n",
    "df_pred[\"percent_1_predictions\"] = df_pred.groupby(\"county\")[\"predictions\"].transform(find_mean)\n",
    "df_pred[\"pred_over_actual\"] = df_pred.percent_1_predictions / (df_pred.percent_1_actual + 0.00000001)\n",
    "\n",
    "ratio_list = []\n",
    "for groupby in df_pred.groupby(\"county\"):\n",
    "    if 10000 > np.mean(np.array(groupby[1].pred_over_actual)):# > 1:\n",
    "        ratio_list.append(np.mean(np.array(groupby[1].pred_over_actual)))\n",
    "#         print(groupby[0], np.mean(np.array(groupby[1].pred_over_actual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ratio_list, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in which counties are false positives more:\n",
    "\n",
    "for groupby in df_pred.groupby(\"county\"):\n",
    "    if 10000 > np.mean(np.array(groupby[1].pred_over_actual)) > 1:\n",
    "        print(groupby[0], np.mean(np.array(groupby[1].pred_over_actual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in which counties are false negatives more:\n",
    "count = 0\n",
    "for groupby in df_pred.groupby(\"county\"):\n",
    "    if np.mean(np.array(groupby[1].pred_over_actual)) < 0.5:\n",
    "        count += 1\n",
    "#         print(groupby[0], np.mean(np.array(groupby[1].pred_over_actual)))\n",
    "print(count)    \n",
    "# are there 463 counties where there is no prediction of 1? out of 689 total counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pred.groupby(\"county\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pred.county.unique()) # 689\n",
    "len(df_pred.district.unique()) # 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look by district. These are the districts where there are no 1's being predicted\n",
    "\n",
    "def find_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "df_pred[\"percent_1_actual_by_district\"] = df_pred.groupby(\"district\")[\"cwa_determination\"].transform(find_mean)\n",
    "df_pred[\"percent_1_predictions_by_district\"] = df_pred.groupby(\"district\")[\"predictions\"].transform(find_mean)\n",
    "df_pred[\"pred_over_actual_by_district\"] = df_pred.percent_1_predictions / (df_pred.percent_1_actual + 0.00000001)\n",
    "\n",
    "ratio_list = []\n",
    "print(\"District           pred/act    tot_cnt    actual_1's,  pred_1's\")\n",
    "for groupby in df_pred.groupby(\"district\"):\n",
    "#     if not np.mean(np.array(groupby[1].pred_over_actual_district)):# > 1:\n",
    "    ratio_list.append(np.mean(np.array(groupby[1].pred_over_actual)))\n",
    "    (print(groupby[0], \" \" * (20 - len(groupby[0])), round(np.mean(np.array(groupby[1].pred_over_actual)), 2), \n",
    "           \" \" * (10 - len(str(round(np.mean(np.array(groupby[1].pred_over_actual)), 2)))), groupby[1].cwa_determination.shape[0], \n",
    "           \" \" * (10 - len(str(np.sum(groupby[1].cwa_determination)))), np.sum(groupby[1].cwa_determination),\n",
    "           \" \" * (10 - len(str(np.sum(groupby[1].predictions)))), np.sum(groupby[1].predictions))\n",
    "\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ratio_list, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Albuquerque\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Fort Worth\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Huntington\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Louisville\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Nashville\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"St. Louis\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Tulsa\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Vicksburg\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.district == \"Walla Walla\"][[\"cwa_determination\", \"district\", \"predictions\", \"percent_1_actual\", \n",
    "                                           \"percent_1_predictions_by_district\", \"percent_1_actual_by_district\", \"percent_1_predictions_by_district\", \"pred_over_actual_by_district\"]].head(1)    #.iloc[:, 494:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred.groupby(\"county\")[\"predictions\"].apply(find_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean(df):\n",
    "    try:\n",
    "        result = df[~df.isna()].mean()\n",
    "        if not np.isnan(result):\n",
    "            return result\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "def find_sum(df):\n",
    "    return np.sum(np.array(df))\n",
    "\n",
    "df_pred.groupby([\"county\"])[\"predictions\"].transform(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[df_pred.county == \"Horry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn_ = df_fn.copy()\n",
    "df_tn_ = df_tn.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df_fn_['longitude'], df_fn_['latitude'])]\n",
    "gdf = GeoDataFrame(df_fn_, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "north_america = world[world.continent == \"North America\"]\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true negatives\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df_tn_['longitude'], df_tn_['latitude'])]\n",
    "gdf = GeoDataFrame(df_tn_, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "north_america = world[world.continent == \"North America\"]\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not false negatives\n",
    "df_not_fn = df_pred[~((df_pred.predictions == 0) & (df_pred.cwa_determination == 1))]\n",
    "\n",
    "df_not_fn_ = df_not_fn.copy()\n",
    "geometry = [Point(xy) for xy in zip(df_not_fn_['longitude'], df_not_fn_['latitude'])]\n",
    "gdf = GeoDataFrame(df_not_fn_, geometry=geometry)   \n",
    "\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positives\n",
    "df_fp = df_pred[(df_pred.predictions == 1) & (df_pred.cwa_determination == 0)]\n",
    "df_fp_ = df_fp.copy()\n",
    "\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df_fp_['longitude'], df_fp_['latitude'])]\n",
    "gdf = GeoDataFrame(df_fp_, geometry=geometry)   \n",
    "\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All good predictions\n",
    "df_good_preds = df_pred[((df_pred.predictions == 0) & (df_pred.cwa_determination == 0)) | ((df_pred.predictions == 1) & (df_pred.cwa_determination == 1))]\n",
    "\n",
    "df_good_preds_ = df_good_preds.copy()\n",
    "geometry = [Point(xy) for xy in zip(df_good_preds_['longitude'], df_good_preds_['latitude'])]\n",
    "gdf = GeoDataFrame(df_good_preds_, geometry=geometry)   \n",
    "\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df_pred['longitude'], df_pred['latitude'])]\n",
    "gdf = GeoDataFrame(df_pred, geometry=geometry)   \n",
    "\n",
    "gdf.plot(ax=north_america.plot(figsize=(20, 12)), marker='o', color='red', markersize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySEfwFTbEOsU"
   },
   "source": [
    "# Break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gximuif9tUZe"
   },
   "outputs": [],
   "source": [
    "df_fn[df_fn.state == '12'][\"hydclprs\"]#['da_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_fn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJxn_V1lcnUd"
   },
   "outputs": [],
   "source": [
    "model_dict[\"file_params\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2021.03.19_WOTUS_restart_v6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
