{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGNyccn1n8ig",
    "outputId": "b765fce2-a360-400b-c62a-3e4800648ddc"
   },
   "outputs": [],
   "source": [
    "# PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "\n",
    "# !earthengine authenticate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8pYNY3YLn9zJ"
   },
   "outputs": [],
   "source": [
    "# # PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "# # authenticate to Google Colab\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkMz8_N5n92p",
    "outputId": "18571437-d60b-41f8-a8b3-e896b8ab5812"
   },
   "outputs": [],
   "source": [
    "# # USE MIDSCWA@gmail.com/cleanwater\n",
    "# # to access csv file\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 42,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62SEig6EPDZ7"
   },
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 43,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "a_aMcWg6_13U"
   },
   "outputs": [],
   "source": [
    "# Sentinel Level 2A surface reflectances\n",
    "# Note: 2A is a processed file whereby Level 1A top-of-atmosphere TOA \n",
    "# reflectance is converted to surface reflectance\n",
    "\n",
    "\n",
    "# Use the latest Sentinel-2 Cloud Masking with s2cloudless\n",
    "# https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless\n",
    "# Parameter | Type\t| Description\n",
    "# AOI\tee.Geometry\tArea of interest\n",
    "# START_DATE\tstring\tImage collection start date (inclusive)\n",
    "# END_DATE\tstring\tImage collection end date (exclusive)\n",
    "# CLOUD_FILTER\tinteger\tMaximum image cloud cover percent allowed in image \n",
    "# collection\n",
    "# CLD_PRB_THRESH\tinteger\tCloud probability (%); values greater than are \n",
    "# considered cloud\n",
    "# NIR_DRK_THRESH\tfloat\tNear-infrared reflectance; values less than are considered \n",
    "# potential cloud shadow\n",
    "# CLD_PRJ_DIST\tfloat\tMaximum distance (km) to search for cloud shadows from \n",
    "# cloud edges\n",
    "# BUFFER\tinteger\tDistance (m) to dilate the edge of cloud-identified objects\n",
    "\n",
    "\n",
    "START_DATE = '2018-08-01'\n",
    "END_DATE = '2020-04-01'\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "\n",
    "# # Build a Sentinel-2 collection\n",
    "# # Sentinel-2 surface reflectance and Sentinel-2 cloud probability are two \n",
    "# different image collections. Each collection must be filtered similarly \n",
    "# (e.g., by date and bounds) and then the two filtered collections must \n",
    "# be joined.\n",
    "\n",
    "# # Define a function to filter the SR and s2cloudless collections according \n",
    "# to area of interest and date parameters, then join them on the system:index \n",
    "# property. The result is a copy of the SR collection where each image has a \n",
    "# new 's2cloudless' property whose value is the corresponding s2cloudless image.\n",
    "\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by \n",
    "    # the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 44,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "F9bVlKz2ka6E"
   },
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 45,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "FqexfNCnkeuA"
   },
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 46,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "Ej9_E_r2kNuX"
   },
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 47,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "1EU750KEkN4X"
   },
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 48,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "Y3v_crqO313l"
   },
   "outputs": [],
   "source": [
    "def s2_sr_median_func(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_median = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .median())\n",
    "  return s2_sr_median\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 49,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B47yUe4SATbx"
   },
   "source": [
    "# SRTM, JRC and NDVI etc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 50,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "xjQ9sH-Gn-Cq"
   },
   "outputs": [],
   "source": [
    "# SRTM for elevation\n",
    "srtm = ee.Image('USGS/SRTMGL1_003')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 51,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "NuSIR949X2GG"
   },
   "outputs": [],
   "source": [
    "# slope of terrain\n",
    "slope = ee.Terrain.slope(srtm)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 52,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VmW0Whdnq5b",
    "outputId": "a5830380-194e-4f9f-82b9-60c5694287b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seasonality', 'transition', 'max_extent']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 14,
=======
     "execution_count": 52,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add JRC bands of interest\n",
    "jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                .bandNames().getInfo()\n",
    "jrc = jrc.select(jrc_bands)\n",
    "jrc.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 53,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "D7BzVImhV_nh"
   },
   "outputs": [],
   "source": [
    "def make_ndvi(image, red='B4', nir='B8'):\n",
    "  return image.normalizedDifference([nir, red])  \n",
    "\n",
    "def make_ndwi(image, green='B3', nir='B8'):\n",
    "  return image.normalizedDifference([green, nir])  \n",
    "\n",
    "\n",
    "def make_mndvi(image, red='B4', nir='B8'):\n",
    "  nir = image.select('B8')\n",
    "  red = image.select('B4')\n",
    "  aerosols = image.select('B1')  \n",
    "  mndvi = (nir.subtract(red)\n",
    "                      .divide(\n",
    "                          nir.add(red)\n",
    "                          .subtract(aerosols.add(aerosols))\n",
    "                          )\n",
    "                      .rename('mndvi'))\n",
    "  return mndvi\n",
    "\n",
    "def make_mndwi(image, green='B3', swir='B11'):\n",
    "  green = image.select('B3')\n",
    "  swir = image.select('B11')\n",
    "  mndwi = (green.subtract(swir)\n",
    "                .divide(\n",
    "                    green.add(swir))\n",
    "                .rename('mndwi'))\n",
    "  return mndwi"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 54,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "lSM5VUj7nynK"
   },
   "outputs": [],
   "source": [
    "# choose median of mndwi image collection\n",
    "def make_med_mndwi(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  med_mndwi = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .median()\n",
    "                             )\n",
    "  return med_mndwi # returns the image with median mndwi in the date range\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 55,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "S4nXxi5um8bZ"
   },
   "outputs": [],
   "source": [
    "# choose the max water pixel from mndwi image collection\n",
    "def s2_sr_greenestpixel_func2(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_greenestpixel = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .qualityMosaic('mndwi')\n",
    "                             )\n",
    "  # returns the image with highest mndwi in the date range\n",
    "  return s2_sr_greenestpixel \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tt96ySPAYIn"
   },
   "source": [
    "# Read in Rapanos dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 56,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wheel\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 57,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2021_02_23_ExportImagestoGCS-multpool.ipynb\r\n",
      "2021_02_23_ExportImagestoGCS.ipynb\r\n",
      "Data_combined_regular_clean.csv\r\n",
      "Data_combined_regular_clean_with_ssurgo_variables.csv\r\n",
      "\u001b[34mcsv_with_SSURGO_NHD_variables\u001b[m\u001b[m\r\n"
=======
      "2021_02_23_ExportImagestoGCS-Copy1.ipynb\n",
      "2021_02_23_ExportImagestoGCS-Copy2.ipynb\n",
      "2021_02_23_ExportImagestoGCS-Copy3.ipynb\n",
      "2021_02_23_ExportImagestoGCS-multpool.ipynb\n",
      "2021_02_23_ExportImagestoGCS.ipynb\n",
      "Data_combined_regular_clean.csv\n",
      "Data_combined_regular_clean_with_ssurgo_variables.csv\n",
      "csv_with_SSURGO_NHD_variables\n"
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 58,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "6_t_J2SataRZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# datapath = \"drive/MyDrive/Data/combined_v2.csv\"\n",
    "datapath = \"Data_combined_regular_clean.csv\"\n",
    "df = pd.read_csv(datapath, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# column name 'index' is conflicting with the native index of dataframe\n",
    "# hence, creating a new column named \"Index\"\n",
    "df[\"Index\"] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
=======
    "id": "lMZyUm24AcWU"
   },
   "source": [
    "# Set up global variables for Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "uPov83g1tiGH"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR BUCKET HERE:\n",
    "BUCKET = 'pollutemenot-ai'\n",
    "# FOLDER = 'test_final'\n",
    "FOLDER = \"GEE_images_final2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
    "id": "Y9IYvD7NLC5T"
   },
   "source": [
    "# Exporting Functions\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 60,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "L9KsSAfHL4nj"
   },
   "outputs": [],
   "source": [
    "def doExport_RBG2(lat, lon, index_danum, band, size=1000):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "  imageRGB = image.visualize(**{'bands': ['B4', 'B3', 'B2'], 'max': 9000, 'min': 0.5})\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  else:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = imageRGB, \n",
    "      description = index_danum,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,      \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,            \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 61,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "Uu0ei3jQLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_index2(lat, lon, index_danum, band, size=1000, func=make_ndvi):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = func(image), \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 62,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "sLq5PXm_o2Wy"
   },
   "outputs": [],
   "source": [
    "def doExport_mmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = make_med_mndwi(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 63,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "Kak-wbbWpOH2"
   },
   "outputs": [],
   "source": [
    "def doExport_gmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = s2_sr_greenestpixel_func2(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 64,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "fciKiExpLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_srtm2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  image = ee.Image('USGS/SRTMGL1_003')\n",
    "  # image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 65,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "6tewPrA5LBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_slope2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "  image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 66,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {},
   "outputs": [],
   "source": [
    "# srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "# lat = 30\n",
    "# lon = -75\n",
    "\n",
    "# folder = FOLDER\n",
    "# size = 1\n",
    "# size_ = \"1\"\n",
    "# band = \"srtm\"\n",
    "# index_danum = \"1\"\n",
    "# task = ee.batch.Export.image.toCloudStorage(\n",
    "#       image = srtm, \n",
    "#       description = \"1\" + '_' + \"1\",\n",
    "#       bucket = BUCKET,\n",
    "#       # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "#       fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "#       region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "#       crs = 'EPSG:4326',\n",
    "#       # crs_transform = crs_transform,\n",
    "#       dimensions = \"256x256\",\n",
    "#       maxPixels = 1E13,\n",
    "#       fileFormat = \"GeoTIFF\",\n",
    "#       formatOptions = {\n",
    "#       \"cloudOptimized\" : True\n",
    "#       }\n",
    "#       )\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 67,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.status()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 68,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "d5DAwuVxLBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_jrc2(lat, lon, index_danum, band, size=1000, func=None, res='hires'):\n",
    "  jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "  # jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                # .bandNames().getInfo()\n",
    "  if band == \"transition\":\n",
    "    jrc = jrc.select(\"transition\")\n",
    "  elif band == \"max_extent\":\n",
    "    jrc = jrc.select(\"max_extent\")\n",
    "  else:\n",
    "    jrc = jrc.select(\"seasonality\")\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = jrc, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 69,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "zUJSFiJ5KSwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def export_data2(index_danum, lat, lon):\n",
    "  for size in [1000, 10000]:\n",
    "    # doExport_RBG2(lat, lon, index_danum, 'RBG', size)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndvi', size, make_ndvi)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndwi', size, make_ndwi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndvi', size, make_mndvi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndwi', size, make_mndwi)\n",
    "    doExport_gmndwi(lat, lon, index_danum, 'gmndwi', size, None)\n",
    "    # doExport_gmndwi(lat, lon, index_danum, 'mmndwi', size, None)\n",
    "    doExport_srtm2(lat, lon, index_danum, 'srtm', size, None)\n",
    "    # doExport_slope2(lat, lon, index_danum, 'slope', size, None)\n",
    "    if size == 1000:\n",
    "      doExport_jrc2(lat, lon, index_danum, 'seasonality', size, None)\n",
    "      doExport_jrc2(lat, lon, index_danum, 'transition', size, None)\n",
    "    # doExport_jrc2(lat, lon, index_danum, 'max_extent', size, None, hires)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "id": "lMZyUm24AcWU"
   },
   "source": [
    "# Set up global variables for Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "uPov83g1tiGH"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR BUCKET HERE:\n",
    "BUCKET = 'pollutemenot-ai'\n",
    "FOLDER = 'test_final3'\n",
    "# FOLDER = \"GEE_images_final2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
=======
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
    "id": "9XprqTJDAaRo"
   },
   "source": [
    "# Exporting Set up"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 70,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {
    "id": "W4n0N8A573Je"
   },
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 4000\n",
    "# SHOBHA: records 4000 - 8000\n",
    "# RADHIKA: records 8000 - 12000\n",
    "# JOE: 12000 - last\n",
    "\n",
    "names = [\"MADHUKAR\", 'SHOBHA', 'RADHIKA', 'JOE']\n",
    "start = [1, 4000, 8000, 12000]\n",
    "end = [4000, 8000, 12000, df.shape[0]]\n",
<<<<<<< HEAD
    "MY_NAME = \"MADHUKAR\"\n",
=======
    "MY_NAME = \"JOE\"\n",
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
    "\n",
    "start_dict = dict(zip(names, start))\n",
    "end_dict = dict(zip(names, end))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
=======
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "# create a service key json file and point to it here,   I will put a copy of the .json key I am using in slack.\n",
    "storage_client = storage.Client.from_service_account_json('../../colab_downloads/access_key1.json')  # create a service key json file and point to it here\n",
    "\n",
    "bucket_name = 'pollutemenot-ai'\n",
    "bucket = storage_client.get_bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images already existing in GCP\n",
    "t0 = time.time()\n",
    "prefix = 'GEE_images_final2/'\n",
    "#dl_dir = '/data/image_final2/'\n",
    "blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "file_blobs = list(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEE_images_final2/hires/gmndwi/1001_LRE-2019-00359-176_gmndwi_hires.tif\n",
      "[1000, 1001, 1002, 1003]\n"
     ]
    }
   ],
   "source": [
    "# get the index numbers of all images already existing in GCP\n",
    "import re\n",
    "print(file_blobs[1].name)\n",
    "file_names = [blob.name.replace('/', '_') for blob in file_blobs]\n",
    "file_index_numbers = [int(re.split('_|-',x)[5]) for x in file_names]\n",
    "print(file_index_numbers[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2 [[1, 'LRB-1983-10120', 42.85821, -76.70773], [2, 'LRB-1985-69031', 43.1523, -75.85524]]\n"
=======
      "520 [[13417, 'SWG-2016-00591', 28.07742, -97.0858], [13418, 'SWG-2016-00592', 28.0773, -97.0857], [13419, 'SWG-2016-00593', 28.076890000000002, -97.08539]]\n"
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "index_begin = 1\n",
    "index_end = 3\n",
    "\n",
    "list_of_parameters = []\n",
    "for count in range(index_begin, index_end):\n",
=======
    "# create a list of input parameters for GEE export, excluding image index numbers already in GCP.\n",
    "index_begin = 13201\n",
    "index_end = 14000\n",
    "\n",
    "list_of_parameters = []\n",
    "for count in range(index_begin, index_end):\n",
    "    if count in file_index_numbers: continue # already downloaded.\n",
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
    "    #if count == index_begin: print(\"exporting index =\", count)\n",
    "    da_number = df.iloc[count-1][\"da_number\"]\n",
    "    latitude = df.iloc[count-1][\"latitude\"]\n",
    "    longitude = df.iloc[count-1][\"longitude\"]\n",
    "    index = count\n",
    "    list_of_parameters.append([index, da_number, latitude, longitude])\n",
    "print(len(list_of_parameters), list_of_parameters[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting processing and uploading of indexstarting processing and uploading of index starting processing and uploading of index starting processing and uploading of indexstarting processing and uploading of index13434starting processing and uploading of index \n",
      " 13596  1341713497starting processing and uploading of index\n",
      "13579\n",
      "starting processing and uploading of index13467 \n",
      "\n",
      "13514 \n",
      "13547\n",
      "\n"
     ]
    }
   ],
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def export_data_pooler(x):\n",
    "    index, da_number, latitude, longitude = x\n",
    "    print('starting processing and uploading of index', index)\n",
    "    export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "    print(\"Done uploading hires and lores of index =\", index)\n",
    "    \n",
    "if __name__ == '__main__':\n",
<<<<<<< HEAD
    "    with Pool(5) as p:\n",
    "        print(p.map(export_data_pooler, list_of_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jurisdiction_type</th>\n",
       "      <th>da_number</th>\n",
       "      <th>district</th>\n",
       "      <th>project_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>date_issued_or_denied</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>...</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>cwa3</th>\n",
       "      <th>cwa4</th>\n",
       "      <th>cwa5</th>\n",
       "      <th>cwa6</th>\n",
       "      <th>cwa7</th>\n",
       "      <th>cwa8</th>\n",
       "      <th>cwa9</th>\n",
       "      <th>potential_wetland</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1983-10120</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>Trade-A-Yacht (Hibiscus Harbor - Union Springs...</td>\n",
       "      <td>-76.70773</td>\n",
       "      <td>42.85821</td>\n",
       "      <td>6/19/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1985-69031</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>POOLEY, MARK A.</td>\n",
       "      <td>-75.85524</td>\n",
       "      <td>43.15230</td>\n",
       "      <td>7/7/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1986-99614</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>Bellamy, Michael (previous: MACKO, JOHN)</td>\n",
       "      <td>-78.04046</td>\n",
       "      <td>42.68911</td>\n",
       "      <td>10/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1990-97632</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>WESTWOOD COUNTRY CLUB</td>\n",
       "      <td>-78.77134</td>\n",
       "      <td>42.97994</td>\n",
       "      <td>6/28/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1991-98611</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>MODERN LANDFILL INCORPORATED</td>\n",
       "      <td>-78.97142</td>\n",
       "      <td>43.21616</td>\n",
       "      <td>3/22/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>SWL-2015-00176</td>\n",
       "      <td>Little Rock</td>\n",
       "      <td>Pine Bluff Arsenal: Building Demolition</td>\n",
       "      <td>-92.09857</td>\n",
       "      <td>34.32060</td>\n",
       "      <td>5/20/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>SWL-2016-00403</td>\n",
       "      <td>Little Rock</td>\n",
       "      <td>City of Marshall - Waterline Replacement</td>\n",
       "      <td>-92.63156</td>\n",
       "      <td>35.90693</td>\n",
       "      <td>1/19/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>SWT-2014-00848-rbh</td>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Sofidel/AEP/Terracon Project 04147070-1 JD alo...</td>\n",
       "      <td>-95.55320</td>\n",
       "      <td>36.12439</td>\n",
       "      <td>1/30/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14617</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>SWT-2016-00539</td>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Proposed Walmart Development 45 acre site, NW ...</td>\n",
       "      <td>-97.58169</td>\n",
       "      <td>35.65544</td>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14618</th>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>SWT-2017-00650</td>\n",
       "      <td>Tulsa</td>\n",
       "      <td>North Fork Solar Project, LLC, Sec 21,22,27,28...</td>\n",
       "      <td>-98.98376</td>\n",
       "      <td>34.62336</td>\n",
       "      <td>5/7/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14619 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      jurisdiction_type           da_number     district  \\\n",
       "0               RAPANOS      LRB-1983-10120      Buffalo   \n",
       "1               RAPANOS      LRB-1985-69031      Buffalo   \n",
       "2               RAPANOS      LRB-1986-99614      Buffalo   \n",
       "3               RAPANOS      LRB-1990-97632      Buffalo   \n",
       "4               RAPANOS      LRB-1991-98611      Buffalo   \n",
       "...                 ...                 ...          ...   \n",
       "14614           RAPANOS      SWL-2015-00176  Little Rock   \n",
       "14615           RAPANOS      SWL-2016-00403  Little Rock   \n",
       "14616           RAPANOS  SWT-2014-00848-rbh        Tulsa   \n",
       "14617           RAPANOS      SWT-2016-00539        Tulsa   \n",
       "14618           RAPANOS      SWT-2017-00650        Tulsa   \n",
       "\n",
       "                                            project_name  longitude  latitude  \\\n",
       "0      Trade-A-Yacht (Hibiscus Harbor - Union Springs...  -76.70773  42.85821   \n",
       "1                                        POOLEY, MARK A.  -75.85524  43.15230   \n",
       "2               Bellamy, Michael (previous: MACKO, JOHN)  -78.04046  42.68911   \n",
       "3                                  WESTWOOD COUNTRY CLUB  -78.77134  42.97994   \n",
       "4                           MODERN LANDFILL INCORPORATED  -78.97142  43.21616   \n",
       "...                                                  ...        ...       ...   \n",
       "14614            Pine Bluff Arsenal: Building Demolition  -92.09857  34.32060   \n",
       "14615           City of Marshall - Waterline Replacement  -92.63156  35.90693   \n",
       "14616  Sofidel/AEP/Terracon Project 04147070-1 JD alo...  -95.55320  36.12439   \n",
       "14617  Proposed Walmart Development 45 acre site, NW ...  -97.58169  35.65544   \n",
       "14618  North Fork Solar Project, LLC, Sec 21,22,27,28...  -98.98376  34.62336   \n",
       "\n",
       "      date_issued_or_denied  rha_determination  cwa_determination  rha1  ...  \\\n",
       "0                 6/19/2020                  0                  0     0  ...   \n",
       "1                  7/7/2016                  0                  1     0  ...   \n",
       "2                10/12/2017                  0                  1     0  ...   \n",
       "3                 6/28/2016                  0                  1     0  ...   \n",
       "4                 3/22/2016                  0                  1     0  ...   \n",
       "...                     ...                ...                ...   ...  ...   \n",
       "14614             5/20/2016                  0                  0     0  ...   \n",
       "14615             1/19/2018                  0                  0     0  ...   \n",
       "14616             1/30/2018                  1                  1     0  ...   \n",
       "14617            11/17/2016                  0                  1     0  ...   \n",
       "14618              5/7/2018                  0                  1     0  ...   \n",
       "\n",
       "       cwa2  cwa3  cwa4  cwa5  cwa6  cwa7  cwa8  cwa9  potential_wetland  \\\n",
       "0         0     0     0     0     0     0     0     0                  1   \n",
       "1         0     1     0     0     0     0     0     0                  0   \n",
       "2         0     0     0     0     0     0     0     0                  0   \n",
       "3         0     1     0     0     0     0     0     0                  1   \n",
       "4         0     1     0     1     1     0     0     0                  1   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...                ...   \n",
       "14614     0     0     0     0     0     0     0     0                  1   \n",
       "14615     0     0     0     0     0     0     0     0                  0   \n",
       "14616     1     0     0     0     0     0     0     0                  0   \n",
       "14617     0     0     1     0     1     0     0     0                  1   \n",
       "14618     0     0     1     0     0     0     0     1                  1   \n",
       "\n",
       "       Index  \n",
       "0          1  \n",
       "1          2  \n",
       "2          3  \n",
       "3          4  \n",
       "4          5  \n",
       "...      ...  \n",
       "14614  14615  \n",
       "14615  14616  \n",
       "14616  14617  \n",
       "14617  14618  \n",
       "14618  14619  \n",
       "\n",
       "[14619 rows x 22 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
=======
    "    with Pool(8) as p:\n",
    "        print(p.map(export_data_pooler, list_of_parameters))\n"
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijL-clnf74GG",
    "outputId": "7c16f74a-3208-4fd0-e671-5f1a0a3eade8"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting index = 12000\n",
      "Done uploading hires and lores of index = 12000\n",
      "Done uploading hires and lores of index = 12001\n",
      "Done uploading hires and lores of index = 12002\n",
      "Done uploading hires and lores of index = 12003\n",
      "Done uploading hires and lores of index = 12004\n",
      "Done uploading hires and lores of index = 12005\n",
      "Done uploading hires and lores of index = 12006\n",
      "Done uploading hires and lores of index = 12007\n",
      "Done uploading hires and lores of index = 12008\n",
      "Done uploading hires and lores of index = 12009\n",
      "Done uploading hires and lores of index = 12010\n",
      "Done uploading hires and lores of index = 12011\n",
      "Done uploading hires and lores of index = 12012\n",
      "Done uploading hires and lores of index = 12013\n",
      "Done uploading hires and lores of index = 12014\n",
      "Done uploading hires and lores of index = 12015\n",
      "Done uploading hires and lores of index = 12016\n",
      "Done uploading hires and lores of index = 12017\n",
      "Done uploading hires and lores of index = 12018\n",
      "Done uploading hires and lores of index = 12019\n",
      "Done uploading hires and lores of index = 12020\n",
      "Done uploading hires and lores of index = 12021\n",
      "Done uploading hires and lores of index = 12022\n",
      "Done uploading hires and lores of index = 12023\n",
      "Done uploading hires and lores of index = 12024\n",
      "Done uploading hires and lores of index = 12025\n",
      "Done uploading hires and lores of index = 12026\n",
      "Done uploading hires and lores of index = 12027\n",
      "Done uploading hires and lores of index = 12028\n",
      "Done uploading hires and lores of index = 12029\n",
      "Done uploading hires and lores of index = 12030\n",
      "Done uploading hires and lores of index = 12031\n",
      "Done uploading hires and lores of index = 12032\n",
      "Done uploading hires and lores of index = 12033\n",
      "Done uploading hires and lores of index = 12034\n",
      "Done uploading hires and lores of index = 12035\n",
      "Done uploading hires and lores of index = 12036\n",
      "Done uploading hires and lores of index = 12037\n",
      "Done uploading hires and lores of index = 12038\n",
      "Done uploading hires and lores of index = 12039\n",
      "Done uploading hires and lores of index = 12040\n",
      "Done uploading hires and lores of index = 12041\n",
      "Done uploading hires and lores of index = 12042\n",
      "Done uploading hires and lores of index = 12043\n",
      "Done uploading hires and lores of index = 12044\n",
      "Done uploading hires and lores of index = 12045\n",
      "Done uploading hires and lores of index = 12046\n",
      "Done uploading hires and lores of index = 12047\n",
      "Done uploading hires and lores of index = 12048\n",
      "Done uploading hires and lores of index = 12049\n",
      "Done uploading hires and lores of index = 12050\n",
      "Done uploading hires and lores of index = 12051\n",
      "Done uploading hires and lores of index = 12052\n",
      "Done uploading hires and lores of index = 12053\n",
      "Done uploading hires and lores of index = 12054\n",
      "Done uploading hires and lores of index = 12055\n",
      "Done uploading hires and lores of index = 12056\n",
      "Done uploading hires and lores of index = 12057\n",
      "Done uploading hires and lores of index = 12058\n",
      "Done uploading hires and lores of index = 12059\n",
      "Done uploading hires and lores of index = 12060\n",
      "Done uploading hires and lores of index = 12061\n",
      "Done uploading hires and lores of index = 12062\n",
      "Done uploading hires and lores of index = 12063\n",
      "Done uploading hires and lores of index = 12064\n",
      "Done uploading hires and lores of index = 12065\n",
      "Done uploading hires and lores of index = 12066\n",
      "Done uploading hires and lores of index = 12067\n",
      "Done uploading hires and lores of index = 12068\n",
      "Done uploading hires and lores of index = 12069\n",
      "Done uploading hires and lores of index = 12070\n",
      "Done uploading hires and lores of index = 12071\n",
      "Done uploading hires and lores of index = 12072\n",
      "Done uploading hires and lores of index = 12073\n",
      "Done uploading hires and lores of index = 12074\n",
      "Done uploading hires and lores of index = 12075\n",
      "Done uploading hires and lores of index = 12076\n",
      "Done uploading hires and lores of index = 12077\n",
      "Done uploading hires and lores of index = 12078\n",
      "Done uploading hires and lores of index = 12079\n",
      "Done uploading hires and lores of index = 12080\n",
      "Done uploading hires and lores of index = 12081\n",
      "Done uploading hires and lores of index = 12082\n",
      "Done uploading hires and lores of index = 12083\n",
      "Done uploading hires and lores of index = 12084\n",
      "Done uploading hires and lores of index = 12085\n",
      "Done uploading hires and lores of index = 12086\n",
      "Done uploading hires and lores of index = 12087\n",
      "Done uploading hires and lores of index = 12088\n",
      "Done uploading hires and lores of index = 12089\n",
      "Done uploading hires and lores of index = 12090\n",
      "Done uploading hires and lores of index = 12091\n",
      "Done uploading hires and lores of index = 12092\n",
      "Done uploading hires and lores of index = 12093\n",
      "Done uploading hires and lores of index = 12094\n",
      "Done uploading hires and lores of index = 12095\n",
      "Done uploading hires and lores of index = 12096\n",
      "Done uploading hires and lores of index = 12097\n"
     ]
    }
   ],
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
   "source": [
    "# started 7:27am 2/24\n",
    "# 1-28 done\n",
    "index_begin = 12000\n",
    "index_end = df.shape[0]\n",
    "\n",
    "if index_begin >= start_dict[MY_NAME] and index_end <= end_dict[MY_NAME]:\n",
    "  for count in range(index_begin, index_end):\n",
    "    if count == index_begin: print(\"exporting index =\", count)\n",
    "    da_number = df.iloc[count-1][\"da_number\"]\n",
    "    latitude = df.iloc[count-1][\"latitude\"]\n",
    "    longitude = df.iloc[count-1][\"longitude\"]\n",
    "    index = count\n",
    "    export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "    print(\"Done uploading hires and lores of index =\", index)\n",
    "else:\n",
    "  print(\"Please ensure the begin and end is within the interval assigned to you\")\n",
    "\n",
    "print(\"Woohoo all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULTYN5stCZXl"
   },
   "outputs": [],
   "source": [
    "stop # stope execution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Y_Ux_ggTCy"
   },
   "source": [
    "# SSURGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxvR5sNlgUrB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ssurgo_path = \"/content/drive/MyDrive/GeoSpatialData/SSURGO/muaggatt.zip\"\n",
    "df_s = pd.read_csv(ssurgo_path, compression='zip', header=0, sep='\\t', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F2QAB0cmMdv"
   },
   "outputs": [],
   "source": [
    "def find_percent_hydric(lat=None, lon=None):\n",
    "  ssurgo = ee.Image(\"users/madhukarreddy/gSSURGO\")\n",
    "  pt = ee.Geometry.Point([lon, lat])\n",
    "  mukey = ssurgo.select('b1').clip(pt).sample(pt).getInfo()[\"features\"][0]['properties']['b1'] \n",
    "  hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "  return int(hydclprs)\n",
    "\n",
    "# lat = float(df[df.index == 100].latitude)\n",
    "# lon = float(df[df.index == 100].longitude)\n",
    "find_percent_hydric(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpXFsD2UxXy"
   },
   "source": [
    "# Opening GeoTIFF images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4p7GDYFtC0L"
   },
   "source": [
    "## Download GCS contents to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jzvTSAcF4BR"
   },
   "outputs": [],
   "source": [
    "# create relevant folder for download using %cd and %mkdir\n",
    "# %cd drive/MyDrive/Madhukar/images\n",
    "# %mkdir /content/drive/MyDrive/Madhukar/images3\n",
    "\n",
    "# https://philipplies.medium.com/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041\n",
    "# !gsutil -m cp -r gs://pollutemenot-ai/test3/hires/gmndwi /content/drive/MyDrive/Madhukar/images/test3_1/hires/gmndwi\n",
    "\n",
    "# !gsutil -m cp -r \"gs://pollutemenot-ai/test3/\" \"/content/drive/MyDrive/Madhukar/images3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQNZ1xBzj-Rp"
   },
   "outputs": [],
   "source": [
    "# use !pwd to get local path\n",
    "# dont forget the / at the end!\n",
    "local_download_path = r\"/content/drive/MyDrive/Madhukar/test_final/lores/srtm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZVPulc6yW-g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [None] * 3\n",
    "\n",
    "# https://howtothink.readthedocs.io/en/latest/PvL_H.html\n",
    "for i in range(1,4):\n",
    "  ax[i-1] = fig.add_subplot(2, 2, i) \n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(local_download_path):\n",
    "    \n",
    "  if filename.endswith(\"tif\"): \n",
    "      print(filename)\n",
    "      try:\n",
    "        dataset = gdal.Open(local_download_path+filename, gdal.GA_ReadOnly) \n",
    "        # Note GetRasterBand() takes band no. starting from 1 not 0\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        arr = band.ReadAsArray()\n",
    "        colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]  # R -> G -> B correct form\n",
    "        # colors = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]  # B -> G -> R\n",
    "        n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "        cmap_name = 'my_list'\n",
    "        cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "\n",
    "        ax[count].imshow(arr, cmap=cm)\n",
    "        ax[count].set_title(filename.split(\"_\")[-2])\n",
    "        # plt.imshow(arr)\n",
    "        # dataset.GetGeoTransform()\n",
    "        print(\"({},{})\".format(dataset.GetRasterBand(1).XSize, dataset.GetRasterBand(1).YSize))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "  count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpnvJSsjpYVh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.02.23_ExportImagestoGCS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.2"
=======
   "version": "3.6.9"
>>>>>>> 0ffba93f2f9e60f37f9d0a43fbdd95238a9e1811
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
